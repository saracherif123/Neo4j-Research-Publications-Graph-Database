PaperID,Title,Year,Abstract,DOI
caf27f4542e8dccdaaa22a27a7b95b5ef0c6d2b8,County-level prioritization for managing the Covid-19 pandemic: a systematic unsupervised learning approach,2024,"
Purpose
The COVID-19 pandemic has posed many challenges in almost all sectors around the globe. Because of the pandemic, government entities responsible for managing health-care resources face challenges in managing and distributing their limited and valuable health resources. In addition, severe outbreaks may occur in a small or large geographical area. Therefore, county-level preparation is crucial for officials and organizations who manage such disease outbreaks. However, most COVID-19-related research projects have focused on either state- or country-level. Only a few studies have considered county-level preparations, such as identifying high-risk counties of a particular state to fight against the COVID-19 pandemic. Therefore, the purpose of this research is to prioritize counties in a state based on their COVID-19-related risks to manage the COVID outbreak effectively.


Design/methodology/approach
In this research, the authors use a systematic hybrid approach that uses a clustering technique to group counties that share similar COVID conditions and use a multi-criteria decision-making approach – the analytic hierarchy process – to rank clusters with respect to the severity of the pandemic. The clustering was performed using two methods, k-means and fuzzy c-means, but only one of them was used at a time during the experiment.


Findings
The results of this study indicate that the proposed approach can effectively identify and rank the most vulnerable counties in a particular state. Hence, state health resources managing entities can identify counties in desperate need of more attention before they allocate their resources and better prepare those counties before another surge.


Originality/value
To the best of the authors’ knowledge, this study is the first to use both an unsupervised learning approach and the analytic hierarchy process to identify and rank state counties in accordance with the severity of COVID-19.
",10.1108/jsit-02-2023-0027
000458c2651ab147d503429f6460aa80155e8e35,Segment as Points for Efficient Online Multi-Object Tracking and Segmentation,2020,"Current multi-object tracking and segmentation (MOTS) methods follow the tracking-by-detection paradigm and adopt convolutions for feature extraction. However, as affected by the inherent receptive field, convolution based feature extraction inevitably mixes up the foreground features and the background features, resulting in ambiguities in the subsequent instance association. In this paper, we propose a highly effective method for learning instance embeddings based on segments by converting the compact image representation to un-ordered 2D point cloud representation. Our method generates a new tracking-by-points paradigm where discriminative instance embeddings are learned from randomly selected points rather than images. Furthermore, multiple informative data modalities are converted into point-wise representations to enrich point-wise features. The resulting online MOTS framework, named PointTrack, surpasses all the state-of-the-art methods including 3D tracking methods by large margins (5.4% higher MOTSA and 18 times faster over MOTSFusion) with the near real-time speed (22 FPS). Evaluations across three datasets demonstrate both the effectiveness and efficiency of our method. Moreover, based on the observation that current MOTS datasets lack crowded scenes, we build a more challenging MOTS dataset named APOLLO MOTS with higher instance density. Both APOLLO MOTS and our codes are publicly available at this https URL.",10.1007/978-3-030-58452-8_16
00059087c954c1af6ece33115315e3e0ecc2f2c2,Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem,2020,"Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men. In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al, 2019) Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. Rather than attempt to create a ‘balanced’ dataset, we use transfer learning on a small set of trusted, gender-balanced examples. This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. A known pitfall of transfer learning on new domains is ‘catastrophic forgetting’, which we address at adaptation and inference time. During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction. At inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al, 2019 on WinoMT with no degradation of general test set BLEU. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability.",10.18653/v1/2020.acl-main.690
000651c8f5971dff64d5cbeafe7302a53895823e,Picture-to-Amount (PITA): Predicting Relative Ingredient Amounts from Food Images,2020,"Increased awareness of the impact of food consumption on health and lifestyle today has given rise to novel data-driven food analysis systems. Although these systems may recognize the ingredients, a detailed analysis of their amounts in the meal, which is paramount for estimating the correct nutrition, is usually ignored. In this paper, we study the novel and challenging problem of predicting the relative amount of each ingredient from a food image. We propose PITA, the Picture-to-Amount deep learning architecture to solve the problem. More specifically, we predict the ingredient amounts using a domain-driven Wasserstein loss from image-to-recipe cross-modal embeddings learned to align the two views of food data. Experiments on a dataset of recipes collected from the Internet show the model generates promising results and improves the baselines on this challenging task. A demo of our system and our data is available at: foodai.cs.rutgers.edu.",10.1109/ICPR48806.2021.9412828
0008cb017b9659277632d26566e512a0bdfde553,Adaptive Variance Based Label Distribution Learning for Facial Age Estimation,2020,,10.1007/978-3-030-58592-1_23
0009d673b46c3f79835ac13e5a987b3c5e153628,ROIAL: Region of Interest Active Learning for Characterizing Exoskeleton Gait Preference Landscapes,2020,"Characterizing what types of exoskeleton gaits are comfortable for users, and understanding the science of walking more generally, require recovering a user’s utility landscape. Learning these landscapes is challenging, as walking trajectories are defined by numerous gait parameters, data collection from human trials is expensive, and user safety and comfort must be ensured. This work proposes the Region of Interest Active Learning (ROIAL) framework, which actively learns each user’s underlying utility function over a region of interest that ensures safety and comfort. ROIAL learns from ordinal and preference feedback, which are more reliable feedback mechanisms than absolute numerical scores. The algorithm’s performance is evaluated both in simulation and experimentally for three non-disabled subjects walking inside of a lower-body exoskeleton. ROIAL learns Bayesian posteriors that predict each exoskeleton user’s utility landscape across four exoskeleton gait parameters. The algorithm discovers both commonalities and discrepancies across users’ gait preferences and identifies the gait parameters that most influenced user feedback. These results demonstrate the feasibility of recovering gait utility landscapes from limited human trials.",10.1109/ICRA48506.2021.9560840
00133d41d5ecef1a9d046d2b92bb2a23a335cb7c,TempLe: Learning Template of Transitions for Sample Efficient Multi-task RL,2020,"Transferring knowledge among various environments is important for efficiently learning multiple tasks online. Most existing methods directly use the previously learned models or previously learned optimal policies to learn new tasks. However, these methods may be inefficient when the underlying models or optimal policies are substantially different across tasks. In this paper, we propose Template Learning (TempLe), a PAC-MDP method for multi-task reinforcement learning that could be applied to tasks with varying state/action space without prior knowledge of inter-task mappings. TempLe gains sample efficiency by extracting similarities of the transition dynamics across tasks even when their underlying models or optimal policies have limited commonalities. We present two algorithms for an ``online'' and a ``finite-model'' setting respectively. We prove that our proposed TempLe algorithms achieve much lower sample complexity than single-task learners or state-of-the-art multi-task methods. We show via systematically designed experiments that our TempLe method universally outperforms the state-of-the-art multi-task methods (PAC-MDP or not) in various settings and regimes.",10.1609/aaai.v35i11.17174
40bd70d2d176d2112259b7350cccfc058e6b7afa,Value function optimistic initialization with uncertainty and confidence awareness in lifelong reinforcement learning,2023,,10.1016/j.knosys.2023.111036
001528e5987d8864a0187dff279eafdce0fbe703,RUMBA-Mouse: Rapid User Mouse-Behavior Authentication Using a CNN-RNN Approach,2020,"Mouse behavior analysis has become an increasingly attractive area to biometric researchers in recent years. Many mouse behavior based user authentication schemes have been proposed in the past decades. However, most of them rely on statistical analysis or heuristic feature extraction of the mouse behavior. In this paper, we present a CNN-RNN combined neural network model for mouse behavior based user authentication, which takes raw sequential mouse data as input rather than relies on heuristic feature extraction. Additionally, we integrate the model with a practical framework of static user authentication and evaluate it on a real dataset. The results show our approach yields a 3.16% EER and a 99.39% AUC, with a short authentication delay of 6.11 seconds on average, which demonstrates the effectiveness and practicality of applying deep learning techniques for static mouse behavior based user authentication. Furthermore, by modifying the activation maximization method, we study and visualize the features learned by different layers of our neural network.",10.1109/CNS48642.2020.9162287
9c04bf14dc03a4290d16e936166a611aac770ae4,Artificial Intelligence Meets Kinesthetic Intelligence: Mouse-based User Authentication based on Hybrid Human-Machine Learning,2022,"Current mainstream biometric user authentication approaches are based on passive measurements of the subject's characteristics, and usually come with less-than-satisfactory accuracy. This paper takes a unique approach to biometric authentication. Specifically, instead of training a machine learning algorithm to recognize a legitimate user, the paper proposes a hybrid type of training, in which the legitimate user is also trained to use a customized instance of the machine. The user thus achieves a level of artificially-induced expertise to interact with the machine, which makes the user easier to recognize. We implement this concept in a mouse-based user authentication system, in which we produce customized machine instances by introducing an angle offset to the standard mouse. Human subjects then rely on their kinesthetic intelligence to achieve motor learning and visual-motor adaptation to the modified mouse. We design a 7-week IRB-approved experiment, collect data from 18 human subjects over this period, and evaluate the proposed approach with two existing state-of-the-art mouse-based authentication schemes. We find that, in both schemes, our approach significantly outperforms the baseline in which a regular unaltered mouse is used. Somewhat surprisingly, results also show that our approach improves the authentication performance even when both legitimate and non-legitimate users are trained to exactly the same instance of customized machine (i.e., the same mouse angle offset). In addition, we also observe that users can generally maintain their learned expertise even after one week of washout, which further demonstrates the practicality of the approach. Finally, we present a practical strategy to manage the enrollment of users in such a proposed system.",10.1145/3488932.3523257
dde42bddcc0c7fe402a1a537a97ff70b2a363df7,Semi-supervised recognition for artificial intelligence assisted pathology image diagnosis,2024,"The analysis and interpretation of cytopathological images are crucial in modern medical diagnostics. However, manually locating and identifying relevant cells from the vast amount of image data can be a daunting task. This challenge is particularly pronounced in developing countries where there may be a shortage of medical expertise to handle such tasks. The challenge of acquiring large amounts of high-quality labelled data remains, many researchers have begun to use semi-supervised learning methods to learn from unlabeled data. Although current semi-supervised learning models partially solve the issue of limited labelled data, they are inefficient in exploiting unlabeled samples. To address this, we introduce a new AI-assisted semi-supervised scheme, the Reliable-Unlabeled Semi-Supervised Segmentation (RU3S) model. This model integrates the ResUNet-SE-ASPP-Attention (RSAA) model, which includes the Squeeze-and-Excitation (SE) network, Atrous Spatial Pyramid Pooling (ASPP) structure, Attention module, and ResUNet architecture. Our model leverages unlabeled data effectively, improving accuracy significantly. A novel confidence filtering strategy is introduced to make better use of unlabeled samples, addressing the scarcity of labelled data. Experimental results show a 2.0% improvement in mIoU accuracy over the current state-of-the-art semi-supervised segmentation model ST, demonstrating our approach’s effectiveness in solving this medical problem.",10.1038/s41598-024-70750-7
001f207cc7a80ad08cdbab2b8eca23b0a8618cb3,Regularized robust fuzzy least squares twin support vector machine for class imbalance learning,2020,"Twin support vector machines (TWSVM) have been successfully applied to the classification problems. TWSVM is computationally efficient model of support vector machines (SVM). However, in real world classification problems issues of class imbalance and noise provide great challenges. Due to this, models lead to the inaccurate classification either due to higher tendency towards the majority class or due to the presence of noise. We provide an improved version of robust fuzzy least squares twin support vector machine (RFLSTSVM) known as regularized robust fuzzy least squares twin support vector machine (RRFLSTSVM) to handle the imbalance problem. The advantage of RRFLSTSVM over RFLSTSVM is that the proposed RRFLSTSVM implements the structural risk minimization principle by the introduction of regularization term in the primal formulation of the objective functions. This modification leads to the improved classification as it embodies the marrow of statistical learning theory. The proposed RRFLSTSVM doesn’t require any extra assumption as the matrices resulting in the dual are positive definite. However, RFLSTSVM is based on the assumption that the inverse of the matrices resulting in the dual always exist as the matrices are positive semi-definite. To subsidize the effects of class imbalance and noise, the data samples are assigned weights via fuzzy membership function. The fuzzy membership function incorporates the imbalance ratio knowledge and assigns appropriate weights to the data samples. Unlike TWSVM which solves a pair of quadratic programming problem (QPP), the proposed RRFLSTSVM method solves a pair of system of linear equations and hence is computationally efficient. Experimental and statistical analysis show the efficacy of the proposed RRFLSTSVM method.",10.1109/IJCNN48605.2020.9207724
f250ef0a7e3bdc60888dabea5ee31ec0e1e59662,Methods for Class-Imbalanced Learning with Support Vector Machines: A Review and an Empirical Evaluation,2024,"This paper presents a review on methods for class-imbalanced learning with the Support Vector Machine (SVM) and its variants. We first explain the structure of SVM and its variants and discuss their inefficiency in learning with class-imbalanced data sets. We introduce a hierarchical categorization of SVM-based models with respect to class-imbalanced learning. Specifically, we categorize SVM-based models into re-sampling, algorithmic, and fusion methods, and discuss the principles of the representative models in each category. In addition, we conduct a series of empirical evaluations to compare the performances of various representative SVM-based models in each category using benchmark imbalanced data sets, ranging from low to high imbalanced ratios. Our findings reveal that while algorithmic methods are less time-consuming owing to no data pre-processing requirements, fusion methods, which combine both re-sampling and algorithmic approaches, generally perform the best, but with a higher computational load. A discussion on research gaps and future research directions is provided.",10.1007/s00500-024-09931-5
786d99ce9386d77f22a59197a53e9a8e81d09b1a,Large-Scale Fuzzy Least Squares Twin SVMs for Class Imbalance Learning,2022,"Twin support vector machines (TSVMs) have been successfully employed for binary classification problems. With the advent of machine learning algorithms, data have proliferated and there is a need to handle or process large-scale data. TSVMs are not successful in handling large-scale data due to the following: 1) the optimization problem solved in the TSVM needs to calculate large matrix inverses, which makes it an ineffective choice for large-scale problems; 2) the empirical risk minimization principle is employed in the TSVM and, hence, may suffer due to overfitting; and 3) the Wolfe dual of TSVM formulation involves positive-semidefinite matrices, and hence, singularity issues need to be resolved manually. Keeping in view the aforementioned shortcomings, in this article, we propose a novel large-scale fuzzy least squares TSVM for class imbalance learning (LS-FLSTSVM-CIL). We formulate the LS-FLSTSVM-CIL such that the proposed optimization problem ensures that: 1) no matrix inversion is involved in the proposed LS-FLSTSVM-CIL formulation, which makes it an efficient choice for large-scale problems; 2) the structural risk minimization principle is implemented, which avoids the issues of overfitting and results in better performance; and 3) the Wolfe dual formulation of the proposed LS-FLSTSVM-CIL model involves positive-definite matrices. In addition, to resolve the issues of class imbalance, we assign fuzzy weights in the proposed LS-FLSTSVM-CIL to avoid bias in dominating the samples of class imbalance problems. To make it more feasible for large-scale problems, we use an iterative procedure known as the sequential minimization principle to solve the objective function of the proposed LS-FLSTSVM-CIL model. From the experimental results, one can see that the proposed LS-FLSTSVM-CIL demonstrates superior performance in comparison to baseline classifiers. To demonstrate the feasibility of the proposed LS-FLSTSVM-CIL on large-scale classification problems, we evaluate the classification models on the large-scale normally distributed clustered (NDC) dataset. To demonstrate the practical applications of the proposed LS-FLSTSVM-CIL model, we evaluate it for the diagnosis of Alzheimer’s disease and breast cancer disease. Evaluation on NDC datasets shows that the proposed LS-FLSTSVM-CIL has feasibility in large-scale problems as it is fast in comparison to the baseline classifiers.",10.1109/TFUZZ.2022.3161729
f7af9c3c1bf4860d2e749074228e4e91a0bd5cd1,Regularized Least Squares Twin SVM for Multiclass Classification,2021,,10.1016/j.bdr.2021.100295
9435afc04d6401babb62a3cd74c8897d9f8a7c50,Fuzzy least squares projection twin support vector machines for class imbalance learning,2021,,10.1016/j.asoc.2021.107933
723e8b15ed134e8940adc2b0af06ac016df340f8,A Novel Ensemble Method of RVFL For Classification Problem,2021,"Ensemble learning methods, which combine several base classifiers, is a common technique to enhance the classification ability of ensemble models in the field of pattern recognition and machine learning. Rotation Forest, an ensemble algorithm, has been used widely in various fields with nice generalization performance. The main idea of Rotation Forest is to animate concurrently both diversity and individual accuracy within the ensemble. On the other hand, random vector functional link (RVFL) neural network, a randomized version of single layer feed-forward neural network (SLFN), is a successful model because of its universal approximation property. In this paper, we propose a novel ensemble method, known as rotated random vector functional link neural network (RoF-RVFL), which combines rotation forest (RoF) and RVFL classifiers. To verify the effectiveness of the proposed RoF- RVFL method, empirical comparisons are carried out among Rotation Forest (RoF), Random Forest (RaF), RVFL and the proposed RoF-RVFL method over 42 UCI benchmark datasets. The experimental results show that the proposed RoF-Rvflmethod is able to generate more robust network with better generalization performance.",10.1109/IJCNN52387.2021.9533836
002544729825daf6843a471ccb22d446969511b7,A survey on Arabic Image Captioning Systems Using Deep Learning Models,2020,"This paper describes a literature survey for the deep leaning approaches used in image captioning. Approaches will be discussed based on four main categories: model architecture, attention mechanism, image model and language model. Most of the current research focuses on generating captions in English language, leaving a gap in research for other languages, especially for Arabic language. Therefore, we will highlight the available research and approaches used to generate captions in Arabic. We will discuss the used datasets, translation approach, evaluation metrics and the results for each method. We conclude the survey by proposing some possible future directions for Arabic image captioning.",10.1109/IIT50501.2020.9299027
d5dc38ed5a3e386a2f4303ef858796f94bbb44d0,Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks,2024,"Multimodal large language models (MLLMs) have proven effective in a wide range of tasks requiring complex reasoning and linguistic comprehension. However, due to a lack of high-quality multimodal resources in languages other than English, success of MLLMs remains relatively limited to English-based settings. This poses significant challenges in developing comparable models for other languages, including even those with large speaker populations such as Arabic. To alleviate this challenge, we introduce a comprehensive family of Arabic MLLMs, dubbed \textit{Peacock}, with strong vision and language capabilities. Through comprehensive qualitative and quantitative analysis, we demonstrate the solid performance of our models on various visual reasoning tasks and further show their emerging dialectal potential. Additionally, we introduce ~\textit{Henna}, a new benchmark specifically designed for assessing MLLMs on aspects related to Arabic culture, setting the first stone for culturally-aware Arabic MLLMs.The GitHub repository for the \textit{Peacock} project is available at \url{https://github.com/UBC-NLP/peacock}.",10.48550/arXiv.2403.01031
8b3298e265545853819add7fbbb93e7a73d1480d,Challenges Designing for FPGAs Using High-Level Synthesis,2022,"High-Level Synthesis (HLS) tools are aimed at enabling performant FPGA designs that are authored in a high-level language. While commercial HLS tools are available today, there is still a substantial performance gap between most designs developed via HLS relative to traditional, labor intensive approaches. We report on several cases where an anticipated performance improvement was either not realized or resulted in decreased performance. These include: programming paradigm choices between data parallel vs. pipelined designs; dataflow implementations; configuration parameter choices; and handling odd data set sizes. The results point to a number of improvements that are needed for HLS tool flows, including a strong need for performance modeling that can reliably guide the compilation optimization process.",10.1109/HPEC55821.2022.9926398
baa718db04e692ca1b42d1bbc4113148ef0ffe51,Electromyography Signals in Embedded Systems: A Review of Processing and Classification Techniques,2025,"This article provides an overview of the implementation of electromyography (EMG) signal classification algorithms in various embedded system architectures. They address the specifications used for implementation in different devices, such as the number of movements and the type of classification method. Architectures analyzed include microcontrollers, DSP, FPGA, SoC, and neuromorphic computers/chips in terms of precision, processing time, energy consumption, and cost. This analysis highlights the capabilities of each technology for real-time wearable applications such as smart prosthetics and gesture control devices, as well as the importance of local inference in artificial intelligence models to minimize execution times and resource consumption. The results show that the choice of device depends on the required system specifications, the robustness of the model, the number of movements to be classified, and the limits of knowledge concerning design and budget. This work provides a reference for selecting technologies for developing embedded biomedical solutions based on EMG.",10.3390/biomimetics10030166
0b9c5a128663118fb9b40e7fbba89272e20b23b2,Development of a novel machine learning-based approach for brain function assessment and integrated software solution,2024,,10.1016/j.aei.2024.102461
003020856c6cfbdecb7f71ca8fb53bfa75e2efec,Data mining Classification Techniques for Intrusion Detection System,2020,"In today’s contemporary era detection of network attacks has become the need of the hour due to increasing network traffic over the network. Data mining technique plays a vital role in searching network attacks and anomalies. These techniques help in selecting and refining useful and relevant information from large data sets. Data mining technique helps in classify relevant data for Intrusion Detection System. Intrusion Detection system generates alarms for the network traffic about the foreign invasions in the system. In the following paper we have used data mining classification techniques for intrusion detection in order to build a safe network. Random tree, Naive Bayes, J48 and Random forest machine learning classifier are used for classification. Comparison based on parameters like data accuracy, finding useful patterns, extracts useful information between the techniques.",10.1109/CICN49253.2020.9242642
0038069fee6b7a4421dc5ddad8c80a8d5533902e,Automatic assessment of students' software models using a simple heuristic and machine learning,2020,"Software models are increasingly popular. To educate the next generation of software engineers, it is important that they learn how to model software systems well, so that they can design them effectively in industry. It is also important that instructors have the tools that can help them assess students' models more effectively. In this paper, we investigate how a tool that combines a simple heuristic with machine learning techniques can be used to help assess student submissions in model-driven engineering courses. We apply our proposed technique to first identify submissions of high quality and second to predict approximate letter grades. The results are comparable to human grading and a complex rule-based technique for the former and surprisingly accurate for the latter.",10.1145/3417990.3418741
935ff86490094be9866d23e44ef0028aa4647506,A customizable framework for multimodal emotion recognition using ensemble of deep neural network models,2023,,10.1007/s00530-023-01188-6
c58dda0a1f11411a83db20e6a2f787c7de23db62,DTL-I-ResNet18: facial emotion recognition based on deep transfer learning and improved ResNet18,2023,,10.1007/s11760-023-02490-6
004147dab6dc3372133f551f06d40d0aecc2951e,Chatbot: A Deep Neural Network Based Human to Machine Conversation Model,2020,"A conversational agent (chatbot) is computer software capable of communicating with humans using natural language processing. The crucial part of building any chatbot is the development of conversation. Despite many developments in Natural Language Processing (NLP) and Artificial Intelligence (AI), creating a good chatbot model remains a significant challenge in this field even today. A conversational bot can be used for countless errands. In general, they need to understand the user's intent and deliver appropriate replies. This is a software program of a conversational interface that allows a user to converse in the same manner one would address a human. Hence, these are used in almost every customer communication platform, like social networks. At present, there are two basic models used in developing a chatbot. Generative based models and Retrieval based models. The recent advancements in deep learning and artificial intelligence, such as the end-to-end trainable neural networks have rapidly replaced earlier methods based on hand-written instructions and patterns or statistical methods. This paper proposes a new method of creating a chatbot using a deep neural learning method. In this method, a neural network with multiple layers is built to learn and process the data.",10.1109/ICCCNT49239.2020.9225395
bba7b84e46e48179020c822b827bd8d4c520e43d,An Adaptive Symmetrical Load Balancing Scheme for Next Generation Wireless Networks,2023,"In dense Wi-Fi networks, achieving load balancing is critical to optimize network utilization and provide equitable network consumption among the users. Traditional Wi-Fi networks have issues in attaining effective load balancing. Software-Defined Networking (SDN) has presented a viable solution by isolating the data plane and control plane, enabling more agile and cost-effective networks. In this paper we put forward an Adaptive Symmetrical Load Balancing (ASLB) scheme to ensure fairness of load symmetry in Software Defined Wi-Fi Networks (SD-Wi-Fi), while also optimizing the flows transition process using an Analytical Hierarchal Process (AHP). User activity is monitored by access points (APs), which operate under OpenFlow standards. Three essential features, packet volume, packet category and delay hindrance, are used for flow assignment to various controllers. The controllers are arranged in two tiers, universal and regional controllers. The universal controller (UC) handles the workload statistics of regional controllers (RC) in the form of clusters. Extensive simulations using OMNeT++ simulator are performed. The performance parameters taken into consideration are throughput, delay, packet loss rate, network transition count and workload distribution. Our findings demonstrate that the ASLB technique effectively optimizes the network utilization and ensures equitable network consumption among the end users. The proposed scheme outperforms the Mean Probe Delay scheme (MPD), Channel Measurement-based Access Selection scheme (CMAS), Received Signal Strength Indicator-based scheme (RSSI) and Distributed Antenna Selection scheme (DASA), being 40% higher in throughput and 25% lower in delay.",10.3390/sym15071316
0043212391ddf7d689c434efd0daa9960225e3b0,Knowledge-Guided Reinforcement Learning Control for Robotic Lower Limb Prosthesis,2020,"Robotic prostheses provide new opportunities to better restore lost functions than passive prostheses for trans-femoral amputees. But controlling a prosthesis device automatically for individual users in different task environments is an unsolved problem. Reinforcement learning (RL) is a naturally promising tool. For prosthesis control with a user in the loop, it is desirable that the controlled prosthesis can adapt to different task environments as quickly and smoothly as possible. However, most RL agents learn or relearn from scratch when the environment changes. To address this issue, we propose the knowledge-guided Q-learning (KG-QL) control method as a principled way for the problem. In this report, we collected and used data from two able-bodied (AB) subjects wearing a RL controlled robotic prosthetic limb walking on level ground. Our ultimate goal is to build an efficient RL controller with reduced time and data requirements and transfer knowledge from AB subjects to amputee subjects. Toward this goal, we demonstrate its feasibility by employing OpenSim, a well-established human locomotion simulator. Our results show the OpenSim simulated amputee subject improved control tuning performance over learning from scratch by utilizing knowledge transfer from AB subjects. Also in this paper, we will explore the possibility of information transfer from AB subjects to help tuning for the amputee subjects.",10.1109/ICRA40945.2020.9196749
0f7c65bd41f40082a59710756cc6665474a66095,Advances in AI-based prosthetics development: editorial,2024,,10.1097/JS9.0000000000001573
db01720987e8070f011f59bbfe6912b58bf87ded,A Novel Collaborative Knowledge Sharing and Self-Learning Framework for Robotic Systems in Search and Rescue Operations,2023,"Robots may face difficulty detecting all the required objects with their current vision in a search and rescue (SAR) operation in a disaster scenario. This can be due to a partial view of some objects from the whole disaster scenario, or the training model of the robots may not be perfect enough to recognize all the available objects. This research established a method for SAR robots to learn without human interaction by combining their own and other robots' knowledge. It assumes that all robots have a simple machine learning model that can detect objects from their whole perspective. Yolov8, trained with a custom dataset, has been used as the basic machine learning model in this study to detect the object. Two robots and one object are used for the proof of concept, with one robot seeing the object fully and the other partially. Partial view images were auto-labeled, and the fundamental machine learning model of the robot with partial view was retrained using auto-labeled data until a predetermined condition was met. Then the other robot in the environment received the final retrained model with partial view information. The suggested system was tested in numerous scenarios and usually works well.",10.1109/IECON51785.2023.10312013
2e14626d8e55ab2b09e1964e361ce8baaf6dfc00,Knowledge-guided robot learning on compliance control for robotic assembly task with predictive model,2023,,10.1016/j.eswa.2023.121037
0044614fb5a290bc2ee1d5141579e5dfe783284c,Compensation of the Radial and Circumferential Mode 0 Vibration of a Permanent Magnet Electric Machine based on an Experimental Characterisation,2020,"This paper presents an approach how to compensate radial and circumferential mode 0 vibrations based on harmonic currents. First, the influence of harmonic currents and the mechanic transfer function are identified. Using these results, harmonic currents are derived which compensate the pre-existing vibrations.",10.23919/EPE20ECCEEurope43536.2020.9215955
0047110e62edb20e946f6d4e498e2506066c67ad,Unification of Machine Learning Features,2020,"In the Information Age, Machine learning (ML) provides a competitive advantage to any business. Machine learning applications are not limited to driverless cars or online recommendations but are widely used in healthcare, social services, government systems, telecommunications, and so on. As many enterprises are trying to step up machine learning applications, it is critical to have a long-term strategy. Most of the enterprises are not able to truly realize the fruits of ML capabilities due to its complexity. It is easier to access a variety of data today due to data democratization, distributed storage, technological advancements, and big data applications. Despite easier data access and recent advancements in ML, developers still spend most of the time in data cleansing, data preparation, and data modeling for ML applications. These steps are often repeated and result in identical features. As identical features can have inconsistent processing while testing and training, more issues pop up at later stages in ML application development. The unification of ML features is an effective way to address these issues. This paper presents details about numerous methods to achieve ML features unification.",10.1109/COMPSAC48688.2020.00-93
004c4777af2589e7759930ea1c674e7537485f73,Optimal Auction Based Automated Negotiation in Realistic Decentralised Market Environments,2020,"Automated negotiations based on learning models have been widely applied in different domains of negotiation. Specifically, for resource allocation in decentralised open market environments with multiple vendors and multiple buyers. In such open market environments, there exists dynamically changing supply and demand of resources, with dynamic arrival of buyers in the market. Besides, each buyer has their own set of constraints, such as budget constraints, time constraints, etc. In this context, efficient negotiation policies should be capable of maintaining the equilibrium between the utilities of both the vendors and the buyers. In this research, we aim to design a mechanism for an optimal auction paradigm, considering the existence of interdependent undisclosed preferences of both, buyers and vendors. Therefore, learning-based negotiation models are immensely appropriate for such open market environments; wherein, self-interested autonomous vendors and buyers cooperate/compete to maximize their utilities based on their undisclosed preferences. Toward this end, we present our current proposal, the two-stage learning-based resource allocation mechanism, wherein utilities of vendors and buyers are optimised at each stage. We are aiming to compare our proposed learning-based resource allocation mechanism with two state-of-the-art bidding-based resource allocation mechanism, which are based on, fixed bidding policy (Samimi, Teimouri, and Mukhtar 2016) and demand-based bidding policy (Kong, Zhang, and Ye 2015). The comparison is to be done based on the overall performance of the open market environment and also based on the individual performances of vendors and buyers.",10.1609/aaai.v34i10.7135
37adf23724e5fe3ae312164f0796b0db526fb077,"A survey of automated negotiation: Human factor, learning, and application",2024,,10.1016/j.cosrev.2024.100683
3d37f255d2cc8d59ff3d5eeef7644166512b3027,Reinforcement learning based monotonic policy for online resource allocation,2022,,10.1016/j.future.2021.09.023
004c7dbd5578865ac72cfa7b6ebc51c7fa7cda31,Two-branch Recurrent Network for Isolating Deepfakes in Videos,2020,"The current spike of hyper-realistic faces artificially generated using deepfakes calls for media forensics solutions that are tailored to video streams and work reliably with a low false alarm rate at the video level. We present a method for deepfake detection based on a two-branch network structure that isolates digitally manipulated faces by learning to amplify artifacts while suppressing the high-level face content. Unlike current methods that extract spatial frequencies as a preprocessing step, we propose a two-branch structure: one branch propagates the original information, while the other branch suppresses the face content yet amplifies multi-band frequencies using a Laplacian of Gaussian (LoG) as a bottleneck layer. To better isolate manipulated faces, we derive a novel cost function that, unlike regular classification, compresses the variability of natural faces and pushes away the unrealistic facial samples in the feature space. Our two novel components show promising results on the FaceForensics++, Celeb-DF, and Facebook's DFDC preview benchmarks, when compared to prior work. We then offer a full, detailed ablation study of our network architecture and cost function. Finally, although the bar is still high to get very remarkable figures at a very low false alarm rate, our study shows that we can achieve good video-level performance when cross-testing in terms of video-level AUC.",10.1007/978-3-030-58571-6_39
0053e5d294cf983a3c0a9b84dfd6477d291a7089,Intermittent Visual Servoing: Efficiently Learning Policies Robust to Instrument Changes for High-precision Surgical Manipulation,2020,"Assisting surgeons with automation of surgical subtasks is challenging due to backlash, hysteresis, and variable tensioning in cable-driven robots. These issues are exacerbated as surgical instruments are changed during an operation. In this work, we propose a framework for automation of high- precision surgical subtasks by learning local, sample-efficient, accurate, closed-loop policies that use visual feedback instead of robot encoder estimates. This framework, which we call deep Intermittent Visual Servoing (IVS), switches to a learned visual servo policy for high-precision segments of repetitive surgical tasks while relying on a coarse open-loop policy for the segments where precision is not necessary. We train the policy using only 180 human demonstrations that are roughly 2 seconds each. Results on a da Vinci Research Kit suggest that combining the coarse policy with half a second of corrections from the learned policy during each high-precision segment improves the success rate on the Fundamentals of Laparoscopic Surgery peg transfer task from 72.9% to 99.2%, 31.3% to 99.2%, and 47.2% to 100.0% for 3 instruments with differing cable properties. In the contexts we studied, IVS attains the highest published success rates for automated surgical peg transfer and is significantly more reliable than previous techniques when instruments are changed. Supplementary material is available at https://tinyurl.com/ivs-icra.",10.1109/ICRA48506.2021.9561070
9e3e0bc628d3e45a1107b1daa84345ce3e6ec2ca,ABM: Attention before Manipulation,2024,"Vision-language models (VLMs) show promising generalization and zero-shot capabilities, offering a potential solution to the impracticality and cost of enabling robots to comprehend diverse human instructions and scene semantics in the real world. Existing approaches most directly integrate the semantic representations from pre-trained VLMs with policy learning. However, these methods are limited to the labeled data learned, resulting in poor generalization ability to unseen instructions and objects. To address the above limitation, we propose a simple method called ""Attention before Manipulation"" (ABM), which fully leverages the object knowledge encoded in CLIP to extract information about the target object in the image. It constructs an Object Mask Field, serving as a better representation of the target object for the model to separate visual grounding from action prediction and acquire specific manipulation skills effectively. We train ABM for 8 RLBench tasks and 2 real-world tasks via behavior cloning. Extensive experiments show that our method significantly outperforms the baselines in the zero-shot and compositional generalization experiment settings.",10.24963/ijcai.2024/201
96b906922390e3a1c2171bb54082a412be2f5d5e,Improving the realism of robotic surgery simulation through injection of learning-based estimated errors,2024,"The development of algorithms for automation of subtasks during robotic surgery can be accelerated by the availability of realistic simulation environments. In this work, we focus on one aspect of the realism of a surgical simulator, which is the positional accuracy of the robot. In current simulators, robots have perfect or near-perfect accuracy, which is not representative of their physical counterparts. We therefore propose a pair of neural networks, trained by data collected from a physical robot, to estimate both the controller error and the kinematic and non-kinematic error. These error estimates are then injected within the simulator to produce a simulated robot that has the characteristic performance of the physical robot. In this scenario, we believe it is sufficient for the estimated error used in the simulation to have a statistically similar distribution to the actual error of the physical robot. This is less stringent, and therefore more tenable, than the requirement for error compensation of a physical robot, where the estimated error should equal the actual error. Our results demonstrate that error injection reduces the mean position and orientation differences between the simulated and physical robots from 5.0 mm / 3.6 deg to 1.3 mm / 1.7 deg, respectively, which represents reductions by factors of 3.8 and 2.1.",10.1109/ISMR63436.2024.10585672
e08d8129ff2092cdf8e0d48d29ad075bb952da9f,STITCH: Augmented Dexterity for Suture Throws Including Thread Coordination and Handoffs,2024,"We present STITCH: an augmented dexterity pipeline that performs Suture Throws Including Thread Coordination and Handoffs. STITCH iteratively performs needle insertion, thread sweeping, needle extraction, suture cinching, needle handover, and needle pose correction with failure recovery policies. We introduce a novel visual 6D needle pose estimation framework using a stereo camera pair and new suturing motion primitives. We compare STITCH to baselines, including a proprioception-only and a policy without visual servoing. In physical experiments across 15 trials, STITCH achieves an average of 2.93 sutures without human intervention and 4.47 sutures with human intervention. See https://sites.google.com/berkeley.edu/stitchfur code and supplemental materials.",10.1109/ISMR63436.2024.10585751
29957ab454c5f19b9a148b56549b056ee340eb51,CineTransfer: Controlling a Robot to Imitate Cinematographic Style from a Single Example,2023,"This work presents CineTransfer, an algorithmic framework that drives a robot to record a video sequence that mimics the cinematographic style of an input video. We propose features that abstract the aesthetic style of the input video, so the robot can transfer this style to a scene with visual details that are significantly different from the input video. The framework builds upon CineMPC, a tool that allows users to control cinematographic features, like subjects' position on the image and the depth of field, by manipulating the intrinsics and extrinsics of a cinematographic camera. However, CineMPC requires a human expert to specify the desired style of the shot (composition, camera motion, zoom, focus, etc). CineTransfer bridges this gap, aiming a fully autonomous cinematographic platform. The user chooses a single input video as a style guide. CineTransfer extracts and optimizes two important style features, the composition of the subject in the image and the scene depth of field, and provides instructions for CineMPC to control the robot to record an output sequence that matches these features as closely as possible. In contrast with other style transfer methods, our approach is a lightweight and portable framework which does not require deep network training or extensive datasets. Experiments with real and simulated videos demonstrate the system's ability to analyze and transfer style between recordings, and are available in the supplementary video11https://youtu.be/_QzNz5WUtpk",10.1109/IROS55552.2023.10342280
3ac439c4c606ec9f7853819cd25ebb158d4d5123,CNS: Correspondence Encoded Neural Image Servo Policy,2023,"Image servo is an indispensable technique in robotic applications that helps to achieve high precision positioning. The intermediate representation of image servo policy is important to sensor input abstraction and policy output guidance. Classical approaches achieve high precision but require clean keypoint correspondence, and suffer from limited convergence basin or weak feature error robustness. Recent learning-based methods achieve moderate precision and large convergence basin on specific scenes but face issues when generalizing to novel environments. In this paper, we encode keypoints and correspondence into a graph and use graph neural network as architecture of controller. This design utilizes both advantages: generalizable intermediate representation from keypoint correspondence and strong modeling ability from neural network. Other techniques including realistic data generation, feature clustering and distance decoupling are proposed to further improve efficiency, precision and generalization. Experiments in simulation and real-world verify the effectiveness of our method in speed (maximum 40fps along with observer), precision (<0.3° and sub-millimeter accuracy) and generalization (sim-to-real without fine-tuning). Project homepage (full paper with supplementary text, video and code): https://hhcaz.github.io/CNS-home.",10.1109/ICRA57147.2024.10611185
0054a11ba67e1ac4c6243f7fb67e202868ef98d7,Joint Progressive Knowledge Distillation and Unsupervised Domain Adaptation,2020,"Currently, the divergence in distributions of design and operational data, and large computational complexity are limiting factors in the adoption of CNNs in real-world applications. For instance, person re-identification systems typically rely on a distributed set of cameras, where each camera has different capture conditions. This can translate to a considerable shift between source (e.g. lab setting) and target (e.g. operational camera) domains. Given the cost of annotating image data captured for fine-tuning in each target domain, unsupervised domain adaptation (UDA) has become a popular approach to adapt CNNs. Moreover, state-of-the-art deep learning models that provide a high level of accuracy often rely on architectures that are too complex for real-time applications. Although several compression and UDA approaches have recently been proposed to overcome these limitations, they do not allow optimizing a CNN to simultaneously address both. In this paper, we propose an unexplored direction – the joint optimization of CNNs to provide a compressed model that is adapted to perform well for a given target domain. In particular, the proposed approach performs unsupervised knowledge distillation (KD) from a complex teacher model to a compact student model, by leveraging both source and target data. It also improves upon existing UDA techniques by progressively teaching the student about domain-invariant features, instead of directly adapting a compact model on target domain data. Our method is compared against state-of-the-art compression and UDA techniques, using two popular classification datasets for UDA – Office31 and ImageClef-DA. In both datasets, results indicate that our method can achieve the highest level of accuracy while requiring a comparable or lower time complexity.",10.1109/IJCNN48605.2020.9206989
00576c3890e9c6d312bc3eb36201bce83fc4284f,Defending Water Treatment Networks: Exploiting Spatio-temporal Effects for Cyber Attack Detection,2020,"While Water Treatment Networks (WTNs) are critical infrastructures for local communities and public health, WTNs are vulnerable to cyber attacks. Effective detection of attacks can defend WTNs against discharging contaminated water, denying access, destroying equipment, and causing public fear. While there are extensive studies in WTNs attack detection, they only exploit the data characteristics partially to detect cyber attacks. After preliminary exploring the sensing data of WTNs, we find that integrating spatio-temporal knowledge, representation learning, and detection algorithms can improve attack detection accuracy. To this end, we propose a structured anomaly detection framework to defend WTNs by modeling the spatiotemporal characteristics of cyber attacks in WTNs. In particular, we propose a spatio-temporal representation framework specially tailored to cyber attacks after separating the sensing data of WTNs into a sequence of time segments. This framework has two key components. The first component is a temporal embedding module to preserve temporal patterns within a time segment by projecting the time segment of a sensor into a temporal embedding vector. We then construct Spatio-Temporal Graphs (STGs), where a node is a sensor and an attribute is the temporal embedding vector of the sensor, to describe the state of the WTNs. The second component is a spatial embedding module, which learns the final fused embedding of the WTNs from STGs. In addition, we devise an improved one class-SVM model that utilizes a new designed pairwise kernel to detect cyber attacks. The devised pairwise kernel augments the distance between normal and attack patterns in the fused embedding space. Finally, we conducted extensive experimental evaluations with real-world data to demonstrate the effectiveness of our framework: it achieves an accuracy of 91.65%, with average improvement ratios of 82.78% and 22.96% with respect to F1 and AUC, compared with baseline methods.",10.1109/ICDM50108.2020.00012
0b509eb7b0b618bb37652b6fc885bd41953e4d12,Masked Graph Neural Networks for Unsupervised Anomaly Detection in Multivariate Time Series,2023,"Anomaly detection has been widely used in grid operation and maintenance, machine fault detection, and so on. In these applications, the multivariate time-series data from multiple sensors with latent relationships are always high-dimensional, which makes multivariate time-series anomaly detection particularly challenging. In existing unsupervised anomaly detection methods for multivariate time series, it is difficult to capture the complex associations among multiple sensors. Graph neural networks (GNNs) can model complex relations in the form of a graph, but the observed time-series data from multiple sensors lack explicit graph structures. GNNs cannot automatically learn the complex correlations in the multivariate time-series data or make good use of the latent relationships among time-series data. In this paper, we propose a new method—masked graph neural networks for unsupervised anomaly detection (MGUAD). MGUAD can learn the structure of the unobserved causality among sensors to detect anomalies. To robustly learn the temporal context from adjacent time points of time-series data from the same sensor, MGUAD randomly masks some points of the time-series data from the sensor and reconstructs the masked time points. Similarly, to robustly learn the graph-level context from adjacent nodes or edges in the relation graph of multivariate time series, MGUAD masks some nodes or edges in the graph under the framework of a GNN. Comprehensive experiments are conducted on three public datasets. According to the experimental findings, MGUAD outperforms state-of-the-art anomaly detection methods.",10.3390/s23177552
e0ae58b1a801e5c968e8d3cddf9d554efdcfa8b6,Deep Graph Stream SVDD: Anomaly Detection in Cyber-Physical Systems,2023,"Our work focuses on anomaly detection in cyber-physical systems. Prior literature has three limitations: (1) Failing to capture long-delayed patterns in system anomalies; (2) Ignoring dynamic changes in sensor connections; (3) The curse of high-dimensional data samples. These limit the detection performance and usefulness of existing works. To address them, we propose a new approach called deep graph stream support vector data description (SVDD) for anomaly detection. Specifically, we first use a transformer to preserve both short and long temporal patterns of monitoring data in temporal embeddings. Then we cluster these embeddings according to sensor type and utilize them to estimate the change in connectivity between various sensors to construct a new weighted graph. The temporal embeddings are mapped to the new graph as node attributes to form weighted attributed graph. We input the graph into a variational graph auto-encoder model to learn final spatio-temporal representation. Finally, we learn a hypersphere that encompasses normal embeddings and predict the system status by calculating the distances between the hypersphere and data samples. Extensive experiments validate the superiority of our model, which improves F1-score by 35.87%, AUC by 19.32%, while being 32 times faster than the best baseline at training and inference.",10.48550/arXiv.2302.12918
6a7123f691f3f0288b5081f3ce06af8f27a094ac,Interactive reinforced feature selection with traverse strategy,2023,,10.1007/s10115-022-01812-3
0058053d9207f79ec0e4dbda7244cfc8215310ec,3D Human Pose Estimation using Spatio-Temporal Networks with Explicit Occlusion Training,2020,"Estimating 3D poses from a monocular video is still a challenging task, despite the significant progress that has been made in the recent years. Generally, the performance of existing methods drops when the target person is too small/large, or the motion is too fast/slow relative to the scale and speed of the training data. Moreover, to our knowledge, many of these methods are not designed or trained under severe occlusion explicitly, making their performance on handling occlusion compromised. Addressing these problems, we introduce a spatio-temporal network for robust 3D human pose estimation. As humans in videos may appear in different scales and have various motion speeds, we apply multi-scale spatial features for 2D joints or keypoints prediction in each individual frame, and multi-stride temporal convolutional networks (TCNs) to estimate 3D joints or keypoints. Furthermore, we design a spatio-temporal discriminator based on body structures as well as limb motions to assess whether the predicted pose forms a valid pose and a valid movement. During training, we explicitly mask out some keypoints to simulate various occlusion cases, from minor to severe occlusion, so that our network can learn better and becomes robust to various degrees of occlusion. As there are limited 3D ground truth data, we further utilize 2D video data to inject a semi-supervised learning capability to our network. Experiments on public data sets validate the effectiveness of our method, and our ablation studies show the strengths of our network's individual submodules.",10.1609/AAAI.V34I07.6689
0058d3db8cc451a88af34de1f921349101b2dca2,Progressive Point Cloud Deconvolution Generation Network,2020,"In this paper, we propose an effective point cloud generation method, which can generate multi-resolution point clouds of the same shape from a latent vector. Specifically, we develop a novel progressive deconvolution network with the learning-based bilateral interpolation. The learning-based bilateral interpolation is performed in the spatial and feature spaces of point clouds so that local geometric structure information of point clouds can be exploited. Starting from the low-resolution point clouds, with the bilateral interpolation and max-pooling operations, the deconvolution network can progressively output high-resolution local and global feature maps. By concatenating different resolutions of local and global feature maps, we employ the multi-layer perceptron as the generation network to generate multi-resolution point clouds. In order to keep the shapes of different resolutions of point clouds consistent, we propose a shape-preserving adversarial loss to train the point cloud deconvolution generation network. Experimental results demonstrate the effectiveness of our proposed method.",10.1007/978-3-030-58555-6_24
005ad31c9cbf4d296e8afafcab2d532b75e2cb2c,"Efficient Quantum Circuits for Accurate State Preparation of Smooth, Differentiable Functions",2020,"Effective quantum computation relies upon making good use of the exponential information capacity of a quantum machine. A large barrier to designing quantum algorithms for execution on real quantum machines is that, in general, it is intractably difficult to construct an arbitrary quantum state to high precision. Many quantum algorithms rely instead upon initializing the machine in a simple state, and evolving the state through an efficient (i.e. at most polynomial-depth) quantum algorithm. In this work, we show that there exist families of quantum states that can be prepared to high precision with circuits of linear size and depth. We focus on real-valued, smooth, differentiable functions with bounded derivatives on a domain of interest, exemplified by commonly used probability distributions. We further develop an algorithm that requires only linear classical computation time to generate accurate linear- depth circuits to prepare these states, and apply this to well-known and heavily-utilized functions including Gaussian and lognormal distributions. Our procedure rests upon the quantum state representation tool known as the matrix product state (MPS). By efficiently and scalably encoding an explicit amplitude function into an MPS, a high fidelity, linear-depth circuit can directly be generated. These results enable the execution of many quantum algorithms that, aside from initialization, are otherwise depth-efficient.",10.1109/QCE49297.2020.00030
005fd196bc4c4631be48d5d4658cd7a15fbc79d5,Deep Learning Based Localization and HO Optimization in 5G NR Networks,2020,"In the emerging 5G radio networks, beamforming-capable nodes are able to densely cover narrow areas with a high-quality signal. Such systems require high-level handover management system to proactively react to upcoming changes in signal quality, while restricting common issues such as ping-ponging or fast-shadowing of the signal. The utilization of deep learning in such a system allows for dynamic optimization of the system policies, based directly on the past behavior of the users and their channel responses. Our approach on handover optimization is purely non-deterministic, proving the idea that a self-learning network is able to efficiently manage user mobility in dense network scenario. The proposed network consists of feature extractors and dense layers. The model is trained in two stages, first serves as an initial weight setting in supervised fashion based on 3GPP model. The second stage is an optimization problem to reduce the number of unnecessary handovers while sustaining a high-quality connection. The model is also trained to predict the user location information as the second output. The presented results show that the number of handovers can be significantly reduced without decreasing the throughput of the system. The predicted location of the user has meter-level accuracy.",10.1109/ICL-GNSS49876.2020.9115530
f5f0a99e9db87cc489fc9d6093f99d16a331ded0,Indoor Localization With Multi-Beam of 5G New Radio Signals,2024,"In this work, we investigate the property of the multi-beam of 5G new radio (NR) signals for indoor localization. Specifically, the 5G NR signals are firstly sampled by a self-developed software-defined receiver, and the multi-beam is extracted via detecting the multiple synchronization signal blocks (SSBs). Secondly, with the assistance of the pilots in the multiple SSBs, the reference signal received power (RSRP) and reference signal received quality (RSRQ) of the multi-beam are calculated. Thirdly, by stacking the RSRP and RSRQ of the multi-beam as the observables, a fingerprint database is constructed. With the aim to efficiently process the fingerprint features and improve the accuracy of indoor localization, a CatBoost-based algorithm is proposed, and the parameters are further optimized by tree-structured parzen estimator (TPE). To verify the effectiveness of the proposed method, indoor field tests are carried out in an office scenario, where real 5G signals are transmitted from a commercial 5G NR base station indoors. The field tests demonstrate that, by taking the advantages of the multi-beam of 5G NR, the localization accuracy can be able to achieve the accuracy of 1.06 m in the metric of root mean squared error (RMSE), even when only one base station is heard indoors. By comparison with the single-beam, the accuracy of multi-beam has improved 48%.",10.1109/TWC.2024.3380737
b84ee2ece8fac2394a0257d1039d395bddf93144,"A survey on the handover management in 5G-NR cellular networks: aspects, approaches and challenges",2023,"With the purpose of providing higher data rate and ultra-reliable and low-latency communications for the users, employing the small cells in the upcoming Fifth-Generation-New Radio (5G-NR) cellular networks and beyond is leading to the serious challenges in mobility management due to dynamicity of the user equipments (UEs). Among different issues related to the mobility of the UEs, the handover management is one of the key procedures to let the UEs experience a high quality of service (QoS)/quality of experience (QoE). So far, many protocols and algorithms have been proposed to enhance the mobility management in 5G-NR from various aspects, but still a thorough survey has not been incorporated to summarize the recent advances and future directions of the mentioned issue. Thus, the handover management and the corresponding challenges stand in the center of concentrations in this paper, with a glance on 4G to the advancements of 5G-NR. The challenges include QoS/QoE, throughput, delay, traffic load, the attacks during authentication process and resource allocation. To overcome the mentioned challenges, the handover procedure is evaluated by some key performance indicators such as handover ratio, handover failure, radio link failure and such like which depend on the received signal quality. The lack of new advancements and novel aspects of handover plus the separation of security and handover issues in previous works are perceived as research gaps and motivations for the current paper. In this regard, this paper aims to specify and analyze the technical issues, to provide an overview on the proposed methods and recent advances and to bring the future directions into the limelight. We categorize the concurrent standards and methods about the handover, and then, we survey the proposed algorithms including theoretical, algorithm-based and pattern-capturing approaches following the authentication process along with the vulnerabilities and the algorithms to counteract the attacks during handover. Also, we study various issues corresponding to network performance during the handover procedure. Finally, we discuss the open problems and future research directions.",10.1186/s13638-023-02261-4
22325dfc59ce9f33d723b68bb9128a1b21227c12,A Literature Survey on AI-Aided Beamforming and Beam Management for 5G and 6G Systems,2023,"Modern wireless communication systems rely heavily on multiple antennas and their corresponding signal processing to achieve optimal performance. As 5G and 6G networks emerge, beamforming and beam management become increasingly complex due to factors such as user mobility, a higher number of antennas, and the adoption of elevated frequencies. Artificial intelligence, specifically machine learning, offers a valuable solution to mitigate this complexity and minimize the overhead associated with beam management and selection, all while maintaining system performance. Despite growing interest in AI-assisted beamforming, beam management, and selection, a comprehensive collection of datasets and benchmarks remains scarce. Furthermore, identifying the most-suitable algorithm for a given scenario remains an open question. This article aimed to provide an exhaustive survey of the subject, highlighting unresolved issues and potential directions for future developments. The discussion encompasses the architectural and signal processing aspects of contemporary beamforming, beam management, and selection. In addition, the article examines various communication challenges and their respective solutions, considering approaches such as centralized/decentralized, supervised/unsupervised, semi-supervised, active, federated, and reinforcement learning.",10.3390/s23094359
2752473905b2669e2afac07c73d0de456a1f9f4a,"Applied Machine Learning for IIoT and Smart Production—Methods to Improve Production Quality, Safety and Sustainability",2022,"Industrial IoT (IIoT) has revolutionized production by making data available to stakeholders at many levels much faster, with much greater granularity than ever before. When it comes to smart production, the aim of analyzing the collected data is usually to achieve greater efficiency in general, which includes increasing production but decreasing waste and using less energy. Furthermore, the boost in communication provided by IIoT requires special attention to increased levels of safety and security. The growth in machine learning (ML) capabilities in the last few years has affected smart production in many ways. The current paper provides an overview of applying various machine learning techniques for IIoT, smart production, and maintenance, especially in terms of safety, security, asset localization, quality assurance and sustainability aspects. The approach of the paper is to provide a comprehensive overview on the ML methods from an application point of view, hence each domain—namely security and safety, asset localization, quality control, maintenance—has a dedicated chapter, with a concluding table on the typical ML techniques and the related references. The paper summarizes lessons learned, and identifies research gaps and directions for future work.",10.3390/s22239148
00696ba295d66f049d70272219f7fea4266171be,Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space,2020,"When trained effectively, the Variational Autoencoder (VAE) can be both a powerful generative model and an effective representation learning framework for natural language. In this paper, we propose the first large-scale language VAE model, Optimus. A universal latent embedding space for sentences is first pre-trained on large text corpus, and then fine-tuned for various language generation and understanding tasks. Compared with GPT-2, Optimus enables guided language generation from an abstract level using the latent vectors. Compared with BERT, Optimus can generalize better on low-resource language understanding tasks due to the smooth latent space structure. Extensive experimental results on a wide range of language tasks demonstrate the effectiveness of Optimus. It achieves new state-of-the-art on VAE language modeling benchmarks. We hope that our first pre-trained big VAE language model itself and results can help the NLP community renew the interests of deep generative models in the era of large-scale pre-training, and make these principled methods more practical.",10.18653/v1/2020.emnlp-main.378
006aaf250b13cdcabf583b9647f2a0004efd94c5,"""We have been magnified for years - Now you are under the microscope!"": Co-researchers with Learning Disabilities Created an Online Survey to Challenge Public Understanding of Learning Disabilities",2020,"Public attitudes towards learning disabilities (LDs) are generally reported as positive, inclusive and empathetic. However, these findings do not reflect the lived experiences of people with LDs. To shed light on this disparity, a team of co-researchers with LDs created the first online survey to challenge public understanding of LDs, asking questions in ways that are important to them and represent how they see themselves. Here, we describe and evaluate the process of creating an accessible survey platform and an online survey in a research team consisting of academic and non-academic professionals with and without LDs or autism. Through this inclusive research process, the co-designed survey met the expectations of the co-researchers and was well-received by the initial survey respondents. We reflect on the co-researchers' perspectives following the study completion, and consider the difficulties and advantages we encountered deploying such approaches and their potential implications on future survey data analysis.",10.1145/3313831.3376278
d7fb85f05ffc372cf713942ee1971bd103cf4557,"""Piece it together"": Insights from one year of engagement with electronics and programming for people with intellectual disabilities",2023,"We present the results of one year spent engaging people living with intellectual disabilities with an electronics and programming package. The program was run in collaboration with a disability support organization and delivered by support workers. We evaluate key qualities of the package at three sites via ongoing communication and reflective interviews with five support workers, along with observation of sessions and contextual inquiry with eleven people with a range of disabilities. Our findings demonstrate the importance of physicality in enabling experiences by creating real-world analogues and supporting diverse group interactions; how groups support members’ attention, motivating each other, and allow space for coping mechanisms; and participants’ growing confidence and creativity in problem solving, and the emergence of self-directed activities. We discuss the importance of diverse repetition for skill development, how skills develop over the year, and pragmatic lessons for conducting a long-term research program with a disability support organization.",10.1145/3544548.3581401
e579f964e2287a9c7b88191464f102c15a026095,Design Principles for Robot-Assisted Feeding in Social Contexts,2023,"Social dining, i.e., eating with/in company, is replete with meaning and cultural significance. Unfortunately, for the 1.8 million Americans with motor impairments who cannot eat without assistance, challenges restrict them from enjoying this pleasant social ritual. In this work, we identify the needs of participants with motor impairments during social dining and how robot-assisted feeding can address them. Using speculative videos that show robot behaviors within a social dining context, we interviewed participants to understand their preferences. Following a community-based participatory research method, we worked with a community researcher with motor impairments throughout this study. We contribute (a) insights into how a robot can help overcome challenges in social dining, (b) design principles for creating robot-assisted feeding systems, (c) and an implementation guide for future research in this area. Our key finding is that robots' unique assistive qualities can address challenges people with motor impairments face during social dining, promoting empowerment and belonging.",10.1145/3568162.3576988
83e1d5189bfdf405ad30d479adb4b676a7e10f1a,Protein Design by Directed Evolution Guided by Large Language Models,2024,"Directed evolution, a strategy for protein engineering, optimizes protein properties (i.e., fitness) by a rigorous and resource-intensive process of screening or selecting among a vast range of mutations. By conducting an in silico screening of sequence properties, machine learning-guided directed evolution (MLDE) can expedite the optimization process and alleviate the experimental workload. In this work, we propose a general MLDE framework in which we apply recent advancements of Deep Learning in protein representation learning and protein property prediction to accelerate the searching and optimization processes. In particular, we introduce an optimization pipeline that utilizes Large Language Models (LLMs) to pinpoint the mutation hotspots in the sequence and then suggest replacements to improve the overall fitness. Our experiments have shown the superior efficiency and efficacy of our proposed framework in the conditional protein generation, in comparision with other state-of-the-art baseline algorithms. We expect this work will shed a new light on not only protein engineering but also on solving combinatorial problems using data-driven methods. Our implementation is publicly available at https://github.com/HySonLab/Directed_Evolution.",10.1109/TEVC.2024.3439690
006e1d8ca4fba1fea02a3a0df0e226b11dbc9581,Greedy adversarial equilibrium: an efficient alternative to nonconvex-nonconcave min-max optimization,2020,"Min-max optimization of an objective function f: ℝd × ℝd → ℝ is an important model for robustness in an adversarial setting, with applications to many areas including optimization, economics, and deep learning. In many applications f may be nonconvex-nonconcave, and finding a global min-max point may be computationally intractable. There is a long line of work that seeks computationally tractable algorithms for alternatives to the min-max optimization model. However, many of the alternative models have solution points which are only guaranteed to exist under strong assumptions on f, such as convexity, monotonicity, or special properties of the starting point. We propose an optimization model, the ε-greedy adversarial equilibrium, and show that it can serve as a computationally tractable alternative to the min-max optimization model. Roughly, we say that a point (x⋆, y⋆) is an ε-greedy adversarial equilibrium if y⋆ is an ε-approximate local maximum for f(x⋆,·), and x⋆ is an ε-approximate local minimum for a “greedy approximation” to the function maxz f(x, z) which can be efficiently estimated using second-order optimization algorithms. We prove the existence of such a point for any smooth function which is bounded and has Lipschitz Hessian. To prove existence, we introduce an algorithm that converges from any starting point to an ε-greedy adversarial equilibrium in a number of evaluations of the function f, the max-player’s gradient ∇y f(x,y), and its Hessian ∇y2 f(x,y), that is polynomial in the dimension d, 1/ε, and the bounds on f and its Lipschitz constant.",10.1145/3406325.3451097
5e4437c0ef2bcfa06102341938d63e68762527a6,Bitrate-Constrained DRO: Beyond Worst Case Robustness To Unknown Group Shifts,2023,"Training machine learning models robust to distribution shifts is critical for real-world applications. Some robust training algorithms (e.g., Group DRO) specialize to group shifts and require group information on all training points. Other methods (e.g., CVaR DRO) that do not need group annotations can be overly conservative, since they naively upweight high loss points which may form a contrived set that does not correspond to any meaningful group in the real world (e.g., when the high loss points are randomly mislabeled training points). In this work, we address limitations in prior approaches by assuming a more nuanced form of group shift: conditioned on the label, we assume that the true group function (indicator over group) is simple. For example, we may expect that group shifts occur along low bitrate features (e.g., image background, lighting). Thus, we aim to learn a model that maintains high accuracy on simple group functions realized by these low bitrate features, that need not spend valuable model capacity achieving high accuracy on contrived groups of examples. Based on this, we consider the two-player game formulation of DRO where the adversary's capacity is bitrate-constrained. Our resulting practical algorithm, Bitrate-Constrained DRO (BR-DRO), does not require group information on training samples yet matches the performance of Group DRO on datasets that have training group annotations and that of CVaR DRO on long-tailed distributions. Our theoretical analysis reveals that in some settings BR-DRO objective can provably yield statistically efficient and less conservative solutions than unconstrained CVaR DRO.",10.48550/arXiv.2302.02931
8ee3099493cc536827cce6c45bcfc15cb6d3085b,On the Convergence of No-Regret Learning Dynamics in Time-Varying Games,2023,"Most of the literature on learning in games has focused on the restrictive setting where the underlying repeated game does not change over time. Much less is known about the convergence of no-regret learning algorithms in dynamic multiagent settings. In this paper, we characterize the convergence of optimistic gradient descent (OGD) in time-varying games. Our framework yields sharp convergence bounds for the equilibrium gap of OGD in zero-sum games parameterized on natural variation measures of the sequence of games, subsuming known results for static games. Furthermore, we establish improved second-order variation bounds under strong convexity-concavity, as long as each game is repeated multiple times. Our results also apply to time-varying general-sum multi-player games via a bilinear formulation of correlated equilibria, which has novel implications for meta-learning and for obtaining refined variation-dependent regret bounds, addressing questions left open in prior papers. Finally, we leverage our framework to also provide new insights on dynamic regret guarantees in static games.",10.48550/arXiv.2301.11241
6f9c78c61b7f6c850e1cd8a9fb5bcea75d90cd0e,Fairness-Aware Regression Robust to Adversarial Attacks,2022,"In this paper, we take a first step towards answering the question of how to design fair machine learning algorithms that are robust to adversarial attacks. Using a minimax framework, we aim to design an adversarially robust fair regression model that achieves optimal performance in the presence of an attacker who is able to add a carefully designed adversarial data point to the dataset or perform a rank-one attack on the dataset. By solving the proposed nonsmooth nonconvex-nonconcave minimax problem, the optimal adversary as well as the robust fairness-aware regression model are obtained. For both synthetic data and real-world datasets, numerical results illustrate that the proposed adversarially robust fair models have better performance on poisoned datasets than other fair machine learning models in both prediction accuracy and group-based fairness measure.",10.1109/TSP.2023.3328111
fc20ba65bc975e09347f6142aa54423679de308f,STay-ON-the-Ridge: Guaranteed Convergence to Local Minimax Equilibrium in Nonconvex-Nonconcave Games,2022,"Min-max optimization problems involving nonconvex-nonconcave objectives have found important applications in adversarial training and other multi-agent learning settings. Yet, no known gradient descent-based method is guaranteed to converge to (even local notions of) min-max equilibrium in the nonconvex-nonconcave setting. For all known methods, there exist relatively simple objectives for which they cycle or exhibit other undesirable behavior different from converging to a point, let alone to some game-theoretically meaningful one~\cite{flokas2019poincare,hsieh2021limits}. The only known convergence guarantees hold under the strong assumption that the initialization is very close to a local min-max equilibrium~\cite{wang2019solving}. Moreover, the afore-described challenges are not just theoretical curiosities. All known methods are unstable in practice, even in simple settings. We propose the first method that is guaranteed to converge to a local min-max equilibrium for smooth nonconvex-nonconcave objectives. Our method is second-order and provably escapes limit cycles as long as it is initialized at an easy-to-find initial point. Both the definition of our method and its convergence analysis are motivated by the topological nature of the problem. In particular, our method is not designed to decrease some potential function, such as the distance of its iterate from the set of local min-max equilibria or the projected gradient of the objective, but is designed to satisfy a topological property that guarantees the avoidance of cycles and implies its convergence.",10.48550/arXiv.2210.09769
b1e01584203710f2cf6e234bd3662aacbdcc9e1a,Provably Efficient Convergence of Primal-Dual Actor-Critic with Nonlinear Function Approximation,2022,"We study the convergence of the actor-critic algorithm with nonlinear function approximation under a nonconvex-nonconcave primal-dual formulation. Stochastic gradient descent ascent is applied with an adaptive proximal term for robust learning rates. We show the first efficient convergence result with primal-dual actor-critic with a convergence rate of $\mathcal{O}\left(\sqrt{\frac{\ln \left(N d G^2 \right)}{N}}\right)$ under Markovian sampling, where $G$ is the element-wise maximum of the gradient, $N$ is the number of iterations, and $d$ is the dimension of the gradient. Our result is presented with only the Polyak-\L{}ojasiewicz condition for the dual variables, which is easy to verify and applicable to a wide range of reinforcement learning (RL) scenarios. The algorithm and analysis are general enough to be applied to other RL settings, like multi-agent RL. Empirical results on OpenAI Gym continuous control tasks corroborate our theoretical findings.",10.5555/3545946.3599028
00793bcd17c56940d437413c9078a76b07841f16,Decentralizing Feature Extraction with Quantum Convolutional Neural Network for Automatic Speech Recognition,2020,"We propose a novel decentralized feature extraction approach in federated learning to address privacy-preservation issues for speech recognition. It is built upon a quantum convolutional neural network (QCNN) composed of a quantum circuit encoder for feature extraction, and a recurrent neural network (RNN) based end-to-end acoustic model (AM). To enhance model parameter protection in a decentralized architecture, an input speech is first up-streamed to a quantum computing server to extract Mel-spectrogram, and the corresponding convolutional features are encoded using a quantum circuit algorithm with random parameters. The encoded features are then down-streamed to the local RNN model for the final recognition. The proposed decentralized framework takes advantage of the quantum learning progress to secure models and to avoid privacy leakage attacks. Testing on the Google Speech Commands Dataset, the proposed QCNN encoder attains a competitive accuracy of 95.12% in a decentralized model, which is better than the previous architectures using centralized RNN models with convolutional features. We conduct an in-depth study of different quantum circuit encoder architectures to provide insights into designing QCNN-based feature extractors. Neural saliency analyses demonstrate a high correlation between the proposed QCNN features, class activation maps, and the input Mel-spectrogram. We provide an implementation1 for future studies.",10.1109/ICASSP39728.2021.9413453
bf5b43c986660ccacab97b54a76da33d9b722ea8,MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training,2023,"Contrastive pretraining of image-text foundation models, such as CLIP, demonstrated excellent zero-shot performance and improved robustness on a wide range of downstream tasks. However, these models utilize large transformer-based encoders with significant memory and latency overhead which pose challenges for deployment on mobile devices. In this work, we introduce MobileCLIP - a new family of efficient image-text models optimized for runtime performance along with a novel and efficient training approach, namely multi-modal reinforced training. The proposed training approach leverages knowledge transfer from an image captioning model and an ensemble of strong CLIP encoders to improve the accuracy of efficient moels. Our approach avoids train-time compute overhead by storing the additional knowledge in a reinforced dataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff for zero-shot classification and retrieval tasks on several datasets. Our MobileCLIP-S2 variant is 2.3× faster while more accurate compared to previous best CLIP model based on ViT-B/16. We further demonstrate the effectiveness of our multi-modal reinforced training by training a CLIP model based on ViT-B/16 image backbone and achieving +2.9% average performance improvement on 38 evaluation benchmarks compared to the previous best. Moreover, we show that the proposed approach achieves 10×-1000 × improved learning efficiency when compared with non-reinforced CLIP training. Code and models are available at https://github.com/apple/ml-mobileclip",10.1109/CVPR52733.2024.01511
01cd6565acfe7b32290fc87980f469489135a6b0,"Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement",2023,"We propose Dataset Reinforcement, a strategy to improve a dataset once such that the accuracy of any model architecture trained on the reinforced dataset is improved at no additional training cost for users. We propose a Dataset Reinforcement strategy based on data augmentation and knowledge distillation. Our generic strategy is designed based on extensive analysis across CNN- and transformer-based models and performing large-scale study of distillation with state-of-the-art models with various data augmentations. We create a reinforced version of the ImageNet training dataset, called ImageNet+, as well as reinforced datasets CIFAR-100+, Flowers-102+, and Food-101+. Models trained with ImageNet+ are more accurate, robust, and calibrated, and transfer well to downstream tasks (e.g., segmentation and detection). As an example, the accuracy of ResNet-50 improves by 1.7% on the ImageNet validation set, 3.5% on ImageNetV2, and 10.0% on ImageNet-R. Expected Calibration Error (ECE) on the ImageNet validation set is also reduced by 9.9%. Using this backbone with Mask-RCNN for object detection on MS-COCO, the mean average precision improves by 0.8%. We reach similar gains for MobileNets, ViTs, and Swin-Transformers. For MobileNetV3 and Swin-Tiny, we observe significant improvements on ImageNet-R/A/C of up to 20% improved robustness. Models pretrained on ImageNet+ and fine-tuned on CIFAR-100+, Flowers-102+, and Food-101+, reach up to 3.4% improved accuracy. The code, datasets, and pretrained models are available at https://github.com/apple/ml-dr.",10.1109/ICCV51070.2023.01562
5df9898247ea803eca085c0859e9fdae6d7056c8,Data Incubation — Synthesizing Missing Data for Handwriting Recognition,2021,"In this paper, we demonstrate how a generative model can be used to build a better recognizer through the control of content and style. We are building an online handwriting recognizer from a modest amount of training samples. By training our controllable handwriting synthesizer on the same data, we can synthesize handwriting with previously underrepresented content (e.g., URLs and email addresses) and style (e.g., cursive and slanted). Moreover, we propose a framework to analyze a recognizer that is trained with a mixture of real and synthetic training data. We use the framework to optimize data synthesis and demonstrate significant improvement on handwriting recognition over a model trained on real data only. Overall, we achieve a 66% reduction in Character Error Rate.",10.1109/icassp43922.2022.9746229
007d877a23a72b40a8614010b234d3f5d288fe79,"Building reproducible, reusable, and robust machine learning software",2020,"We have seen significant achievements with machine learning in recent years. Yet reproducing results for state-of-the-art deep learning methods is seldom straightforward. High variance of some methods can make learning particularly difficult. Furthermore, results can be brittle to even minor perturbations in the domain or experimental procedure. In this talk, I will review challenges that arise in experimental techniques and reporting procedures in deep learning, with a particular focus on reinforcement learning. I will also describe several recent results and guidelines designed to make future results more reproducible, reusable and robust.",10.1145/3401025.3407941
cb66bfd016126a2dde7c276d5d6858153ba00a21,Proof of Federated Training: Accountable Cross-Network Model Training and Inference,2022,"Blockchain has widely been adopted to design accountable federated learning frameworks; however, the existing frameworks do not scale for distributed model training over multiple independent blockchain networks. For storing the pre-trained models over blockchain, current approaches primarily embed a model using its structural properties that are neither scalable for cross-chain exchange nor suitable for cross-chain verification. This paper proposes an architectural framework for cross-chain verifiable model training using federated learning, called Proof of Federated Training (PoFT), the first of its kind that enables a federated training procedure span across the clients over multiple blockchain networks. Instead of structural embedding, PoFT uses model parameters to embed the model over a blockchain and then applies a verifiable model exchange between two blockchain networks for cross-network model training. We implement and test PoFT over a large-scale setup using Amazon EC2 instances and observe that cross-chain training can significantly boosts up the model efficacy. In contrast, PoFT incurs marginal overhead for inter-chain model exchanges.",10.48550/arXiv.2204.06919
69501ba1eb9ae00112e5747245b55c6f22b9211c,Can Offline Testing of Deep Neural Networks Replace Their Online Testing?,2021,"We distinguish two general modes of testing for Deep Neural Networks (DNNs): Offline testing where DNNs are tested as individual units based on test datasets obtained without involving the DNNs under test, and online testing where DNNs are embedded into a specific application environment and tested in a closed-loop mode in interaction with the application environment. Typically, DNNs are subjected to both types of testing during their development life cycle where offline testing is applied immediately after DNN training and online testing follows after offline testing and once a DNN is deployed within a specific application environment. In this paper, we study the relationship between offline and online testing. Our goal is to determine how offline testing and online testing differ or complement one another and if offline testing results can be used to help reduce the cost of online testing? Though these questions are generally relevant to all autonomous systems, we study them in the context of automated driving systems where, as study subjects, we use DNNs automating end-to-end controls of steering functions of self-driving vehicles. Our results show that offline testing is less effective than online testing as many safety violations identified by online testing could not be identified by offline testing, while large prediction errors generated by offline testing always led to severe safety violations detectable by online testing. Further, we cannot exploit offline testing results to reduce the cost of online testing in practice since we are not able to identify specific situations where offline testing could be as accurate as online testing in identifying safety requirement violations.",10.1007/s10664-021-09982-4
007f005ee74ca35825425df749dea9d794f0f18f,Integrative Object and Pose to Task Detection for an Augmented-Reality-based Human Assistance System using Neural Networks,2020,"As a result of an increasingly automatized and digitized industry, processes are becoming more complex. Augmented Reality has shown considerable potential in assisting workers with complex tasks by enhancing user understanding and experience with spatial information. However, the acceptance and integration of AR into industrial processes is still limited due to the lack of established methods and tedious integration efforts. Meanwhile, deep neural networks have achieved remarkable results in computer vision tasks and bear great prospects to enrich Augmented Reality applications. In this paper, we propose an Augmented-Reality-based human assistance system to assist workers in complex manual tasks where we incorporate deep neural networks for computer vision tasks. More specifically, we combine Augmented Reality with object and action detectors to make workflows more intuitive and flexible. To evaluate our system in terms of user acceptance and efficiency, we conducted several user studies. We found a significant reduction in time to task completion in untrained workers and a decrease in error rate. Furthermore, we investigated the users learning curve with our assistance system.",10.1109/ICCE48956.2021.9352121
709519a959a971b31054cd70ebaa1c28ef240dc9,Integrative human and object aware online progress observation for human-centric augmented reality assembly,2025,,10.1016/j.aei.2024.103081
008522610ebc5d2e4b49cbd6f2529dcc985c5cae,Linear Thompson Sampling Under Unknown Linear Constraints,2020,"We study how adding unknown linear safety constraints affects the performance of Thompson Sampling in the linear stochastic bandit problem. The additional constraints must be met at each round in spite of uncertainty about the environment requiring that the learner acts conservatively in choosing her actions. In this setting, we propose Safe-LTS, the first safe Thompson Sampling based algorithm, and we prove that it achieves no-regret learning. We obtain regrets that have the same dependence on the total number of rounds (modulo logarithmic factors) as Safe-UCB, a recently proposed safe algorithm that uses the upper confidence bound principle. Finally, we provide numerical simulations that demonstrate the efficacy of our algorithm.",10.1109/ICASSP40776.2020.9053865
74dde76f9734a76982c2fb4d2545c49ed32f2de5,Honor Among Bandits: No-Regret Learning for Online Fair Division,2024,"We consider the problem of online fair division of indivisible goods to players when there are a finite number of types of goods and player values are drawn from distributions with unknown means. Our goal is to maximize social welfare subject to allocating the goods fairly in expectation. When a player's value for an item is unknown at the time of allocation, we show that this problem reduces to a variant of (stochastic) multi-armed bandits, where there exists an arm for each player's value for each type of good. At each time step, we choose a distribution over arms which determines how the next item is allocated. We consider two sets of fairness constraints for this problem: envy-freeness in expectation and proportionality in expectation. Our main result is the design of an explore-then-commit algorithm that achieves $\tilde{O}(T^{2/3})$ regret while maintaining either fairness constraint. This result relies on unique properties fundamental to fair-division constraints that allow faster rates of learning, despite the restricted action space. We also prove a lower bound of $\tilde{\Omega}(T^{2/3})$ regret for our setting, showing that our results are tight.",10.48550/arXiv.2407.01795
22a05209ed4141bfb9bba694de53dde3c3b23d08,Convex Methods for Constrained Linear Bandits,2023,"Recently, bandit optimization has received significant attention in real-world safety-critical systems that involve repeated interactions with humans. While there exist various algorithms with performance guarantees in the literature, practical implementation of the algorithms has not received as much attention. This work presents a comprehensive study on the computational aspects of safe bandit algorithms, specifically safe linear bandits, by introducing a framework that leverages convex programming tools to create computationally efficient policies. In particular, we first characterize the properties of the optimal policy for safe linear bandit problem and then propose an end-to-end pipeline of safe linear bandit algorithms that only involves solving convex problems. We also numerically evaluate the performance of our proposed methods.",10.23919/ECC64448.2024.10591173
ec06ac71619ddd96b713b060e85399d56fdb8607,Collaborative Multi-agent Stochastic Linear Bandits,2022,"We study a collaborative multi-agent stochastic linear bandit setting, where N agents that form a network communicate locally to minimize their overall regret. In this setting, each agent has its own linear bandit problem (its own reward parameter) and the goal is to select the best global action w.r.t. the average of their reward parameters. At each round, each agent proposes an action, and one action is randomly selected and played as the network action. All the agents observe the corresponding rewards of the played action, and use an accelerated consensus procedure to compute an estimate of the average of the rewards obtained by all the agents. We propose a distributed upper confidence bound (UCB) algorithm and prove a high probability bound on its T-round regret in which we include a linear growth of regret associated with each communication round. Our regret bound is of order $\mathcal{O}\left( {\sqrt {\frac{T}{{N\log \left( {1/\left| {{\lambda _2}} \right|} \right)}}} \cdot {{\left( {\log T} \right)}^2}} \right)$, where λ2 is the second largest (in absolute value) eigenvalue of the communication matrix.",10.23919/ACC53348.2022.9867382
0087829d3e3f4eda0b29c266c94b08c3893e9a48,Emotion Recognition with Refined Labels for Deep Learning,2020,"The traditional emotion classification framework usually fits all the features segments of the same trial to a fixed annotation. Considering the fact that emotion is a reaction to stimuli that lasts for varied periods, we argue that the indiscriminate annotation is equivalent to taking the emotional state as fixed within the whole trial, leading to a decrease of the classification accuracy. In this study, we attempt to alleviate this issue by developing a thresholding scheme, converting the continuous emotional trace into a three-class annotation temporally. The features within a trial are therefore assigned to varied emotional states, resulting in an improvement in the accuracy. A long short term memory (LSTM) networks-based emotion classification framework is implemented, to which the proposed thresholding scheme is applied. A subset of MAHNOB-HCI dataset with continuous emotional annotation is used. The EEG signal and frontal facial video are used for feature extraction. The experiment results demonstrate that the proposed scheme provides statistically significant improvement to the three-class classification accuracy of the EEG feature-based LSTM network (p-value = 0.0329).",10.1109/EMBC44109.2020.9176111
c2f595e224428cfb7fdfafe8b6b5ae0901f69a07,MASA-TCN: Multi-Anchor Space-Aware Temporal Convolutional Neural Networks for Continuous and Discrete EEG Emotion Recognition,2023,"Emotion recognition from electroencephalogram (EEG) signals is a critical domain in biomedical research with applications ranging from mental disorder regulation to human-computer interaction. In this paper, we address two fundamental aspects of EEG emotion recognition: continuous regression of emotional states and discrete classification of emotions. While classification methods have garnered significant attention, regression methods remain relatively under-explored. To bridge this gap, we introduce MASA-TCN, a novel unified model that leverages the spatial learning capabilities of Temporal Convolutional Networks (TCNs) for EEG emotion regression and classification tasks. The key innovation lies in the introduction of a space-aware temporal layer, which empowers TCN to capture spatial relationships among EEG electrodes, enhancing its ability to discern nuanced emotional states. Additionally, we design a multi-anchor block with attentive fusion, enabling the model to adaptively learn dynamic temporal dependencies within the EEG signals. Experiments on two publicly available datasets show that MASA-TCN achieves higher results than the state-of-the-art methods for both EEG emotion regression and classification tasks.",10.1109/JBHI.2024.3392564
ae43b148a6578da4209c10580398d154bf37e2b9,Feature Pyramid Networks and Long Short-Term Memory for EEG Feature Map-Based Emotion Recognition,2023,"The original EEG data collected are the 1D sequence, which ignores spatial topology information; Feature Pyramid Networks (FPN) is better at small dimension target detection and insufficient feature extraction in the scale transformation than CNN. We propose a method of FPN and Long Short-Term Memory (FPN-LSTM) for EEG feature map-based emotion recognition. According to the spatial arrangement of brain electrodes, the Azimuth Equidistant Projection (AEP) is employed to generate the 2D EEG map, which preserves the spatial topology information; then, the average power, variance power, and standard deviation power of three frequency bands (α, β, and γ) are extracted as the feature data for the EEG feature map. BiCubic interpolation is employed to interpolate the blank pixel among the electrodes; the three frequency bands EEG feature maps are used as the G, R, and B channels to generate EEG feature maps. Then, we put forward the idea of distributing the weight proportion for channels, assign large weight to strong emotion correlation channels (AF3, F3, F7, FC5, and T7), and assign small weight to the others; the proposed FPN-LSTM is used on EEG feature maps for emotion recognition. The experiment results show that the proposed method can achieve Value and Arousal recognition rates of 90.05% and 90.84%, respectively.",10.3390/s23031622
01ca3855bc315a86f12097cf056bfcd79a7a1476,Improving the Performance of Deep Learning Model-Based Classification by the Analysis of Local Probability,2021,"Generally, the performance of deep learning-based classification models is highly related to the captured features of training samples. When a sample is not clear or contains a similar number of features of many objects, we cannot easily classify what it is. Actually, human beings classify objects by not only the features but also some information such as the probability of these objects in an environment. For example, when we know further information such as one object has a higher probability in the environment than the others, we can easily give the answer about what is in the sample. We call this kind of probability as local probability as this is related to the local environment. In this paper, we carried out a new framework that is named L-PDL to improve the performance of deep learning based on the analysis of this kind of local probability. Firstly, our method trains the deep learning model on the training set. Then, we can get the probability of objects on each sample by this trained model. Secondly, we get the posterior local probability of objects on the validation set. Finally, this probability conditionally cooperates with the probability of objects on testing samples. We select three popular deep learning models on three real datasets for the evaluation. The experimental results show that our method can obviously improve the performance on the real datasets, which is better than the state-of-the-art methods.",10.1155/2021/5534385
008a5805f0ad64b30a88b24c895bf6e0e792d58d,Automotive Radar Interference Reduction Based on Sparse Bayesian Learning,2020,"Automotive radar plays an important role in advanced driver assistant systems to support level-2 automated driving functions. However, the mutual interference between automotive radars increases due to the rising density of radars on the road. Therefore, the radar signal will be distorted to some extent and the performance of radars will degrade if no countermeasures are taken. In this paper, an interference mitigation approach using compressive sensing (CS) and Bayesian learning is introduced. By utilizing the sparsity of the beat signal in the frequency domain, the range-Doppler (RD) spectrum can be reconstructed with the help of undistorted samples in the beat signal. The sparse Bayesian learning method (SBL) is used to estimate the posterior of the signal's sparse representation and to infer the maximally sparse representation by using the Expectation- Maximization (EM) algorithm. It is shown that the SBL-based method has its advantages in signal-to-interference-plus-noise ratio (SINR) and target peak detection in comparison to conventional CS or classical signal reconstruction algorithms like linear predictive coding (LPC).",10.1109/RadarConf2043947.2020.9266706
008d409c4daaad89f5cc0fced8845d323a9f1109,Diagnosing Autism Using T1-W MRI With Multi-Kernel Learning and Hypergraph Neural Network,2020,"The field of network neuroscience provided unprecedented insights into how brain connectivity gets altered by autism spectrum disorder (ASD) on functional, structural, and morphological levels. However, a few studies have looked to design a framework that captures the complex network structure of the brain and disentangles the heterogeneity of ASD. In this paper, we leverage multi-kernel unsupervised learning in the construction of multiview hypergraph neural networks (HGNN), each capturing a particular view of the brain connectome, to eventually distinguish between ASD and normal control (NC) subjects. Additionally, we tested and measured how our proposed framework compares to other variants based on previous baseline methods. Our classification results outperformed comparison methods and agreed with the literature in the sense that the right hemisphere connectivity was more discriminative in ASD diagnosis than the left hemisphere.",10.1109/ICIP40778.2020.9190924
b528b5184b6191c6867d28e3d034645ec723274d,A Survey on Hypergraph Neural Networks: An In-Depth and Step-By-Step Guide,2024,"Higher-order interactions (HOIs) are ubiquitous in real-world complex systems and applications. Investigation of deep learning for HOIs, thus, has become a valuable agenda for the data mining and machine learning communities. As networks of HOIs are expressed mathematically as hypergraphs, hypergraph neural networks (HNNs) have emerged as a powerful tool for representation learning on hypergraphs. Given the emerging trend, we present the first survey dedicated to HNNs, with an in-depth and step-by-step guide. Broadly, the present survey overviews HNN architectures, training strategies, and applications. First, we break existing HNNs down into four design components: (i) input features, (ii) input structures, (iii) message-passing schemes, and (iv) training strategies. Second, we examine how HNNs address and learn HOIs with each of their components. Third, we overview the recent applications of HNNs in recommendation, bioinformatics and medical science, time series analysis, and computer vision. Lastly, we conclude with a discussion on limitations and future directions.",10.1145/3637528.3671457
00969b4dcf8f9b21895bd038a51a038018da84f0,How Well Do Self-Supervised Models Transfer?,2020,"Self-supervised visual representation learning has seen huge progress recently, but no large scale evaluation has compared the many models now available. We evaluate the transfer performance of 13 top self-supervised models on 40 downstream tasks, including many-shot and few-shot recognition, object detection, and dense prediction. We compare their performance to a supervised baseline and show that on most tasks the best self-supervised models outperform supervision, confirming the recently observed trend in the literature. We find ImageNet Top-1 accuracy to be highly correlated with transfer to many-shot recognition, but increasingly less so for few-shot, object detection and dense prediction. No single self-supervised method dominates overall, suggesting that universal pre-training is still unsolved. Our analysis of features suggests that top self-supervised learners fail to preserve colour information as well as supervised alternatives, but tend to induce better classifier calibration, and less attentive overfitting than supervised learners.",10.1109/CVPR46437.2021.00537
009a5f6d6b455b5af325f6853b94b881d9acaabc,Uncertainty-Wizard: Fast and User-Friendly Neural Network Uncertainty Quantification,2020,"Uncertainty and confidence have been shown to be useful metrics in a wide variety of techniques proposed for deep learning testing, including test data selection and system supervision. We present Uncertainty-Wizard, a tool that allows to quantify such uncertainty and confidence in artificial neural networks. It is built on top of the industry-leading TF.KERAS deep learning API and it provides a near-transparent and easy to understand interface. At the same time, it includes major performance optimizations that we benchmarked on two different machines and different configurations.",10.1109/ICST49551.2021.00056
876e1072eaefd526bd9cdbdaca6797c7062ad704,Predicting Safety Misbehaviours in Autonomous Driving Systems Using Uncertainty Quantification,2024,"The automated real-time recognition of unexpected situations plays a crucial role in the safety of autonomous vehicles, especially in unsupported and unpredictable scenarios. This paper evaluates different Bayesian uncertainty quantification methods from the deep learning domain for the anticipa-tory testing of safety-critical misbehaviours during system-level simulation-based testing. Specifically, we compute uncertainty scores as the vehicle executes, following the intuition that high uncertainty scores are indicative of unsupported runtime conditions that can be used to distinguish safe from failure-inducing driving behaviors. In our study, we conducted an evaluation of the effectiveness and computational overhead associated with two Bayesian uncertainty quantification methods, namely Me-Dropout and Deep Ensembles, for misbehaviour avoidance. Over-all, for three benchmarks from the Udacity simulator comprising both out-of-distribution and unsafe conditions introduced via mutation testing, both methods successfully detected a high number of out-of-bounds episodes providing early warnings several seconds in advance, outperforming two state-of-the-art misbehaviour prediction methods based on autoencoders and attention maps in terms of effectiveness and efficiency. Notably, Deep Ensembles detected most misbehaviours without any false alarms and did so even when employing a relatively small number of models, making them computationally feasible for real-time detection. Our findings suggest that incorporating uncertainty quantification methods is a viable approach for building fail-safe mechanisms in deep neural network-based autonomous vehicles.",10.1109/ICST60714.2024.00016
eb6f92b6a6739a2563591e47c98d67b7298d4c04,Evolve the Model Universe of a System Universe,2023,"Uncertain, unpredictable, real-time, and lifelong evolution causes operational failures in intelligent software systems, leading to significant damages, safety and security hazards, and tragedies. To fully unleash such systems' potential and facilitate their wider adoption, ensuring the trustworthiness of their decision-making under uncertainty is the prime challenge. To overcome this challenge, an intelligent software system and its operating environment should be continuously monitored, tested, and refined during its lifetime operation. Existing technologies, such as digital twins, can enable continuous synchronisation with such systems to reflect their most up-to-date states. Such representations are often in the form of prior-knowledge-based and machine-learning models, together called ‘model universe’. In this paper, we present our vision of combining techniques from software engineering, evolutionary computation, and machine learning to support the model universe evolution.",10.1109/ASE56229.2023.00022
a9ca47de8b490f94ad027796e2db816183152d23,Building an open-source system test generation tool: lessons learned and empirical analyses with EvoMaster,2023,"Research in software testing often involves the development of software prototypes. Like any piece of software, there are challenges in the development, use and verification of such tools. However, some challenges are rather specific to this problem domain. For example, often these tools are developed by PhD students straight out of bachelor/master degrees, possibly lacking any industrial experience in software development. Prototype tools are used to carry out empirical studies, possibly studying different parameters of novel designed algorithms. Software scaffolding is needed to run large sets of experiments efficiently. Furthermore, when using AI-based techniques like evolutionary algorithms, care needs to be taken to deal with their randomness, which further complicates their verification. The aforementioned represent some of the challenges we have identified for this domain. In this paper, we report on our experience in building the open-source EvoMaster tool, which aims at system-level test case generation for enterprise applications. Many of the challenges we faced would be common to any researcher needing to build software testing tool prototypes. Therefore, one goal is that our shared experience here will boost the research community, by providing concrete solutions to many development challenges in the building of such kind of research prototypes. Ultimately, this will lead to increase the impact of scientific research on industrial practice.",10.1007/s11219-023-09620-w
20209377ff14381d2b106da6774e40457c4ff01a,Assessing operational accuracy of CNN-based image classifiers using an oracle surrogate,2023,,10.1016/j.iswa.2022.200172
1313fa894fd1055e69e07c9eee284597161e793e,Uncertainty quantification for deep neural networks: An empirical comparison and usage guidelines,2022,"Deep neural networks (DNN) are increasingly used as components of larger software systems that need to process complex data, such as images, written texts, audio/video signals. DNN predictions cannot be assumed to be always correct for several reasons, amongst which the huge input space that is dealt with, the ambiguity of some inputs data, as well as the intrinsic properties of learning algorithms, which can provide only statistical warranties. Hence, developers have to cope with some residual error probability. An architectural pattern commonly adopted to manage failure prone components is the supervisor, an additional component that can estimate the reliability of the predictions made by untrusted (e.g., DNN) components and can activate an automated healing procedure when these are likely to fail, ensuring that the deep learning‐based system (DLS) does not cause damages, despite its main functionality being suspended.",10.1002/stvr.1840
8b90037b8ac73fd57082b38ee1e07635aa6eaf14,Research on English Classroom Teaching Programs in Colleges and Universities Based on Wireless Communication Technology Support in the Context of 5G,2024,"The purpose of this article is to investigate how 5G and wireless communication technologies (5G+WCT) might be applied to English language classroom programs in higher education. The paper describes the complimentary roles of 5G and wireless communication technologies in English language teaching, includes student data collecting and standardization as part of the pre-processing, and optimizes the teaching system using an enhanced ant colony algorithm (BACO). Furthermore, the thesis delineates diverse approaches to verify and assess the program's efficacy. Through the creation of cutting-edge instructional strategies and data collection techniques, this study fosters innovation in English language instruction in higher education. The study offers promise for real-world application and is significant for raising student success, instructional quality, and educational efficacy. However, since 5G technology is still developing, the results of this study could not be applicable in the future or would need to be updated.",10.4018/ijicte.339202
009ba446176d2103d8733bd05a013b151c77de39,A Framework for Online Updates to Safe Sets for Uncertain Dynamics,2020,"Safety is crucial for deploying robots in the real world. One way of reasoning about safety of robots is by building safe sets through Hamilton-Jacobi (HJ) reachability. However, safe sets are often computed offline, assuming perfect knowledge of the dynamics, due to high compute time. In the presence of uncertainty, the safe set computed offline becomes inaccurate online, potentially leading to dangerous situations on the robot. We propose a novel framework to learn a safe control policy in simulation, and use it to generate online safe sets under uncertain dynamics. We start with a conservative safe set and update it online as we gather more information about the robot dynamics. We also show an application of our framework to a model-based reinforcement learning problem, proposing a safe model-based RL setup. Our framework enables robots to simultaneously learn about their dynamics, accomplish tasks, and update their safe sets. It also generalizes to complex high-dimensional dynamical systems, like 3-link manipulators and quadrotors, and reliably avoids obstacles, while achieving a task, even in the presence of unmodeled noise.",10.1109/IROS45743.2020.9341606
40b1162937a2b748f00ec6e38d8b70290ebe8fed,Safe Learning-based Predictive Control from Efficient Reachability,2023,"We consider a dynamical system subject to a disturbance input that is an unknown function of the state. Given a target goal region, we propose a control scheme that encourages exploration of the state space in order to sample the dynamics and obtain an estimate of the unknown component while avoiding unsafe regions of the state space until the goal is able to be reached with high probability. By estimating the unknown component as a Gaussian process, we efficiently obtain hyperrectangular overapproximations of the reachable set for the system using the theory of mixed monotone systems, and these sets are improved over time as measurements of the dynamics are collected. Using these reachability estimates, we propose a model predictive scheme that avoids the unsafe region and ensures the system is always within reach of a conservative, guaranteed safe region that is given a priori, thus always ensuring feasibility until the goal is reachable. We demonstrate the approach on a model of an autonomous vehicle operating on an icy road and on a planar multirotor moving in an unknown wind field.",10.23919/ACC55779.2023.10156298
4a065e21eee4197aaeef4acf868c4886c185d84f,Learning-based Initialization Strategy for Safety of Multi-Vehicle Systems,2021,"Multi-vehicle collision avoidance is a highly crucial problem due to the soaring interests of introducing autonomous vehicles into the real world in recent years. The safety of these vehicles while they complete their objectives is of paramount importance. Hamilton-Jacobi (HJ) reachability is a promising tool for guaranteeing safety for low-dimensional systems. However, due to its exponential complexity in computation time, no reachability-based methods have been able to guarantee safety for more than three vehicles successfully in unstructured scenarios. For systems with four or more vehicles, we can only empirically validate their safety performance. While reachability-based safety methods enjoy a flexible least-restrictive control strategy, it is challenging to reason about long-horizon trajectories online because safety at any given state is determined by looking up the safety value in a pre-computed table that does not exhibit favorable properties that continuous functions have. This motivates the problem of improving the safety performance of unstructured multi-vehicle systems when safety cannot be guaranteed given any least-restrictive safety-aware collision avoidance algorithm while avoiding online trajectory optimization. In this paper, we propose a novel approach using supervised learning to enhance the safety of vehicles by proposing new initial states in very close neighborhood of the original initial states of vehicles. Our experiments demonstrate the effectiveness of our proposed approach and show that vehicles are able to get to their goals with better safety performance with our approach compared to a baseline approach in wide-ranging scenarios.",10.23919/ACC53348.2022.9867872
824727f9e1f7461f8947966e554abf7841afb10d,Safe Value Functions,2021,"Safety constraints and optimality are important but sometimes conflicting criteria for controllers. Although these criteria are often solved separately with different tools to maintain formal guarantees, it is also common practice in reinforcement learning (RL) to simply modify reward functions by penalizing failures, with the penalty treated as a mere heuristic. We rigorously examine the relationship of both safety and optimality to penalties, and formalize sufficient conditions for safe value functions (SVFs): value functions that are both optimal for a given task, and enforce safety constraints. We reveal this structure by examining when rewards preserve viability under optimal control, and show that there always exists a finite penalty that induces an SVF. This penalty is not unique, but upper-unbounded: larger penalties do not harm optimality. Although it is often not possible to compute the minimum required penalty, we reveal clear structure of how the penalty, rewards, discount factor, and dynamics interact. This insight suggests practical, theory-guided heuristics to design reward functions for control problems where safety is important.",10.1109/TAC.2022.3200948
53bb28b81f5b400423945ad1851d86b08851bf96,Analysing the effectiveness of online digital audio software and offline audio studios in fostering Chinese folk music composition skills in music education,2024,"
Object: This study was designed to compare the effectiveness of online digital audio software Logic Pro X and a university offline audio studio in terms of the perspective and quality of the created tracks.
Results: The empirical results indicated that there was no significant difference between the two groups in either perspectives or expert assessments (p ≥ 0.05). These findings can help design appropriate music curricula and integrate digital technologies into music education.
Practical implications: The findings have practical implications for music teachers and educational institutions in China and other countries; they also provide scientific evidence to support the use of digital technologies in music education.
",10.1111/jcal.13031
00a0af9100a2c5f5f9bab91dc51cec538be767f1,McFlow: Monte Carlo Flow Models for Data Imputation,2020,"We consider the topic of data imputation, a foundational task in machine learning that addresses issues with missing data. To that end, we propose MCFlow, a deep framework for imputation that leverages normalizing flow generative models and Monte Carlo sampling. We address the causality dilemma that arises when training models with incomplete data by introducing an iterative learning scheme which alternately updates the density estimate and the values of the missing entries in the training data. We provide extensive empirical validation of the effectiveness of the proposed method on standard multivariate and image datasets, and benchmark its performance against state-of-the-art alternatives. We demonstrate that MCFlow is superior to competing methods in terms of the quality of the imputed data, as well as with regards to its ability to preserve the semantic structure of the data.",10.1109/cvpr42600.2020.01421
00a5d564d83c79ea0f39000d10ad618156da66a2,Active exploration for body model learning through self-touch on a humanoid robot with artificial skin,2020,"The mechanisms of infant development are far from understood. Learning about one's own body is likely a foundation for subsequent development. Here we look specifically at the problem of how spontaneous touches to the body in early infancy may give rise to first body models and bootstrap further development such as reaching competence. Unlike visually elicited reaching, reaching to own body requires connections of the tactile and motor space only, bypassing vision. Still, the problems of high dimensionality and redundancy of the motor system persist. In this work, we present an embodied computational model on a simulated humanoid robot with artificial sensitive skin on large areas of its body. The robot should autonomously develop the capacity to reach for every tactile sensor on its body. To do this efficiently, we employ the computational framework of intrinsic motivations and variants of goal babbling-as opposed to motor babbling-that prove to make the exploration process faster and alleviate the ill-posedness of learning inverse kinematics. Based on our results, we discuss the next steps in relation to infant studies: what information will be necessary to further ground this computational model in behavioral data.",10.1109/ICDL-EpiRob48136.2020.9278035
47520150611da85cf67442fccd82c715bd9f99a3,Goal-Directed Tactile Exploration for Body Model Learning Through Self-Touch on a Humanoid Robot,2023,"An early integration of tactile sensing into motor coordination is the norm in animals, but still a challenge for robots. Tactile exploration through touches on the body gives rise to first body models and bootstraps further development such as reaching competence. Reaching to one’s own body requires connections of the tactile and motor space only. Still, the problems of high dimensionality and motor redundancy persist. Through an embodied computational model for the learning of self-touch on a simulated humanoid robot with artificial sensitive skin, we demonstrate that this task can be achieved 1) effectively and 2) efficiently at scale by employing the computational frameworks for the learning of internal models for reaching: intrinsic motivation and goal babbling. We relate our results to infant studies on spontaneous body exploration as well as reaching to vibrotactile targets on the body. We analyze the reaching configurations of one infant followed weekly between 4 and 18 months of age and derive further requirements for the computational model: accounting for 3) continuous rather than sporadic touch and 4) consistent redundancy resolution. Results show the general success of the learning models in the touch domain, but also point out limitations in achieving fully continuous touch.",10.1109/TCDS.2021.3104881
bc8e0dc4d29665977ff3dedddb8ed7d416012b99,Toward Creative Problem Solving Agents: Action Discovery through Behavior Babbling,2021,"Creative problem solving (CPS) is the process by which an agent discovers unknown information about itself and its environment, allowing it to accomplish a previously impossible goal. We propose a framework for CPS by robots for discovering novel actions via behavior babbling, capable of learning a representation of novel actions at both a symbolic planning level, and a sub-symbolic action controller level. Our framework employs two modes of discovery – a focused incubation method that scopes its search to the actions and entities composing the failed plan, and a defocused incubation method which enables exploration of actions and entities outside of the failed plan. We implemented and tested our framework using a Baxter robot in a 3D physics-based simulation environment, where we ran three proof-of-concept object manipulation scenarios. Results suggest that it is possible to use behavior babbling as a method for the autonomous discovery of flexible and reusable actions.",10.1109/ICDL49984.2021.9515658
48b2c146fcc3646534dc824169d5ce6a79a27eb9,Computational models of the “active self” and its disturbances in schizophrenia,2021,,10.1016/j.concog.2021.103155
00a656134e13d82bf96fb23ac7c71d2a489bb85e,A Multi-Task Mean Teacher for Semi-Supervised Shadow Detection,2020,"Existing shadow detection methods suffer from an intrinsic limitation in relying on limited labeled datasets, and they may produce poor results in some complicated situations. To boost the shadow detection performance, this paper presents a multi-task mean teacher model for semi-supervised shadow detection by leveraging unlabeled data and exploring the learning of multiple information of shadows simultaneously. To be specific, we first build a multi-task baseline model to simultaneously detect shadow regions, shadow edges, and shadow count by leveraging their complementary information and assign this baseline model to the student and teacher network. After that, we encourage the predictions of the three tasks from the student and teacher networks to be consistent for computing a consistency loss on unlabeled data, which is then added to the supervised loss on the labeled data from the predictions of the multi-task baseline model. Experimental results on three widely-used benchmark datasets show that our method consistently outperforms all the compared state-of- the-art methods, which verifies that the proposed network can effectively leverage additional unlabeled data to boost the shadow detection performance.",10.1109/CVPR42600.2020.00565
00a6ee03c2a8d22644513f1e983d5159197b201a,Ego-Topo: Environment Affordances From Egocentric Video,2020,"First-person video naturally brings the use of a physical environment to the forefront, since it shows the camera wearer interacting fluidly in a space based on his intentions. However, current methods largely separate the observed actions from the persistent space itself. We introduce a model for environment affordances that is learned directly from egocentric video. The main idea is to gain a human-centric model of a physical space (such as a kitchen) that captures (1) the primary spatial zones of interaction and (2) the likely activities they support. Our approach decomposes a space into a topological map derived from first-person activity, organizing an ego-video into a series of visits to the different zones. Further, we show how to link zones across multiple related environments (e.g., from videos of multiple kitchens) to obtain a consolidated representation of environment functionality. On EPIC-Kitchens and EGTEA+, we demonstrate our approach for learning scene affordances and anticipating future actions in long-form video.",10.1109/CVPR42600.2020.00024
2bfc1bb68b39f880bf927d49b7674bfed0a963d5,Topological SLAM in Colonoscopies Leveraging Deep Features and Topological Priors,2024,"We introduce ColonSLAM, a system that combines classical multiple-map metric SLAM with deep features and topological priors to create topological maps of the whole colon. The SLAM pipeline by itself is able to create disconnected individual metric submaps representing locations from short video subsections of the colon, but is not able to merge covisible submaps due to deformations and the limited performance of the SIFT descriptor in the medical domain. ColonSLAM is guided by topological priors and combines a deep localization network trained to distinguish if two images come from the same place or not and the soft verification of a transformer-based matching network, being able to relate far-in-time submaps during an exploration, grouping them in nodes imaging the same colon place, building more complex maps than any other approach in the literature. We demonstrate our approach in the Endomapper dataset, showing its potential for producing maps of the whole colon in real human explorations. Code and models are available at: https://github.com/endomapper/ColonSLAM.",10.1007/978-3-031-72120-5_68
00b2c98c83799c469245d4ec4588f5bbf8b7c256,Open-Set Metric Learning For Person Re-Identification In The Wild,2020,"Person re-identification in the wild needs to simultaneously (frame-wise) detect and re-identify persons and has wide utility in practical scenarios. However, such tasks come with an additional open-set re-ID challenge as all probe persons may not necessarily be present in the (frame-wise) dynamic gallery. Traditional or close-set re-ID systems are not equipped to handle such cases and raise several false alarms as a result. To cope with such challenges open-set metric learning (OSML), based on the concept of Large margin nearest neighbor (LMNN) approach, is proposed. We term our method Open-Set LMNN (OS-LMNN). The goal of separating impostor samples from the genuine samples is achieved through a joint optimization of the Weibull distribution and the Mahalanobis metric learned through this OS-LMNN approach. The rejection is performed based on low probability over distance of imposter pairs. Exhaustive experiments with other metric learning techniques over the publicly available PRW dataset clearly demonstrate the robustness of our approach.",10.1109/ICIP40778.2020.9190744
b9844447a7ca09cdb166b1b918ec87980afdf359,HMMN: Online metric learning for human re-identification via hard sample mining memory network,2021,,10.1016/j.engappai.2021.104489
be4b8c00404c61f25dd735a0c9e02fe5a6400fca,Person re-identification based on metric learning: a survey,2021,,10.1007/s11042-021-10953-6
00b30ed463625da04166eb78ca617539b41a9846,jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models,2020,"We introduce jiant, an open source toolkit for conducting multitask and transfer learning experiments on English NLU tasks. jiant enables modular and configuration driven experimentation with state-of-the-art models and a broad set of tasks for probing, transfer learning, and multitask training experiments. jiant implements over 50 NLU tasks, including all GLUE and SuperGLUE benchmark tasks. We demonstrate that jiant reproduces published performance on a variety of tasks and models, e.g., RoBERTa and BERT.",10.18653/v1/2020.acl-demos.15
212d41449bc2133537234d75cfc2bad9c7a9370a,Paraphrasing in Affirmative Terms Improves Negation Understanding,2024,"Negation is a common linguistic phenomenon. Yet language models face challenges with negation in many natural language understanding tasks such as question answering and natural language inference. In this paper, we experiment with seamless strategies that incorporate affirmative interpretations (i.e., paraphrases without negation) to make models more robust against negation. Crucially, our affirmative interpretations are obtained automatically. We show improvements with CondaQA, a large corpus requiring reasoning with negation, and five natural language understanding tasks.",10.48550/arXiv.2406.07492
a3262800242b9cfa8f8f4ecb058c01630bbaf727,Hate Cannot Drive out Hate: Forecasting Conversation Incivility following Replies to Hate Speech,2023,"User-generated counter hate speech is a promising means to combat hate speech, but questions about whether it can stop incivility in follow-up conversations linger. We argue that effective counter hate speech stops incivility from emerging in follow-up conversations—counter hate that elicits more incivility is counterproductive. This study introduces the task of predicting the incivility of conversations following replies to hate speech. We first propose a metric to measure conversation incivility based on the number of civil and uncivil comments as well as the unique authors involved in the discourse. Our metric approximates human judgments more accurately than previous metrics. We then use the metric to evaluate the outcomes of replies to hate speech. A linguistic analysis uncovers the differences in the language of replies that elicit follow-up conversations with high and low incivility. Experimental results show that forecasting incivility is challenging. We close with a qualitative analysis shedding light into the most common errors made by the best model.",10.48550/arXiv.2312.04804
00b37f80a049a4cf1693a0614b27f438a7c86cde,The Pipeline Process of Signature-based and Behavior-based Malware Detection,2020,"There is a mushroom growth of malware which has caused a serious threat towards computer software and the internet. The number of malware is increasing with each passing day. There are two methods to deal with malware detection namely signature-based malware detection and behavior-based malware detection. Both methods have their advantages and disadvantages. In this paper, the pipeline process of both signature-based malware detection and behavior-based malware detection is explained. This will help researchers to understand these techniques in a detailed manner. In addition to this, an experiment is performed in which a dataset of 1494 malware and 1347 benign samples is collected. Then two kinds of features are extracted from these samples one is string feature for static analysis and one is nonrepetitive consecutive API calls for dynamic analysis. Then accuracy is calculated by using various machine learning classifiers like k-Nearest Neighbors, Gaussian Naive Bayes, Multi Naive Bayes, Decision Tree, Support Vector Machine and Random Forest.",10.1109/ICCCA49541.2020.9250879
00b677e971ded11ac4a7da1b80ffda95b4f1ed78,Pre-Training Transformers as Energy-Based Cloze Models,2020,"We introduce Electric, an energy-based cloze model for representation learning over text. Like BERT, it is a conditional generative model of tokens given their contexts. However, Electric does not use masking or output a full distribution over tokens that could occur in a context. Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context. We train Electric using an algorithm based on noise-contrastive estimation and elucidate how this learning objective is closely related to the recently proposed ELECTRA pre-training method. Electric performs well when transferred to downstream tasks and is particularly effective at producing likelihood scores for text: it re-ranks speech recognition n-best lists better than language models and much faster than masked language models. Furthermore, it offers a clearer and more principled view of what ELECTRA learns during pre-training.",10.18653/v1/2020.emnlp-main.20
d5a772bb21ffefebfe13552ea20d65e12e78d334,Unified Interpretation of Smoothing Methods for Negative Sampling Loss Functions in Knowledge Graph Embedding,2024,"Knowledge Graphs (KGs) are fundamental resources in knowledge-intensive tasks in NLP. Due to the limitation of manually creating KGs, KG Completion (KGC) has an important role in automatically completing KGs by scoring their links with KG Embedding (KGE). To handle many entities in training, KGE relies on Negative Sampling (NS) loss that can reduce the computational cost by sampling. Since the appearance frequencies for each link are at most one in KGs, sparsity is an essential and inevitable problem. The NS loss is no exception. As a solution, the NS loss in KGE relies on smoothing methods like Self-Adversarial Negative Sampling (SANS) and subsampling. However, it is uncertain what kind of smoothing method is suitable for this purpose due to the lack of theoretical understanding. This paper provides theoretical interpretations of the smoothing methods for the NS loss in KGE and induces a new NS loss, Triplet Adaptive Negative Sampling (TANS), that can cover the characteristics of the conventional smoothing methods. Experimental results of TransE, DistMult, ComplEx, RotatE, HAKE, and HousE on FB15k-237, WN18RR, and YAGO3-10 datasets and their sparser subsets show the soundness of our interpretation and performance improvement by our TANS.",10.48550/arXiv.2407.04251
00b6adb2eea6ddb6706e4d5056f8a249449b74e9,Real-Time Powerline Detection System for an Unmanned Aircraft System,2020,"This paper will explore the development of a powerline detection system using Deep Learning to autonomously recognize powerlines in real-time using an Unmanned Aerial System. Additionally, the detection system can identify the individual components of the powerline and electric utility pole such as the cross-arms, insulators, transformers, and primary wires. This proposed model has fast recognition speed and high accuracy, which allows the Unmanned Aerial System to inspect quickly; decreasing both time and cost and increasing safety. To achieve this the Real-Time Powerline Detection System leverages the capability of the YOLACT algorithm, a recently proposed real-time instance segmentation model that achieves 29.8 minimum average precision on the MSCOCO Dataset at 33.5 fps. The results using this approach is promising and perform much better when compared to the Mask R-CNN algorithm. This paper demonstrates the application and results of the Real-Time Powerline Detection System in combination with the YOLACT algorithm for detecting individual components of the powerline using instance segmentation.",10.1109/SMC42975.2020.9283354
2ec579748905bcefcb71ae0435b4eedf5bca5554,Object detection in power line infrastructure: A review of the challenges and solutions,2024,,10.1016/j.engappai.2023.107781
eb84a3ad9e8b923569e0ee38a2f8d1eff8c1331b,Decision support system for real-time segmentation and identification algorithm for wires in mobile terminals using fuzzy AHP method,2022,,10.1007/s00500-022-07197-3
125fb1a4ea5c3dba2d7c35e1218a955a231e0d62,Autonomous UAV System for Cleaning Insulators in Power Line Inspection and Maintenance,2021,"The inspection and maintenance tasks of electrical installations are very demanding. Nowadays, insulator cleaning is carried out manually by operators using scaffolds, ropes, or even helicopters. However, these operations involve potential risks for humans and the electrical structure. The use of Unmanned Aerial Vehicles (UAV) to reduce the risk of these tasks is rising. This paper presents an UAV to autonomously clean insulators on power lines. First, an insulator detection and tracking algorithm has been implemented to control the UAV in operation. Second, a cleaning tool has been designed consisting of a pump, a tank, and an arm to direct the flow of cleaning liquid. Third, a vision system has been developed that is capable of detecting soiled areas using a semantic segmentation neuronal network, calculating the trajectory for cleaning in the image plane, and generating arm trajectories to efficiently clean the insulator. Fourth, an autonomous system has been developed to land on a charging pad to charge the batteries and potentially fill the tank with cleaning liquid. Finally, the autonomous system has been validated in a controlled outdoor environment.",10.3390/s21248488
00b931f5703cbfb74582d8c6fdd52ae202629ee4,Machine Learning Algorithms in Quantum Computing: A Survey,2020,"Machine Learning (ML) aims at designing models that learn from previous experience, without being explicitly formulated. Applications of machine learning are inexhaustible, including recognizing patterns, predicting future trends and making decisions, and they are capable of handling sizable quantities of multi-dimensional data in the form of large vectors and tensors. To perform these operations on classical computers, however, requires vast time and computational resources. Unlike the classical computers that rely on computations using binary bits, Quantum Computers (QC) benefit from qubits which can hold combinations of 0 and 1 at the same time via superposition and entanglement. This makes QCs powerful at handling and post processing large tensors, making them a prime target for implementing ML algorithms. While several models used for ML on QCs are based on concepts from their classical computing counterparts, utilization of the QC’s potential has made them the superior of the two. This paper presents an overview of the current state of knowledge in application of ML on QC, and evaluates the speed up, and complexity advantages of using quantum machines.",10.1109/IJCNN48605.2020.9207714
29aed170c8225ceea1203dd68562d49bc791ca52,Medical supervised masked autoencoder: Crafting a better masking strategy and efficient fine-tuning schedule for medical image classification,2024,,10.1016/j.asoc.2024.112536
00c4187e801e0a68401ce1d73e00afa62eae6829,Smart Irrigation IoT Solution using Transfer Learning for Neural Networks,2020,"In this paper we develop a reliable system for smart irrigation of greenhouses using artificial neural networks, and an IoT architecture. Our solution uses four sensors in different layers of soil to predict future moisture. Using a dataset we collected by running experiments on different soils, we show high performance of neural networks compared to existing alternative method of support vector regression. To reduce the processing power of neural network for the IoT edge devices, we propose using transfer learning. Transfer learning also speeds up training performance with small amount of training data, and allows integrating climate sensors to a pre-trained model, which are the other two challenges of smart irrigation of greenhouses. Our proposed IoT architecture shows a complete solution for smart irrigation.",10.1109/ICCKE50421.2020.9303612
0fd0896dd9768ae783789ea70ec3adff0bea4e57,Indoor Positioning via Gradient Boosting Enhanced with Feature Augmentation using Deep Learning,2022,"With the emerge of the Internet of Things (IoT), localization within indoor environments has become inevitable and has attracted a great deal of attention in recent years. Several efforts have been made to cope with the challenges of accurate positioning systems in the presence of signal interference. In this paper, we propose a novel deep learning approach through Gradient Boosting Enhanced with Step-Wise Feature Augmentation using Artificial Neural Network (AugBoost-ANN) for indoor localization applications as it trains over labeled data. For this purpose, we propose an IoT architecture using a star network topology to collect the Received Signal Strength Indicator (RSSI) of Bluetooth Low Energy (BLE) modules by means of a Raspberry Pi as an Access Point (AP) in an indoor environment. The dataset for the experiments is gathered in the real world in different periods to match the real environments. Next, we address the challenges of the AugBoost-ANN training which augments features in each iteration of making a decision tree using a deep neural network and the transfer learning technique. Experimental results show more than 8% improvement in terms of accuracy in comparison with the existing gradient boosting and deep learning methods recently proposed in the literature, and our proposed model acquires a mean location accuracy of 0.77 m.",10.1109/VTC2022-Spring54318.2022.9860759
00cad0201287c7ff08c6bcf1a9be2425321c645c,Simultaneous Translation,2020,"Simultaneous translation, which performs translation concurrently with the source speech, is widely useful in many scenarios such as international conferences, negotiations, press releases, legal proceedings, and medicine. This problem has long been considered one of the hardest problems in AI and one of its holy grails. Recently, with rapid improvements in machine translation, speech recognition, and speech synthesis, there has been exciting progress towards simultaneous translation. This tutorial will focus on the design and evaluation of policies for simultaneous translation, to leave attendees with a deep technical understanding of the history, the recent advances, and the remaining challenges in this field.",10.18653/v1/2020.emnlp-tutorials.6
0c915fa4f80ddece05b798d586d732862a3b48a2,MeetDot: Videoconferencing with Live Translation Captions,2021,"We present MeetDot, a videoconferencing system with live translation captions overlaid on screen. The system aims to facilitate conversation between people who speak different languages, thereby reducing communication barriers between multilingual participants. Currently, our system supports speech and captions in 4 languages and combines automatic speech recognition (ASR) and machine translation (MT) in a cascade. We use the re-translation strategy to translate the streamed speech, resulting in caption flicker. Additionally, our system has very strict latency requirements to have acceptable call quality. We implement several features to enhance user experience and reduce their cognitive load, such as smooth scrolling captions and reducing caption flicker. The modular architecture allows us to integrate different ASR and MT services in our backend. Our system provides an integrated evaluation suite to optimize key intrinsic evaluation metrics such as accuracy, latency and erasure. Finally, we present an innovative cross-lingual word-guessing game as an extrinsic evaluation metric to measure end-to-end system performance. We plan to make our system open-source for research purposes.",10.18653/v1/2021.emnlp-demo.23
00cb286010ebbc394854cf4abdad1ee5ab00ac93,Deep Depth Estimation from Visual-Inertial SLAM,2020,"This paper addresses the problem of learning to complete a scene’s depth from sparse depth points and images of indoor scenes. Specifically, we study the case in which the sparse depth is computed from a visual-inertial simultaneous localization and mapping (VI-SLAM) system. The resulting point cloud has low density, it is noisy, and has nonuniform spatial distribution, as compared to the input from active depth sensors, e.g., LiDAR or Kinect. Since the VI-SLAM produces point clouds only over textured areas, we compensate for the missing depth of the low-texture surfaces by leveraging their planar structures and their surface normals which is an important intermediate representation. The pre-trained surface normal network, however, suffers from large performance degradation when there is a significant difference in the viewing direction (especially the roll angle) of the test image as compared to the trained ones. To address this limitation, we use the available gravity estimate from the VI-SLAM to warp the input image to the orientation prevailing in the training dataset. This results in a significant performance gain for the surface normal estimate, and thus the dense depth estimates. Finally, we show that our method outperforms other state-of-the-art approaches both on training (ScanNet [1] and NYUv2 [2]) and testing (collected with Azure Kinect [3]) datasets.",10.1109/IROS45743.2020.9341448
52bb2cce78284228bfe33d3c0b0566b9057dc45a,RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned Metric Scale,2024,"We present a novel approach for metric dense depth estimation based on the fusion of a single-view image and a sparse, noisy Radar point cloud. The direct fusion of heterogeneous Radar and image data, or their encodings, tends to yield dense depth maps with significant artifacts, blurred boundaries, and suboptimal accuracy. To circumvent this issue, we learn to augment versatile and robust monocular depth prediction with the dense metric scale induced from sparse and noisy Radar data. We propose a Radar-Camera framework for highly accurate and fine-detailed dense depth estimation with four stages, including monocular depth prediction, global scale alignment of monocular depth with sparse Radar points, quasi-dense scale estimation through learning the association between Radar points and image patches, and local scale refinement of dense depth using a scale map learner. Our proposed method significantly outperforms the state-of-the-art Radar-Camera depth estimation methods by reducing the mean absolute error (MAE) of depth estimation by 25.6% and 40.2% on the challenging nuScenes dataset and our self-collected ZJU-4DRadarCam dataset, respectively. Our code and dataset will be released at https://github.com/MMOCKING/RadarCam-Depth.",10.1109/ICRA57147.2024.10610929
8f4f2656338f4b7e8da367f15814c0be15ca8982,SiTAR: Situated Trajectory Analysis for In-the-Wild Pose Error Estimation,2023,"Virtual content instability caused by device pose tracking error remains a prevalent issue in markerless augmented reality (AR), especially on smartphones and tablets. However, when examining environments which will host AR experiences, it is challenging to determine where those instability artifacts will occur; we rarely have access to ground truth pose to measure pose error, and even if pose error is available, traditional visualizations do not connect that data with the real environment, limiting their usefulness. To address these issues we present SiTAR (Situated Trajectory Analysis for Augmented Reality), the first situated trajectory analysis system for AR that incorporates estimates of pose tracking error. We start by developing the first uncertainty-based pose error estimation method for visual-inertial simultaneous localization and mapping (VI-SLAM), which allows us to obtain pose error estimates without ground truth; we achieve an average accuracy of up to 96.1% and an average FI score of up to 0.77 in our evaluations on four VI-SLAM datasets. Next, we present our SiTAR system, implemented for ARCore devices, combining a backend that supplies uncertainty-based pose error estimates with a frontend that generates situated trajectory visualizations. Finally, we evaluate the efficacy of SiTAR in realistic conditions by testing three visualization techniques in an in-the-wild study with 15 users and 13 diverse environments; this study reveals the impact both environment scale and the properties of surfaces present can have on user experience and task performance.",10.1109/ISMAR59233.2023.00043
00cff46813fb3aa82c3a87dfa60feeb09f4e0537,Contention-based Grant-free Transmission with Independent Multi-pilot Scheme,2020,"Contention-based grant-free transmission is very promising for future massive machine-type communication (mMTC). In contention-based transmission, the random pilot collision becomes a challenge. To solve it, multi-pilot scheme is proposed. However, the existing work relies on the low correlation of spatial channels, and cannot be applied when the number of antennas is relatively small. This paper proposes a novel independent multi-pilot scheme, which utilizes the diversity of multiple pilots, instead of the spatial correlation. The interference cancellation of both data symbols and multiple pilots is utilized to improve the performance. The simulation results show that the proposed scheme significantly improves the performance in high-overloading mMTC and outperforms the existing work in a massive antenna array case.",10.1109/VTC2020-Fall49728.2020.9348828
e92685b97dba91f08688467a7ae4fd4b7721fb3d,An Efficient NB-IoT Compatible GF-NOMA PHY Mechanism for mMTC,2023,"The third-generation partnership project (3GPP) has proposed the narrowband Internet of Things (NB-IoT) standard for serving Internet of Things (IoT) users. Improvements to the NB-IoT standard are being considered to efficiently support massive machine-type communication (mMTC) IoT applications. Subsequent releases of the NB-IoT standard envision to support mMTC using different access mechanisms. In this work, a grant-free (GF) nonorthogonal multiple access (NOMA) system compatible with the current NB-IoT standard is proposed, where the resources are divided into transmission opportunities (TOs). The TOs are further divided into multiple groups based on the repetitions adopted by the users in the system. Physical (PHY) layer procedures for preamble transmission/detection, channel estimation, data transmission/decoding, and acknowledgment (ACK) transmission/reception are described in detail for the proposed GF-NOMA system. Specifically, the preamble sequences are also used as spreading sequences for data transmission and three types of sequences—Zadoff–Chu, random Gaussian, and Golay sequences are explored. Results are presented for per-user success probability, latency, and energy consumption considering two NB-IoT compatible settings along with repetitions and retransmissions. It is demonstrated that the proposed GF-NOMA scheme meets the mMTC requirement recommended by the 3GPP with more than 99% per-user success probability and with the energy consumption reduced by at least 80% compared to NB-IoT, while being readily compatible with the current NB-IoT standard.",10.1109/JIOT.2023.3280598
b51ab98c3db736b14b0d99afaa8af7ef238921df,On Determining the Number of Preambles in Grant-Free mMTC Uplink to Reduce Collisions,2023,"Grant-free (GF) access is considered in the uplink for efficiently supporting massive machine type communication (mMTC) scenarios in the Internet of things (IoT). Being a contention-based procedure, GF transmissions consisting of preamble followed by data are susceptible to collisions. This results in an increased number of failed users who retransmit and add to the load on the network. Given that the network is already dense, it is essential to reduce collisions and hence minimize the number of failed users. In this work, a system model for multi-user GF uplink transmission with repetitions is proposed, where the time-frequency resources are divided into transmission opportunity (TO) groups whose size is equal to the number of repetitions adopted by the users. Considering preamble transmissions at the link layer, the key performance indicators (KPIs) corresponding to the per-user and all-user success probabilities of the users in such a system are derived. Further, a method to determine the number of preambles based on the sum of the all-user success probability and the probability of a single user failing in the system is proposed, and closedform expressions for the same are derived. It is demonstrated that our proposed method to find the number of preambles ensures that the collisions (and hence the number of failed users) are reduced considering two scenarios based on the number of users arriving - a) fixed and b) random (following a Poisson distribution). The correctness of our analysis is verified through Monte-Carlo simulations in both the scenarios.",10.1109/WCNC55385.2023.10118604
3f194698c6ad681e50e490a3d6180443941ab1b0,Machine type communications: key drivers and enablers towards the 6G era,2021,"The recently introduced 5G New Radio is the first wireless standard natively designed to support critical and massive machine type communications (MTC). However, it is already becoming evident that some of the more demanding requirements for MTC cannot be fully supported by 5G networks. Alongside, emerging use cases and applications towards 2030 will give rise to new and more stringent requirements on wireless connectivity in general and MTC in particular. Next generation wireless networks, namely 6G, should therefore be an agile and efficient convergent network designed to meet the diverse and challenging requirements anticipated by 2030. This paper explores the main drivers and requirements of MTC towards 6G, and discusses a wide variety of enabling technologies. More specifically, we first explore the emerging key performance indicators for MTC in 6G. Thereafter, we present a vision for an MTC-optimized holistic end-to-end network architecture. Finally, key enablers towards (1) ultra-low power MTC, (2) massively scalable global connectivity, (3) critical and dependable MTC, and (4) security and privacy preserving schemes for MTC are detailed. Our main objective is to present a set of research directions considering different aspects for an MTC-optimized 6G network in the 2030-era.",10.1186/s13638-021-02010-5
08edeae40fc65159a8785e84c2492c4c49aee50d,Contention-based Grant-free Transmission with Extremely Sparse Orthogonal Pilot Scheme,2021,"Due to the limited number of traditional orthogonal pilots, pilot collision will severely degrade the performance of contention-based grant-free transmission. To alleviate the pilot collision and exploit the spatial degree of freedom as much as possible, an extremely sparse orthogonal pilot scheme is proposed for uplink grant-free transmission. The proposed sparse pilot is used to perform active user detection and estimate the spatial channel. Then, inter-user interference suppression is performed by spatially combining the received data symbols using the estimated spatial channel. After that, the estimation and compensation of wireless channel and time/frequency offset are performed utilizing the geometric characteristics of combined data symbols. The task of pilot is much lightened, so that the extremely sparse orthogonal pilot can occupy minimized resources, and the number of orthogonal pilots can be increased significantly, which greatly reduces the probability of pilot collision. The numerical results show that the proposed extremely sparse orthogonal pilot scheme significantly improves the performance in high-overloading grant-free scenario.",10.1109/VTC2021-Fall52928.2021.9625265
00d63299b5cd49574b806c5b8e813f3c3208d894,A Novel Random Forest Dissimilarity Measure for Multi-View Learning,2020,"Multi-view learning is a learning task in which data is described by several concurrent representations. Its main challenge is most often to exploit the complementarities between these representations to help solve a classification/regression task. This is a challenge that can be met nowadays if there is a large amount of data available for learning. However, this is not necessarily true for all real-world problems, where data are sometimes scarce (e.g. problems related to the medical environment). In these situations, an effective strategy is to use intermediate representations based on the dissimilarities between instances. This work presents new ways of constructing these dissimilarity representations, learning them from data with Random Forest classifiers. More precisely, two methods are proposed, which modify the Random Forest proximity measure, to adapt it to the context of High Dimension Low Sample Size (HDLSS) multi-view classification problems. The second method, based on an Instance Hardness measurement, is significantly more accurate than other state-of-the-art measurements including the original RF Proximity measurement and the Large Margin Nearest Neighbor (LMNN) metric learning measurement.",10.1109/ICPR48806.2021.9412961
3d0295080dc4fa890c74a78099e3ee95422df5fc,Random forest kernel for high-dimension low sample size classification,2023,"High dimension, low sample size (HDLSS) problems are numerous among real-world applications of machine learning. From medical images to text processing, traditional machine learning algorithms are usually unsuccessful in learning the best possible concept from such data. In a previous work, we proposed a dissimilarity-based approach for multi-view classification, the random forest dissimilarity, that perfoms state-of-the-art results for such problems. In this work, we transpose the core principle of this approach to solving HDLSS classification problems, by using the RF similarity measure as a learned precomputed SVM kernel (RFSVM). We show that such a learned similarity measure is particularly suited and accurate for this classification context. Experiments conducted on 40 public HDLSS classification datasets, supported by rigorous statistical analyses, show that the RFSVM method outperforms existing methods for the majority of HDLSS problems and remains at the same time very competitive for low or non-HDLSS problems.",10.1007/s11222-023-10309-0
6a9677cb8ff0309ea9e39010b0b824520323eea2,Supervised similarity learning for corporate bonds using Random Forest proximities,2022,"Financial literature consists of ample research on similarity and comparison of financial assets and securities such as stocks, bonds, mutual funds, etc. However, going beyond correlations or aggregate statistics has been arduous since financial datasets are noisy, lack useful features, have missing data and often lack ground truth or annotated labels. However, though similarity extrapolated from these traditional models heuristically may work well on an aggregate level, such as risk management when looking at large portfolios, they often fail when used for portfolio construction and trading which require a local and dynamic measure of similarity on top of global measure. In this paper we propose a supervised similarity framework for corporate bonds which allows for inference based on both local and global measures. From a machine learning perspective, this paper emphasis that random forest (RF), which is usually viewed as a supervised learning algorithm, can also be used as a similarity learning (more specifically, a distance metric learning) algorithm. In addition, this framework proposes a novel metric to evaluate similarities, and analyses other metrics which further demonstrate that RF outperforms all other methods experimented with, in this work.",10.1145/3533271.3561736
6e898449f84451df2dc8fb94f4f4799a21c04217,Geometry- and Accuracy-Preserving Random Forest Proximities,2022,"Random forests are considered one of the best out-of-the-box classification and regression algorithms due to their high level of predictive performance with relatively little tuning. Pairwise proximities can be computed from a trained random forest and measure the similarity between data points relative to the supervised task. Random forest proximities have been used in many applications including the identification of variable importance, data imputation, outlier detection, and data visualization. However, existing definitions of random forest proximities do not accurately reflect the data geometry learned by the random forest. In this paper, we introduce a novel definition of random forest proximities called Random Forest-Geometry- and Accuracy-Preserving proximities (RF-GAP). We prove that the proximity-weighted sum (regression) or majority vote (classification) using RF-GAP exactly matches the out-of-bag random forest prediction, thus capturing the data geometry learned by the random forest. We empirically show that this improved geometric representation outperforms traditional random forest proximities in tasks such as data imputation and provides outlier detection and visualization results consistent with the learned data geometry.",10.1109/TPAMI.2023.3263774
00dd6ae26c9d9c1e00dfd9d73f84eb33203cbc00,Learning Sparse Graph Laplacian with K Eigenvector Prior via Iterative Glasso and Projection,2020,"Learning a suitable graph is an important precursor to many graph signal processing (GSP) pipelines, such as graph signal compression and denoising. Previous graph learning algorithms either i) make assumptions on graph connectivity (e.g., graph sparsity), or ii) make edge weight assumptions such as positive edges only. In this paper, given an empirical covariance matrix ${\mathbf{\bar C}}$ computed from data as input, we consider an eigen-structural assumption on the graph Laplacian matrix L: the first K eigenvectors of L are pre-selected, e.g., based on domain-specific criteria, and the remaining eigenvectors are then learned from data. One example use case is image coding, where the first eigenvector is pre-chosen to be constant, regardless of available observed data. We first prove that the subspace $\mathcal{H}_{\mathbf{u}}^ + $ of symmetric positive semi-definite (PSD) matrices with the first K eigenvectors being {uk} in a defined Hilbert space is a convex cone. We then construct an operator to project a given positive definite (PD) matrix L to $\mathcal{H}_{\mathbf{u}}^ + $, inspired by the Gram-Schmidt procedure. Finally, we design an efficient hybrid graphical lasso / projection algorithm to compute the most suitable graph Laplacian matrix ${{\mathbf{L}}^ * } \in \mathcal{H}_{\mathbf{u}}^ + $ given ${\mathbf{\bar C}}$. Experimental results show that given the first K eigenvectors as a prior, our algorithm outperforms competing graph learning schemes using a variety of graph comparison metrics.",10.1109/ICASSP39728.2021.9414693
de1cb4c8868db6254ed863c2711a585b6fc344d3,Hybrid Model-Based / Data-Driven Graph Transform for Image Coding,2022,"Transform coding to sparsify signal representations remains crucial in an image compression pipeline. While the Karhunen-Loève transform (KLT) computed from an empirical covariance matrix ${\mathbf{\bar C}}$ is theoretically optimal for a stationary process, in practice, collecting sufficient statistics from a non-stationary image to reliably estimate ${\mathbf{\bar C}}$ can be difficult. In this paper, to encode an intra-prediction residual block, we pursue a hybrid model-based / data-driven approach: the first K eigenvectors of a transform matrix are derived from a statistical model, e.g., the asymmetric discrete sine transform (ADST), for stability, while the remaining N −K are computed from ${\mathbf{\bar C}}$ for data adaptivity. The transform computation is posed as a graph learning problem, where we seek a graph Laplacian matrix minimizing a graphical lasso objective inside a convex cone sharing the first K eigenvectors in a Hilbert space of real symmetric matrices. We efficiently solve the problem via augmented Lagrangian relaxation and proximal gradient (PG). Using open-source WebP as a baseline image codec, experimental results show that our hybrid graph transform achieved better coding performance than discrete cosine transform (DCT), ADST and KLT, and better stability than KLT.",10.1109/ICIP46576.2022.9897653
a3602bbf674f4aaa4f2b5837b52102e6d1213e35,Sparse Graph Learning with Spectrum Prior for Deep Graph Convolutional Networks,2022,"A graph convolutional network (GCN) employs a graph filtering kernel tailored for data with irregular structures. However, simply stacking more GCN layers does not improve performance; instead, the output converges to an uninformative low-dimensional subspace, where the convergence rate is characterized by the graph spectrum— this is the known over-smoothing problem in GCN. In this paper, we propose a sparse graph learning algorithm incorporating a new spectrum prior to compute a graph topology that circumvents over-smoothing while preserving pairwise correlations inherent in data. Specifically, based on a spectral analysis of multilayer GCN output, we derive a spectrum prior for the graph Laplacian matrix L to robustify the model expressiveness against over-smoothing. Then, we formulate a sparse graph learning problem with the spectrum prior, solved efficiently via block coordinate descent (BCD). Moreover, we optimize the weight parameter trading off the fidelity term with the spectrum prior, based on data smoothness on the original graph learned without spectrum manipulation. The output L is then normalized for supervised GCN training. Experiments show that our proposal produced deeper GCNs and higher prediction accuracy for regression and classification tasks compared to competing schemes.",10.1109/ICASSP49357.2023.10095900
43e99ba85ad00452b7b97e205c72b6e07fea03f5,Spectral Graph Learning With Core Eigenvectors Prior via Iterative GLASSO and Projection,2024,"Before the execution of many standard graph signal processing (GSP) modules, such as compression and restoration, learning of a graph that encodes pairwise (dis)similarities in data is an important precursor. In data-starved scenarios, to reduce parameterization, previous graph learning algorithms make assumptions in the nodal domain on i) graph connectivity (e.g., edge sparsity), and/or ii) edge weights (e.g., positive edges only). In this paper, given an empirical covariance matrix <inline-formula><tex-math notation=""LaTeX"">$\bar{{\mathbf{C}}}$</tex-math></inline-formula> estimated from sparse data, we consider instead a spectral-domain assumption on the graph Laplacian matrix <inline-formula><tex-math notation=""LaTeX"">${\mathcal{L}}$</tex-math></inline-formula>: the first <inline-formula><tex-math notation=""LaTeX"">$K$</tex-math></inline-formula> eigenvectors (called “core” eigenvectors) <inline-formula><tex-math notation=""LaTeX"">$\{{\mathbf{u}}_{k}\}$</tex-math></inline-formula> of <inline-formula><tex-math notation=""LaTeX"">${\mathcal{L}}$</tex-math></inline-formula> are pre-selected—e.g., based on domain-specific knowledge—and only the remaining eigenvectors are learned and parameterized. We first prove that, inside a Hilbert space of real symmetric matrices, the subspace <inline-formula><tex-math notation=""LaTeX"">${\mathcal{H}}_{\mathbf{u}}^{+}$</tex-math></inline-formula> of positive semi-definite (PSD) matrices sharing a common set of core <inline-formula><tex-math notation=""LaTeX"">$K$</tex-math></inline-formula> eigenvectors <inline-formula><tex-math notation=""LaTeX"">$\{{\mathbf{u}}_{k}\}$</tex-math></inline-formula> is a convex cone. Inspired by the Gram-Schmidt procedure, we then construct an efficient operator to project a given positive definite (PD) matrix onto <inline-formula><tex-math notation=""LaTeX"">${\mathcal{H}}_{\mathbf{u}}^{+}$</tex-math></inline-formula>. Finally, we design a hybrid graphical lasso/projection algorithm to compute a locally optimal inverse Laplacian <inline-formula><tex-math notation=""LaTeX"">${\mathcal{L}}^{-1}\in{\mathcal{H}}_{\mathbf{u}}^{+}$</tex-math></inline-formula> given <inline-formula><tex-math notation=""LaTeX"">$\bar{{\mathbf{C}}}$</tex-math></inline-formula>. We apply our graph learning algorithm in two practical settings: parliamentary voting interpolation and predictive transform coding in image compression. Experiments show that our algorithm outperformed existing graph learning schemes in data-starved scenarios for both synthetic data and these two settings.",10.1109/TSP.2024.3446453
00e0adb2e5a09588d779bd887db800d7cea87e0f,SeqXY2SeqZ: Structure Learning for 3D Shapes by Sequentially Predicting 1D Occupancy Segments From 2D Coordinates,2020,"Structure learning for 3D shapes is vital for 3D computer vision. State-of-the-art methods show promising results by representing shapes using implicit functions in 3D that are learned using discriminative neural networks. However, learning implicit functions requires dense and irregular sampling in 3D space, which also makes the sampling methods affect the accuracy of shape reconstruction during test. To avoid dense and irregular sampling in 3D, we propose to represent shapes using 2D functions, where the output of the function at each 2D location is a sequence of line segments inside the shape. Our approach leverages the power of functional representations, but without the disadvantage of 3D sampling. Specifically, we use a voxel tubelization to represent a voxel grid as a set of tubes along any one of the X, Y, or Z axes. Each tube can be indexed by its 2D coordinates on the plane spanned by the other two axes. We further simplify each tube into a sequence of occupancy segments. Each occupancy segment consists of successive voxels occupied by the shape, which leads to a simple representation of its 1D start and end location. Given the 2D coordinates of the tube and a shape feature as condition, this representation enables us to learn 3D shape structures by sequentially predicting the start and end locations of each occupancy segment in the tube. We implement this approach using a Seq2Seq model with attention, called SeqXY2SeqZ, which learns the mapping from a sequence of 2D coordinates along two arbitrary axes to a sequence of 1D locations along the third axis. SeqXY2SeqZ not only benefits from the regularity of voxel grids in training and testing, but also achieves high memory efficiency. Our experiments show that SeqXY2SeqZ outperforms the state-ofthe-art methods under widely used benchmarks.",10.1007/978-3-030-58586-0_36
e9f7ea48fa9d03da80cb6f4b4cd27df8f8cd79b8,NeuralTPS: Learning Signed Distance Functions Without Priors From Single Sparse Point Clouds,2024,"Surface reconstruction for point clouds is one of the important tasks in 3D computer vision. The latest methods rely on generalizing the priors learned from large scale supervision. However, the learned priors usually do not generalize well to various geometric variations that are unseen during training, especially for extremely sparse point clouds. To resolve this issue, we present a neural network to directly infer SDFs from single sparse point clouds without using signed distance supervision, learned priors or even normals. Our insight here is to learn surface parameterization and SDFs inference in an end-to-end manner. To make up the sparsity, we leverage parameterized surfaces as a coarse surface sampler to provide many coarse surface estimations in training iterations, according to which we mine supervision for our thin plate splines (TPS) based network to infer smooth SDFs in a statistical way. Our method significantly improves the generalization ability and accuracy on unseen point clouds. Our experimental results show our advantages over the state-of-the-art methods in surface reconstruction for sparse point clouds under synthetic datasets and real scans.",10.1109/TPAMI.2024.3476349
7c948de637aea6889dfa6a7bcbdf8cfba8a1c7ed,Learning Local Pattern Modularization for Point Cloud Reconstruction from Unseen Classes,2024,"It is challenging to reconstruct 3D point clouds in unseen classes from single 2D images. Instead of object-centered coordinate system, current methods generalized global priors learned in seen classes to reconstruct 3D shapes from unseen classes in viewer-centered coordinate system. However, the reconstruction accuracy and interpretability are still eager to get improved. To resolve this issue, we introduce to learn local pattern modularization for reconstructing 3D shapes in unseen classes, which achieves both good generalization ability and high reconstruction accuracy. Our insight is to learn a local prior which is class-agnostic and easy to generalize in object-centered coordinate system. Specifically, the local prior is learned via a process of learning and customizing local pattern modularization in seen classes. During this process, we first learn a set of patterns in local regions, which is the basis in the object-centered coordinate system to represent an arbitrary region on shapes across different classes. Then, we modularize each region on an initially reconstructed shape using the learned local patterns. Based on that, we customize the local pattern modularization using the input image by refining the reconstruction with more details. Our method enables to reconstruct high fidelity point clouds from unseen classes in object-centered coordinate system without requiring a large number of patterns or any additional information, such as segmentation supervision or camera poses. Our experimental results under widely used benchmarks show that our method achieves the state-of-the-art reconstruction accuracy for shapes from unseen classes. The code is available at https://github.com/chenchao15/Unseen.",10.48550/arXiv.2408.14279
ea05b2edfadcbd8a060e228f440c7a5b42abce51,Fast Learning of Signed Distance Functions From Noisy Point Clouds via Noise to Noise Mapping,2024,"Learning signed distance functions (SDFs) from point clouds is an important task in 3D computer vision. However, without ground truth signed distances, point normals or clean point clouds, current methods still struggle from learning SDFs from noisy point clouds. To overcome this challenge, we propose to learn SDFs via a noise to noise mapping, which does not require any clean point cloud or ground truth supervision. Our novelty lies in the noise to noise mapping which can infer a highly accurate SDF of a single object or scene from its multiple or even single noisy observations. We achieve this by a novel loss which enables statistical reasoning on point clouds and maintains geometric consistency although point clouds are irregular, unordered and have no point correspondence among noisy observations. To accelerate training, we use multi-resolution hash encodings implemented in CUDA in our framework, which reduces our training time by a factor of ten, achieving convergence within one minute. We further introduce a novel schema to improve multi-view reconstruction by estimating SDFs as a prior. Our evaluations under widely-used benchmarks demonstrate our superiority over the state-of-the-art methods in surface reconstruction from point clouds or multi-view images, point cloud denoising and upsampling.",10.1109/TPAMI.2024.3416068
00e5a917421055c3c848099ea29ab679c2fe71c8,Buy Me That Look: An Approach for Recommending Similar Fashion Products,2020,"Have you ever looked at an Instagram model, or a model in a fashion e-commerce web-page, and thought ""Wish I could get a list of fashion items similar to the ones worn by the model!"". This is what we address in this paper, where we propose a novel computer vision based technique called ShopLook to address the challenging problem of recommending similar fashion products. The proposed method has been evaluated at Myntra (www.myntra.com), a leading online fashion e-commerce platform. In particular, given a user query and the corresponding Product Display Page (PDP) against the query, the goal of our method is to recommend similar fashion products corresponding to the entire set of fashion articles worn by a model in the PDP full-shot image (the one showing the entire model from head to toe). The novelty and strength of our method lies in its capability to recommend similar articles for all the fashion items worn by the model, in addition to the primary article corresponding to the query. This is not only important to promote cross-sells for boosting revenue, but also for improving customer experience and engagement. In addition, our approach is also capable of recommending similar products for User Generated Content (UGC), eg., fashion article images uploaded by users. Formally, our proposed method consists of the following components (in the same order): i) Human keypoint detection, ii) Pose classification, iii) Article localisation and object detection, along with active learning feedback, and iv) Triplet network based image embedding model.",10.1109/MIPR51284.2021.00022
ba95ef3e2028f8ca9293a9ae4371f307a45790c7,A Review of Modern Fashion Recommender Systems,2022,"The textile and apparel industries have grown tremendously over the past few years. Customers no longer have to visit many stores, stand in long queues, or try on garments in dressing rooms, as millions of products are now available in online catalogs. However, given the plethora of options available, an effective recommendation system is necessary to properly sort, order, and communicate relevant product material or information to users. Effective fashion recommender systems (RSs) can have a noticeable impact on billions of customers’ shopping experiences and increase sales and revenues on the provider side. The goal of this survey is to provide a review of RSs that operate in the specific vertical domain of garment and fashion products. We have identified the most pressing challenges in fashion RS research and created a taxonomy that categorizes the literature according to the objective they are trying to accomplish (e.g., item or outfit recommendation, size recommendation, and explainability, among others) and type of side information (users, items, context). We have also identified the most important evaluation goals and perspectives (outfit generation, outfit recommendation, pairing recommendation, and fill-in-the-blank outfit compatibility prediction) and the most commonly used datasets and evaluation metrics.",10.1145/3624733
00e604fb55a7c17f107893a7928ffc4afd95fecb,Smart Contracts on the Move,2020,"Blockchain systems have received much attention and promise to revolutionize many services. Yet, despite their popularity, current blockchain systems exist in isolation, that is, they cannot share information. While interoperability is crucial for blockchain to reach widespread adoption, it is difficult to achieve due to differences among existing blockchain technologies. This paper presents a technique to allow blockchain interoperability. The core idea is to provide a primitive operation to developers so that contracts and objects can switch from one blockchain to another, without breaking consistency and violating key blockchain properties. To validate our ideas, we implemented our protocol in two popular blockchain clients that use the Ethereum virtual machine. We discuss how to build applications using the proposed protocol and show examples of applications based on real use cases that can move across blockchains. To analyze the system performance we use a real trace from one of the most popular Ethereum applications and replay it in a multi-blockchain environment.",10.1109/DSN48063.2020.00040
a85dd4ceb20d48988074cc551cf2e6e51a055c17,Exploring the topic evolution of Dunhuang murals through image classification,2022,"Dunhuang is a unique art treasure and a world heritage site. In order to organise and manage Dunhuang cultural heritage resources, this article studies the classification of Dunhuang murals in different dynasties, and explores the topic distribution characteristics and evolution rules of them. First, image features are extracted through scale-invariant feature transform (SIFT) and Canny and scale-invariant feature transform (CSIFT), a visual dictionary is generated through the k-means clustering algorithm, and the term frequency–inverse document frequency (TF-IDF) vector is calculated and combined with the colour feature vector extracted via hue, saturation and value (HSV). Second, Dunhuang mural images are collected and the support vector machine (SVM) classifier is built. Finally, the knowledge graph-based topic maps are constructed, and graph theory is introduced to analyse the topic distribution and evolution of Dunhuang murals in different dynasties. The results show that the Dunhuang murals of different dynasties can be effectively classified through the bag of words, HSV and support vector machine (BOW_HSV_SVM) based on their visual features. Through topic maps, the topic distribution characteristics and evolution rules of Dunhuang murals with the dynasties are revealed.",10.1177/01655515221074336
00ea24a6f15c450002a8665dfc806c753a4b3892,Automatic stimuli classification from ERP data for augmented communication via Brain-Computer Interfaces,2020,"Brain-computer interfaces (BCIs) are systems initially designed to compensate for motor disabilities affecting people whose control of the muscular system is compromised. However, recent developments open the BCIs market to a wide range of medical and non-medical applications. This raises the need for systems capable of interpreting more and more stimuli, even from different sensory domains. In this work, we design a machine-learning system able to fit both application domains accurately recognizing visual and auditory stimuli starting from the event-related potentials (ERPs) they generate. The obtained results are promising and some practical and realization aspects are discussed.",10.1109/ICHMS49158.2020.9209393
5d2ff541f31e8e31b1c744af5458e2639994d7a3,Classification algorithm for motor imagery fusing CNN and attentional mechanisms based on functional near-infrared spectroscopy brain image.,2024,,10.1007/s11571-024-10116-x
07d73c9ae0d6511d9392917b1375d0954033903d,"Neural signatures of imaginary motivational states: desire for music, movement and social play",2024,"The literature has demonstrated the potential for detecting accurate electrical signals that correspond to the will or intention to move, as well as decoding the thoughts of individuals who imagine houses, faces or objects. This investigation examines the presence of precise neural markers of imagined motivational states through the combining of electrophysiological and neuroimaging methods. 20 participants were instructed to vividly imagine the desire to move, listen to music or engage in social activities. Their EEG was recorded from 128 scalp sites and analysed using individual standardized Low-Resolution Brain Electromagnetic Tomographies (LORETAs) in the N400 time window (400–600 ms). The activation of 1056 voxels was examined in relation to the 3 motivational states. The most active dipoles were grouped in eight regions of interest (ROI), including Occipital, Temporal, Fusiform, Premotor, Frontal, OBF/IF, Parietal, and Limbic areas. The statistical analysis revealed that all motivational imaginary states engaged the right hemisphere more than the left hemisphere. Distinct markers were identified for the three motivational states. Specifically, the right temporal area was more relevant for “Social Play”, the orbitofrontal/inferior frontal cortex for listening to music, and the left premotor cortex for the “Movement” desire. This outcome is encouraging in terms of the potential use of neural indicators in the realm of brain-computer interface, for interpreting the thoughts and desires of individuals with locked-in syndrome. Supplementary Information The online version contains supplementary material available at 10.1007/s10548-024-01047-1.",10.1007/s10548-024-01047-1
9cc1b209e079aa340f47c30aded754ac231545c8,Brain-computer interfaces and human factors: the role of language and cultural differences—Still a missing gap?,2024,"Brain-computer interfaces (BCIs) aim at the non-invasive investigation of brain activity for supporting communication and interaction of the users with their environment by means of brain-machine assisted technologies. Despite technological progress and promising research aimed at understanding the influence of human factors on BCI effectiveness, some topics still remain unexplored. The aim of this article is to discuss why it is important to consider the language of the user, its embodied grounding in perception, action and emotions, and its interaction with cultural differences in information processing in future BCI research. Based on evidence from recent studies, it is proposed that detection of language abilities and language training are two main topics of enquiry of future BCI studies to extend communication among vulnerable and healthy BCI users from bench to bedside and real world applications. In addition, cultural differences shape perception, actions, cognition, language and emotions subjectively, behaviorally as well as neuronally. Therefore, BCI applications should consider cultural differences in information processing to develop culture- and language-sensitive BCI applications for different user groups and BCIs, and investigate the linguistic and cultural contexts in which the BCI will be used.",10.3389/fnhum.2024.1305445
3b64e6b9f0c7c17fd7cd78516b64c7ac5fd24487,"Neural correlates of recalled sadness, joy, and fear states: a source reconstruction EEG study",2024,"Introduction The capacity to understand the others’ emotional states, particularly if negative (e.g. sadness or fear), underpins the empathic and social brain. Patients who cannot express their emotional states experience social isolation and loneliness, exacerbating distress. We investigated the feasibility of detecting non-invasive scalp-recorded electrophysiological signals that correspond to recalled emotional states of sadness, fear, and joy for potential classification. Methods The neural activation patterns of 20 healthy and right-handed participants were studied using an electrophysiological technique. Analyses were focused on the N400 component of Event-related potentials (ERPs) recorded during silent recall of subjective emotional states; Standardized weighted Low-resolution Electro-magnetic Tomography (swLORETA) was employed for source reconstruction. The study classified individual patterns of brain activation linked to the recollection of three distinct emotional states into seven regions of interest (ROIs). Results Statistical analysis (ANOVA) of the individual magnitude values revealed the existence of a common emotional circuit, as well as distinct brain areas that were specifically active during recalled sad, happy and fearful states. In particular, the right temporal and left superior frontal areas were more active for sadness, the left limbic region for fear, and the right orbitofrontal cortex for happy affective states. Discussion In conclusion, this study successfully demonstrated the feasibility of detecting scalp-recorded electrophysiological signals corresponding to internal and subjective affective states. These findings contribute to our understanding of the emotional brain, and have potential applications for future BCI classification and identification of emotional states in LIS patients who may be unable to express their emotions, thus helping to alleviate social isolation and sense of loneliness.",10.3389/fpsyt.2024.1357770
00eaa0b41b987d5e7badf086c3ff7bbd3187b7e7,Multimodal Aggregation Approach for Memory Vision-Voice Indoor Navigation with Meta-Learning,2020,"Vision and voice are two vital keys for agents’ interaction and learning. In this paper, we present a novel indoor navigation model called Memory Vision-Voice Indoor Navigation (MVV-IN), which receives voice commands and analyzes multimodal information of visual observation in order to enhance robots’ environment understanding. We make use of single RGB images taken by a rst-view monocular camera. We also apply a self-attention mechanism to keep the agent focusing on key areas. Memory is important for the agent to avoid repeating certain tasks unnecessarily and in order for it to adapt adequately to new scenes, therefore, we make use of meta-learning. We have experimented with various functional features extracted from visual observation. Comparative experiments prove that our methods outperform state-of-the-art baselines.",10.1109/IROS45743.2020.9341398
c264c7ae1b60325da97811855c919b421e010318,Multimodal Image-Based Indoor Localization with Machine Learning—A Systematic Review,2024,"Outdoor positioning has become a ubiquitous technology, leading to the proliferation of many location-based services such as automotive navigation and asset tracking. Meanwhile, indoor positioning is an emerging technology with many potential applications. Researchers are continuously working towards improving its accuracy, and one general approach to achieve this goal includes using machine learning to combine input data from multiple available sources, such as camera imagery. For this active research area, we conduct a systematic literature review and identify around 40 relevant research papers. We analyze contributions describing indoor positioning methods based on multimodal data, which involves combinations of images with motion sensors, radio interfaces, and LiDARs. The conducted survey allows us to draw conclusions regarding the open research areas and outline the potential future evolution of multimodal indoor positioning.",10.3390/s24186051
eec6737c15d7916a4bfa9534d6b4345057a64cf0,Radiance Field Learners As UAV First-Person Viewers,2024,"First-Person-View (FPV) holds immense potential for revolutionizing the trajectory of Unmanned Aerial Vehicles (UAVs), offering an exhilarating avenue for navigating complex building structures. Yet, traditional Neural Radiance Field (NeRF) methods face challenges such as sampling single points per iteration and requiring an extensive array of views for supervision. UAV videos exacerbate these issues with limited viewpoints and significant spatial scale variations, resulting in inadequate detail rendering across diverse scales. In response, we introduce FPV-NeRF, addressing these challenges through three key facets: (1) Temporal consistency. Leveraging spatio-temporal continuity ensures seamless coherence between frames; (2) Global structure. Incorporating various global features during point sampling preserves space integrity; (3) Local granularity. Employing a comprehensive framework and multi-resolution supervision for multi-scale scene feature representation tackles the intricacies of UAV video spatial scales. Additionally, due to the scarcity of publicly available FPV videos, we introduce an innovative view synthesis method using NeRF to generate FPV perspectives from UAV footage, enhancing spatial perception for drones. Our novel dataset spans diverse trajectories, from outdoor to indoor environments, in the UAV domain, differing significantly from traditional NeRF scenarios. Through extensive experiments encompassing both interior and exterior building structures, FPV-NeRF demonstrates a superior understanding of the UAV flying space, outperforming state-of-the-art methods in our curated UAV dataset. Explore our project page for further insights: https://fpv-nerf.github.io/.",10.48550/arXiv.2408.05533
00f3411d9e3db3ccf893efe11e5ee94109f10af7,Learning VAE-LDA Models with Rounded Reparameterization Trick,2020,"The introduction of VAE provides an efficient framework for the learning of generative models, including generative topic models. However, when the topic model is a Latent Dirichlet Allocation (LDA) model, a central technique of VAE, the reparameterization trick, fails to be applicable. This is because no reparameterization form of Dirichlet distributions is known to date that allows the use of the reparameterization trick. In this work, we propose a new method, which we call Rounded Reparameterization Trick (RRT), to reparameterize Dirichlet distributions for the learning of VAE-LDA models. This method, when applied to a VAE-LDA model, is shown experimentally to outperform the existing neural topic models on several benchmark datasets and on a synthetic dataset.",10.18653/v1/2020.emnlp-main.101
d6f24dddbc02d205954f9a900f71b0a37cbdd0a5,"A Survey on Neural Topic Models: Methods, Applications, and Challenges",2024,"Topic models have been prevalent for decades to discover latent topics and infer topic proportions of documents in an unsupervised fashion. They have been widely used in various applications like text analysis and context recommendation. Recently, the rise of neural networks has facilitated the emergence of a new research field—neural topic models (NTMs). Different from conventional topic models, NTMs directly optimize parameters without requiring model-specific derivations. This endows NTMs with better scalability and flexibility, resulting in significant research attention and plentiful new methods and applications. In this paper, we present a comprehensive survey on neural topic models concerning methods, applications, and challenges. Specifically, we systematically organize current NTM methods according to their network structures and introduce the NTMs for various scenarios like short texts and cross-lingual documents. We also discuss a wide range of popular applications built on NTMs. Finally, we highlight the challenges confronted by NTMs to inspire future research.",10.1007/s10462-023-10661-7
3da1697dd847b853bd44a6fb059580122280bd55,VoxAtnNet: A 3D Point Clouds Convolutional Neural Network for Generalizable Face Presentation Attack Detection,2024,"Facial biometrics are an essential components of smartphones to ensure reliable and trustworthy authentication. However, face biometric systems are vulnerable to Presentation Attacks (PAs), and the availability of more sophisticated presentation attack instruments such as 3D silicone face masks will allow attackers to deceive face recognition systems easily. In this work, we propose a novel Presentation Attack Detection (PAD) algorithm based on 3D point clouds captured using the frontal camera of a smartphone to detect presentation attacks. The proposed PAD algorithm, VoxAtnNet, processes 3D point clouds to obtain voxelization to preserve the spatial structure. Then, the voxelized 3D samples were trained using the novel convolutional attention network to detect PAs on the smartphone. Extensive experiments were carried out on the newly constructed 3D face point cloud dataset comprising bona fide and two different 3D PAIs (3D silicone face mask and wrap photo mask), resulting in 3480 samples. The performance of the proposed method was compared with existing methods to benchmark the detection performance using three different evaluation protocols. The experimental results demonstrate the improved performance of the proposed method in detecting both known and unknown face presentation attacks.",10.1109/FG59268.2024.10582037
147ee2ec5fd0d2815cb09b090ba872566cd83fdf,E-Commerce Fraud Detection Based on Machine Learning Techniques: Systematic Literature Review,2024,": The e-commerce industry’s rapid growth, accelerated by the COVID-19 pandemic, has led to an alarming increase in digital fraud and associated losses. To establish a healthy e-commerce ecosystem, robust cyber security and anti-fraud measures are crucial. However, research on fraud detection systems has struggled to keep pace due to limited real-world datasets. Advances in artificial intelligence, Machine Learning (ML), and cloud computing have revitalized research and applications in this domain. While ML and data mining techniques are popular in fraud detection, specific reviews focusing on their application in e-commerce platforms like eBay and Facebook are lacking depth. Existing reviews provide broad overviews but fail to grasp the intricacies of ML algorithms in the e-commerce context. To bridge this gap, our study conducts a systematic literature review using the Preferred Reporting Items for Systematic reviews and Meta-Analysis (PRISMA) methodology. We aim to explore the effectiveness of these techniques in fraud detection within digital marketplaces and the broader e-commerce landscape. Understanding the current state of the literature and emerging trends is crucial given the rising fraud incidents and associated costs. Through our investigation, we identify research opportunities and provide insights to industry stakeholders on key ML and data mining techniques for combating e-commerce fraud. Our paper examines the research on these techniques as published in the past decade. Employing the PRISMA approach, we conducted a content analysis of 101 publications, identifying research gaps, recent techniques, and highlighting the increasing utilization of artificial neural networks in fraud detection within the industry.",10.26599/bdma.2023.9020023
65ea369792fbf8f62bb91a8f79200eb7cc23739c,Toward the Automatic Modulation Classification With Adaptive Wavelet Network,2023,"With the evolutionary development of modern communications technology, automatic modulation classification (AMC) has played an increasing role in the complex wireless communication environment. Existing AMC schemes based on deep learning use a neural network to extract features and calculate feature maps, then feed them into fully connected layers for classification. However, existing schemes still are insufficient in utilizing feature maps. To overcome this limitation, a novel adaptive wavelet network (AWN) is proposed, which combines adaptive wavelet decomposition based on the lifting scheme and channel attention mechanism.In contrast to the previous models, the multi-level decomposition of AWN explicitly extracts the features of multiple frequency bands. The channel attention mechanism further selects the optimal frequencies from the candidate frequencies. AWN explores a novel AMC paradigm that efficiently integrates the inherent properties of the signal by introducing prior knowledge in the frequency domain. Simulation results demonstrate that our proposed AMC scheme outperforms the benchmark scheme and has rather low computational complexity.",10.1109/TCCN.2023.3252580
010530897f5a25113c47d904877d21b867ec4596,Imbalanced Time Series Classification for Flight Data Analyzing with Nonlinear Granger Causality Learning,2020,"Identifying the faulty class of multivariate time series is crucial for today's flight data analysis. However, most of the existing time series classification methods suffer from imbalanced data and lack of model interpretability, especially on flight data of which faulty events are usually uncommon with a limited amount of data. Here, we present a neural network classification model for imbalanced multivariate time series by leveraging the information learned from normal class, which can also learn the nonlinear Granger causality for each class, so that we can pinpoint how time series classes differ from each other. Experiments on simulated data and real flight data shows that this model can achieve high accuracy of identifying anomalous flights.",10.1145/3340531.3412710
ab53571202e42d96b06179c2fd6cc8b788fd434e,Causal Discovery from Temporal Data: An Overview and New Perspectives,2024,"
 Temporal data, representing chronological observations of complex systems, has always been a typical data structure that can be widely generated by many domains, such as industry, finance, healthcare and climatology. Analyzing the underlying structures,
 i.e.
 , the causal relations, could be extremely valuable for various applications. Recently, causal discovery from temporal data has been considered as an interesting yet critical task and attracted much research attention. According to the nature and structure of temporal data, existing causal discovery works can be divided into two highly correlated categories
 i.e.
 , multivariate time series causal discovery, and event sequence causal discovery. However, most previous surveys are only focused on the multivariate time series causal discovery but ignore the second category. In this paper, we specify the similarity between the two categories and provide an overview of existing solutions. Furthermore, we provide public datasets, evaluation metrics and new perspectives for temporal data causal discovery.
",10.1145/3705297
8b369c5ff808489955fdfa813773cbc5e32a7c96,Hard Landing Pattern Recognition and Precaution With QAR Data by Functional Data Analysis,2024,"Hard landing is one of the most common safety events in the aviation industry, which has been a critical concern of airlines and aviation administration for a long time. Although the analysis of quick access recorder (QAR) data has the potential to illuminate the formation reason of a hard landing event, most existing methodologies overlook the curve characteristics of QAR parameters and focus on a straightforward prediction problem for hard landing. These methods usually lack interpretability and provide limited preventative insights. This article presents the hard landing pattern recognition and precaution pipeline, an innovative framework designed to recognize different landing patterns of flights and provide proactive suggestions against hard landing. Utilizing functional data analysis techniques, we first identify the key QAR parameters that have critical impacts on hard landing and subsequently recognize distinctive landing patterns that exhibit noticeable disparities. Through a detailed comparison of landing curves and pilot operations between normal and hard landing flights, we provide insights into the formation reason for hard landing and offer practicable landing advice for pilots",10.1109/TAES.2024.3387919
010de277d006c2daefa0797c3669a8c774cd5554,Trajectory Planning of UAV in Wireless Powered IoT System Based on Deep Reinforcement Learning,2020,"In this paper, a UAV-assisted wireless powered communication system for IoT network is studied. Specifically, the UAV performs as base station (BS) to collect the sensory information of the IoT devices as well as to broadcast energy signals to charge them. Considering the devices' limited data storage capacity and battery life, we propose a multi-objective optimization problem that aims to minimize the average data buffer length, maximize the residual battery level of the system and avoid data overflow and running out of battery of devices. Since the services requirements of IoT devices are dynamic and uncertain and the system can not be full observed by the UAV, it is challenging for UAV to achieve trajectory planning. In this regard, a deep Q network (DQN) is applied for UAV's flight control. Simulation results indicate that the DQN-based algorithm provides an efficient UAV's flight control policy for the proposed optimization problem.",10.1109/ICCC49849.2020.9238842
cf3c4b0c58f19f844e52dbfedce0e364c91c4179,Design of Anti-Interference Path Planning for Cellular-Connected UAVs Based on Improved DDPG,2024,"The flight and communication security of the Cellular-Connected Unmanned Aerial Vehicles (UAVs) is an important and popular research direction. Due to the complexity of the environmental space, UAVs face a complex and ever-changing task space. In recent years, reinforcement learning has rapidly advanced and widely applied in complex scenarios path planning problems. However, due to the discrete action space, their accuracy is limited. To address aforementioned problems, a new method for UAV path planning based on Deep Reinforcement Learning has been proposed in this paper. Specifically, this paper adopts an improved DDPG method with Actor-Critic framework, which can improve the accuracy. To further enhance the algorithm's precision and training speed, this paper introduces Post-Decision State method, which leverages experience for prediction to optimize the training results and enable UAVs to adapt to the ever-changing environment. Simulation experiments have proved that the improved method can increase training speed and make significant improvements in path performance.",10.1109/HPSC62738.2024.00020
7fd15bd6182a55d0823465b0e7b19755ede0fa2c,Wireless power transfer with unmanned aerial vehicles: State of the art and open challenges,2023,,10.1016/j.pmcj.2023.101820
239766e8600624ccbd890011d0098a1f4af703e2,Energy-Efficient and Context-aware Trajectory Planning for Mobile Data Collection in IoT using Deep Reinforcement Learning,2022,"IoT networks are often composed of spatially distributed nodes. This is why mobile data collection (MDC) emerged as an efficient solution to gather data from IoT networks that tolerate delay. In this paper, we study the use of reinforcement learning (RL) to plan the data collection trajectory of a mobile node (MN) in cluster-based IoT networks. Most of the existing solutions use static methods. However, in a context where the MN has little information (no previous data set) about the environment and where the environment is subject to changes (cluster mobility, etc.), we want the MN to learn an energy-efficient trajectory and adapt the trajectory to the significant changes in the environment. For that purpose, we will train two reinforcement learning (RL) algorithms: Q-learning and state-action-reward-state-action (SARSA) combined with deep learning (DL). This solution will allow us to maximize the collected data while minimizing the energy consumption of the MN. These algorithms will also adapt the trajectory of the MN to the signiflcant changes in the environment.",10.23919/softcom55329.2022.9911304
0110160545753817688e8e87be40442e712414d1,Gait Recognition via Semi-supervised Disentangled Representation Learning to Identity and Covariate Features,2020,"Existing gait recognition approaches typically focus on learning identity features that are invariant to covariates (e.g., the carrying status, clothing, walking speed, and viewing angle) and seldom involve learning features from the covariate aspect, which may lead to failure modes when variations due to the covariate overwhelm those due to the identity. We therefore propose a method of gait recognition via disentangled representation learning that considers both identity and covariate features. Specifically, we first encode an input gait template to get the disentangled identity and covariate features, and then decode the features to simultaneously reconstruct the input gait template and the canonical version of the same subject with no covariates in a semi-supervised manner to ensure successful disentanglement. We finally feed the disentangled identity features into a contrastive/triplet loss function for a verification/identification task. Moreover, we find that new gait templates can be synthesized by transferring the covariate feature from one subject to another. Experimental results on three publicly available gait data sets demonstrate the effectiveness of the proposed method compared with other state-of-the-art methods.",10.1109/cvpr42600.2020.01332
011a1bbb4059b703d9b366468ef9effdb49f4df9,Graph Structure Learning for Robust Graph Neural Networks,2020,"Graph Neural Networks (GNNs) are powerful tools in representation learning for graphs. However, recent studies show that GNNs are vulnerable to carefully-crafted perturbations, called adversarial attacks. Adversarial attacks can easily fool GNNs in making predictions for downstream tasks. The vulnerability to adversarial attacks has raised increasing concerns for applying GNNs in safety-critical applications. Therefore, developing robust algorithms to defend adversarial attacks is of great significance. A natural idea to defend adversarial attacks is to clean the perturbed graph. It is evident that real-world graphs share some intrinsic properties. For example, many real-world graphs are low-rank and sparse, and the features of two adjacent nodes tend to be similar. In fact, we find that adversarial attacks are likely to violate these graph properties. Therefore, in this paper, we explore these properties to defend adversarial attacks on graphs. In particular, we propose a general framework Pro-GNN, which can jointly learn a structural graph and a robust graph neural network model from the perturbed graph guided by these properties. Extensive experiments on real-world graphs demonstrate that the proposed framework achieves significantly better performance compared with the state-of-the-art defense methods, even when the graph is heavily perturbed. We release the implementation of Pro-GNN to our DeepRobust repository for adversarial attacks and defenses. The specific experimental settings to reproduce our results can be found in https://github.com/ChandlerBang/Pro-GNN.",10.1145/3394486.3403049
fbc3f2138f2bed5ca29aa902ebde3daedc369d99,Authoring tools for virtual reality experiences: a systematic review,2022,,10.1007/s11042-022-12829-9
2c9a9e9b382ac8841e5ed95f0b0bea344d214454,First-person Cinematographic Videogames,2021,"We present and explore the fruitfulness of “first-person cinematographic videogames,” a game model we have devised for the promotion of cultural, environmental, and territorial heritage. To support and foster the development of these type of games, we have developed a Web-based user-friendly authoring environment, extensively presented in the article. While employing standard first-person point-and-click game mechanics, the game model's distinctive feature is that the game environment is not based on a digital reconstruction (3D model) of the real-world settings but on cinematographic techniques combining videos and photos of existing places, integrating videoclips of mostly practical effects to obtain the interactivity typical of the first-person point-and-click adventure games. Our goal with such a game model is to mobilise mechanisms of engendering affection for real-world places when they become settings of the game world, arousing in the player forms of affection, attachment, and desire to visit them.",10.1145/3446977
c291d42c2fe91393bbd18d081ab8d47ec303bad8,A diachronic study determining syntactic and semantic features of Urdu-English neural machine translation,2023,,10.1016/j.heliyon.2023.e22883
e7f11c587f09ae41e0d3e4a508e20279a8ff59c4,A Corpus-Based Auto-encoder-and-Decoder Machine Translation Using Deep Neural Network for Translation from English to Telugu Language,2023,,10.1007/s42979-023-01678-4
011e11065f8a6c2ba9cd7b3197806b3a59fe4d43,A Constrained Reinforcement Learning Based Approach for Network Slicing,2020,"With the proliferation of mobile networks, we face strong diversification of services, demanding the current network to embed more flexibility. To satisfy this daring need, network slicing is embraced as a promising solution for resource utilization, in 5G and future networks. In network slicing, dynamic resource orchestration and network slice management are critical for resource efficiency. However, it is highly complicated such that the traditional approaches can not effectively perform resource orchestration due to the lack of accurate models and hidden problem structures. To address this challenge, we propose a constrained reinforcement learning based approach for network slicing. We formulate the resource allocation problem as a Constrained Markov Decision Process (CMDP) and solve it using constrained reinforcement learning algorithms. Specifically, we use the adaptive interior-point policy optimization and policy safety layer methods to deal with cumulative and instantaneous constraints. Our evaluations show that our method is effective in resource allocation with service demand guarantees and significantly outperforms baselines.",10.1109/ICNP49622.2020.9259378
bd38f50239bd7efa6697bb6d40a8c44785d1ddad,Prediction maintenance based on vibration analysis and deep learning - A case study of a drying press supported on a Hidden Markov Model,2024,,10.1016/j.asoc.2024.111885
5f5df3fc839496e06c88fa170db150d01ca98369,High-resolution reconstruction of non-motorized trajectory in shared space: A new approach integrating the social force model and particle filtering,2023,,10.1016/j.eswa.2023.120753
82730ec1e7ac4368e86b01ac3a861e5480d21534,Temporal Spiking Neural Networks with Synaptic Delay for Graph Reasoning,2024,"Spiking neural networks (SNNs) are investigated as biologically inspired models of neural computation, distinguished by their computational capability and energy efficiency due to precise spiking times and sparse spikes with event-driven computation. A significant question is how SNNs can emulate human-like graph-based reasoning of concepts and relations, especially leveraging the temporal domain optimally. This paper reveals that SNNs, when amalgamated with synaptic delay and temporal coding, are proficient in executing (knowledge) graph reasoning. It is elucidated that spiking time can function as an additional dimension to encode relation properties via a neural-generalized path formulation. Empirical results highlight the efficacy of temporal delay in relation processing and showcase exemplary performance in diverse graph reasoning tasks. The spiking model is theoretically estimated to achieve $20\times$ energy savings compared to non-spiking counterparts, deepening insights into the capabilities and potential of biologically inspired SNNs for efficient reasoning. The code is available at https://github.com/pkuxmq/GRSNN.",10.48550/arXiv.2405.16851
01285f118436d8078bac3400a7e7155a8e7f2375,"A Low-Cost, IMU-Based Real-Time On Device Gesture Recognition Glove",2020,"This paper evaluates the possibility of performing fine gesture recognition including finger movements on a low-tech device. In particular, we present a solution with a recognition model that is small enough to fit in the memory of a low- tech device and describe related difficulties associated with this approach. Several different Machine Learning techniques are employed and their individual advantages and drawbacks are explored for the task at hand. Our results indicate an average of 95% accuracy during real-time testing for an eight class decoding task with a custom Recurrent Neural Network approach, that runs on the low-tech device, namely an Arduino Nano 33 BLE. The novelty and strength of this research lies in the fact that we are able to recognize fine hand gestures including finger movements rather than recognizing only coarse hand gestures. The recognition process is conducted on the low-tech device and as a result this solution has all advantages that are typically associated with embedded systems, namely cost-efficiency, battery life efficiency, and a high degree of independence from other devices as well as compatibility with them.",10.1109/SMC42975.2020.9283231
595321242099676c4c6b3236683bead956930004,Random Channel Ablation for Robust Hand Gesture Classification with Multimodal Biosignals,2024,"Biosignal-based hand gesture classification is an important component of effective human-machine interaction. For multimodal biosignal sensing, the modalities often face data loss due to missing channels in the data which can adversely affect the gesture classification performance. To make the classifiers robust to missing channels in the data, this paper proposes using Random Channel Ablation (RChA) during the training process. Ultrasound and force myography (FMG) data were acquired from the forearm for 12 hand gestures over 2 subjects. The resulting multimodal data had 16 total channels, 8 for each modality. The proposed method was applied to convolutional neural network architecture, and compared with baseline, imputation, and oracle methods. Using 5-fold cross-validation for the two subjects, on average, 12.2% and 24.5% improvement was observed for gesture classification with up to 4 and 8 missing channels respectively compared to the baseline. Notably, the proposed method is also robust to an increase in the number of missing channels compared to other methods. These results show the efficacy of using random channel ablation to improve classifier robustness for multimodal and multi-channel biosignal-based hand gesture classification.",10.1109/EMBC53108.2024.10782851
012f29dce143f1eae82c56d4980a024e9874d79a,On the Impact of Perceptual Compression on Deep Learning,2020,"This paper proposes a fundamental answer to a frequently asked question in multimedia evaluation and data set creation: Do artifacts from perceptual compression contribute to error in the machine learning process and if so, how much? Our approach to the problem is an information reinterpretation of the Helmholtz free energy formula to explain the relationship between content and noise when using sensors (such as cameras or microphones) to capture multimedia data. The reinterpretation guides a bit-measurement of the noise contained in images, audio, and video by combining a classifier with perceptual compression, such as JPEG or MP3. Our experiments on CIFAR-10, ImageNet, and CSAIL Places as well as Fraunhofer's IDMT-SMT-Audio-Effects dataset indicate that, at the right quality level, perceptual compression is actually not harmful but contributes to a significant reduction of complexity of the machine learning process. That is, our noise quantification method can be used to speed up the training of deep learning classifiers significantly while maintaining, or sometimes even improving, overall classification accuracy.",10.1109/MIPR49039.2020.00052
796d75aa95a23324986103e406c74d2f64ddc5ee,Deep Layers Beware: Unraveling the Surprising Benefits of JPEG Compression for Image Classification Pre-processing,2023,"In this paper, we explore the intriguing effects of JPEG compression as a pre-processing technique for image classification tasks. Building upon the findings of a previous study by Friedland et al., which demonstrated that substantial JPEG compression does not significantly degrade classification accuracy, we investigate the potential benefits and limitations of this approach when applied to various classifiers, such as AutoGluon-multimodal and EfficientNet. Our experiments not only confirm the original results but also reveal notable I/O benefits, with compressed images occupying as little as 14 % of the original dataset size while maintaining comparable accuracy.Despite these promising findings, we also document several investigations that did not yield beneficial outcomes. We found no evidence to suggest that JPEG compression leads to faster model convergence or allows smaller models to achieve the same accuracy. Additionally, our experiments showed that tabular classifiers could not match the performance of deep neural networks when trained on JPEG-compressed input, and that JPEG compression does not make classifiers more resilient to noise in input images.Together, our results provide a comprehensive evaluation of JPEG compression as a pre-processing technique for image classification. While the approach offers undeniable benefits in terms of data storage and accuracy preservation, it does not appear to yield advantages in terms of model convergence, model size, or robustness to noise. This study contributes valuable insights for researchers and practitioners working in multimedia signal processing and image recognition, paving the way for further exploration and optimization of multimedia compression techniques.",10.1109/ISM59092.2023.00033
aa7359aaca24d89f342ce862421ade73f93b0dd9,An impact of tensor-based data compression methods on deep neural network accuracy,2021,"—The emergence of the deep neural architectures greatly inﬂuenced the contemporary big data revolution. However, requirements on large datasets even increased a necessity for efﬁcient data storage. The storage problem is present at all stages, from the dataset creation up to the training and prediction stages. However, compression algorithms can signiﬁcantly deteriorate the quality of data and in effect the classiﬁcation models. In this article, an in-depth analysis of the inﬂuence of the tensor-based lossy data compression on the performance of the various deep neural architectures is presented. We show that the Tucker and the Tensor Train decomposition methods, with properly selected parameters, allow for very high compression ratios, while conveying enough information in the decompressed data to achieve only a negligible or very small drop in the accuracy. The measurements were performed on the popular deep neural architectures: AlexNet, ResNet, VGG, and MNASNet. We show that further augmentation of the tensor decompositions with the ZFP ﬂoating-point compression algorithm allows for ﬁnding optimal parameters and even higher compressions ratios at the same recognition accuracy. Our experiments show data compressions of 94%-97% that result in less than 1% accuracy drop.",10.15439/2021F127
285ef30520a791fd5db205063bf418eac451f8ad,On the Convergence of Continual Learning with Adaptive Methods,2024,"One of the objectives of continual learning is to prevent catastrophic forgetting in learning multiple tasks sequentially, and the existing solutions have been driven by the conceptualization of the plasticity-stability dilemma. However, the convergence of continual learning for each sequential task is less studied so far. In this paper, we provide a convergence analysis of memory-based continual learning with stochastic gradient descent and empirical evidence that training current tasks causes the cumulative degradation of previous tasks. We propose an adaptive method for nonconvex continual learning (NCCL), which adjusts step sizes of both previous and current tasks with the gradients. The proposed method can achieve the same convergence rate as the SGD method when the catastrophic forgetting term which we define in the paper is suppressed at each iteration. Further, we demonstrate that the proposed algorithm improves the performance of continual learning over existing methods for several image classification tasks.",10.48550/arXiv.2404.05555
014731db28d89713d8dfd7ea06404f27a5218fff,Pseudo Labeling and Negative Feedback Learning for Large-Scale Multi-Label Domain Classification,2020,"In large-scale domain classification, an utterance can be handled by multiple domains with overlapped capabilities. However, only a limited number of ground-truth domains are provided for each training utterance in practice while knowing as many as correct target labels is helpful for improving the model performance. In this paper, given one ground-truth domain for each training utterance, we regard domains consistently predicted with the highest confidences as additional pseudo labels for the training. In order to reduce prediction errors due to incorrect pseudo labels, we leverage utterances with negative system responses to decrease the confidences of the incorrectly predicted domains. Evaluating on user utterances from an intelligent conversational system, we show that the proposed approach significantly improves the performance of domain classification with hypothesis reranking.",10.1109/ICASSP40776.2020.9053728
2d227f3a0203c2491bcf7c77ae594cea3a7caf89,Cluster-Guided Label Generation in Extreme Multi-Label Classification,2023,"For extreme multi-label classification (XMC), existing classification-based models poorly per- form for tail labels and often ignore the semantic relations among labels, like treating”Wikipedia” and “Wiki” as independent and separate labels. In this paper, we cast XMC as a generation task (XLGen), where we benefit from pre-trained text-to-text models. However, generating labels from the extremely large label space is challenging without any constraints or guidance. We, therefore, propose to guide label generation using label cluster information to hierarchically generate lower-level labels. We also find that frequency-based label ordering and using decoding ensemble methods are critical factors for the improvements in XLGen. XLGen with cluster guidance significantly outperforms the classification and generation baselines on tail labels, and also generally improves the overall performance in four popular XMC benchmarks. In human evaluation, we also find XLGen generates unseen but plausible labels. Our code is now available at https:// github.com/alexa/xlgen-eacl-2023.",10.48550/arXiv.2302.09150
85fe9855caa6d20f6866847f9ea7d3280d45ebb6,Federated Learning with Noisy User Feedback,2022,"Machine Learning (ML) systems are getting increasingly popular, and drive more and more applications and services in our daily life. Thishas led to growing concerns over user privacy, since human interaction data typically needs to be transmitted to the cloud in order to trainand improve such systems. Federated learning (FL) has recently emerged as a method for training ML models on edge devices using sensitive user data and is seen as a way to mitigate concerns over data privacy. However, since ML models are most commonly trained with label supervision, we need a way to extract labels on edge to make FL viable. In this work, we propose a strategy for training FL models using positive and negative user feedback. We also design a novel framework to study different noise patterns in user feedback, and explore how well standard noise-robust objectives can help mitigate this noise when training models in a federated setting. We evaluate our proposed training setup through detailed experiments on two text classification datasets and analyze the effects of varying levels of user reliability and feedback noise on model performance. We show that our method improves substantially over a self-training baseline, achieving performance closer to models trained with full supervision.",10.48550/arXiv.2205.03092
677974d992deb20254fc1aa238ff1ede9772ddba,Couple learning for semi-supervised sound event detection,2021,"The recently proposed Mean Teacher method, which exploits large-scale unlabeled data in a self-ensembling manner, has achieved state-of-the-art results in several semi-supervised learning benchmarks. Spurred by current achievements, this paper proposes an effective Couple Learning method that combines a well-trained model and a Mean Teacher model. The suggested pseudo-labels generated model (PLG) increases strongly- and weakly-labeled data to improve the Mean Teacher method-s performance. Moreover, the Mean Teacher-s consistency cost reduces the noise impact in the pseudo-labels introduced by detection errors. The experimental results on Task 4 of the DCASE2020 challenge demonstrate the superiority of the proposed method, achieving about 44.25% F1-score on the public evaluation set, significantly outperforming the baseline system-s 32.39%. At the same time, we also propose a simple and effective experiment called the Variable Order Input (VOI) experiment, which proves the significance of the Couple Learning method. Our developed Couple Learning code is available on GitHub.",10.21437/interspeech.2022-103
014c3a09de04d32e2fbee2bff693346a01fb0575,MONSTOR: An Inductive Approach for Estimating and Maximizing Influence over Unseen Networks,2020,"Influence maximization (IM) is one of the most important problems in social network analysis. Its objective is to find a given number of seed nodes that maximize the spread of information through a social network. Since it is an NP-hard problem, many approximate/heuristic methods have been developed, and a number of them repeat Monte Carlo (MC) simulations over and over to reliably estimate the influence (i.e., the number of infected nodes) of a seed set. In this work, we present an inductive machine learning method, called Monte Carlo Simulator (MONSTOR), for estimating the influence of given seed nodes in social networks unseen during training. To the best of our knowledge, MONSTOR is the first inductive method for this purpose. MONSTOR can greatly accelerate existing IM algorithms by replacing repeated MC simulations. In our experiments, MONSTOR provided highly accurate estimates, achieving 0.998 or higher Pearson and Spearman correlation coefficients in unseen real-world social networks. Moreover, IM algorithms equipped with MONSTOR are more accurate than state-of-the-art competitors in 63% of IM use cases.",10.1109/ASONAM49781.2020.9381460
631a7565ff91187a03ea4e62d81c8660f49b429a,AdaRisk: Risk-Adaptive Deep Reinforcement Learning for Vulnerable Nodes Detection,2024,"Vulnerable node detection in uncertain graphs is a typical graph mining problem that seeks to identify nodes at a high risk of breakdown under the joint effect from both the self and contagion risk probability. This is an NP-hard problem that is crucial for risk management in many real-world applications such as networked loans and smart grids. Monte Carlo (MC) simulation and its optimized algorithms are commonly used to approximate the breakdown probability, but these methods require a large number of samples to ensure accuracy, which is computationally expensive for large-scale networks. Although recent studies employ Graph Neural Networks (GNNs) to model the contagion process and accelerate the inference, many of these methods suffer from the over-smoothing problem, leading to suboptimal performance under the long-distance risk contagion process. To this end, we propose a novel risk-adaptive deep reinforcement learning-based framework (AdaRisk) for vulnerable nodes detection in uncertain graphs. In particular, we design the Markov Decision Process (MDP) of the vulnerability estimation process in which our agent would approach the risk adaptively based on contagion probability accumulated in prior iterations. To encode state embeddings that incorporate multi-hop contagion information, the agent utilizes a long-distance adaptable policy network to process the input graph and output actions as the vulnerable probability of nodes. We conducted extensive experiments on four benchmark networks and three real-world financial networks to evaluate our proposed framework's performance. Our results demonstrate that AdaRisk outperforms state-of-the-art baselines in terms of detection performance, and also offers significant running time reductions compared to MC simulation.",10.1109/TKDE.2024.3409869
12959b619828731d03f57653b561cd84c7020f9a,Modeling Personalized Retweeting Behaviors for Multi-Stage Cascade Popularity Prediction,2024,"Predicting the size of message cascades is critical in various applications, such as online advertising and early detection of rumors. However, most existing deep learning approaches rely on cascade observation, which hinders accurate cascade prediction before message posting. Besides, these approaches overlook personalized retweeting behaviors that reflect users' inclination to retweeting specific types of information. In this study, we propose a universal cascade prediction framework, namely Cascade prediction regarding Multiple Stage (CasMS), that effectively predicts cascade popularity across message generation stage as well as short-term and long-term stages. Unlike previous methods, our approach not only captures users' personalized retweeting behaviors but also incorporates temporal cascade features. We perform the experiments in datasets collected ourselves as well as public datasets. The results show that our method significantly surpasses existing approaches in predicting the cascade during the message generation stage and different time periods in the cascade dynamics.",10.24963/ijcai.2024/287
019fa05e2b6458fe57103b31c23675db6a9200eb,PGSL: A probabilistic graph diffusion model for source localization,2023,,10.1016/j.eswa.2023.122028
a0507e1b2a0bbe79b97740a296dcc31fbb51b6d9,DySuse: Susceptibility Estimation in Dynamic Social Networks,2023,,10.1016/j.eswa.2023.121042
653b406e03cac1c03f38e47211c3156f7bbe2c2f,Deep Graph Representation Learning and Optimization for Influence Maximization,2023,"Influence maximization (IM) is formulated as selecting a set of initial users from a social network to maximize the expected number of influenced users. Researchers have made great progress in designing various traditional methods, and their theoretical design and performance gain are close to a limit. In the past few years, learning-based IM methods have emerged to achieve stronger generalization ability to unknown graphs than traditional ones. However, the development of learning-based IM methods is still limited by fundamental obstacles, including 1) the difficulty of effectively solving the objective function; 2) the difficulty of characterizing the diversified underlying diffusion patterns; and 3) the difficulty of adapting the solution under various node-centrality-constrained IM variants. To cope with the above challenges, we design a novel framework DeepIM to generatively characterize the latent representation of seed sets, and we propose to learn the diversified information diffusion pattern in a data-driven and end-to-end manner. Finally, we design a novel objective function to infer optimal seed sets under flexible node-centrality-based budget constraints. Extensive analyses are conducted over both synthetic and real-world datasets to demonstrate the overall performance of DeepIM. The code and data are available at: https://github.com/triplej0079/DeepIM.",10.48550/arXiv.2305.02200
8dc63d91d85c3c9abee897db4a89a254cd6ee296,CMINet: a Graph Learning Framework for Content-aware Multi-channel Influence Diffusion,2023,"The phenomena of influence diffusion on social networks have received tremendous research interests in the past decade. While most prior works mainly focus on predicting the total influence spread on a single network, a marketing campaign that exploits influence diffusion often involves multiple channels with various information disseminated on different media. In this paper, we introduce a new influence estimation problem, namely Content-aware Multi-channel Influence Diffusion (CMID), and accordingly propose CMINet to predict newly influenced users, given a set of seed users with different multimedia contents. In CMINet, we first introduce DiffGNN to encode the influencing power of users (nodes) and Influence-aware Optimal Transport (IOT) to align the embeddings to address the distribution shift across different diffusion channels. Then, we transform CMID into a node classification problem and propose Social-based Multimedia Feature Extractor (SMFE) and Content-aware Multi-channel Influence Propagation (CMIP) to jointly learn the user preferences on multimedia contents and predict the susceptibility of users. Furthermore, we prove that CMINet preserves monotonicity and submodularity, thus enabling (1 − 1/e)-approximate solutions for influence maximization. Experimental results manifest that CMINet outperforms eleven baselines on three public datasets.",10.1145/3543507.3583465
47e3c3f9f2f5d40d85a339228793dd047370abf4,Social-Inverse: Inverse Decision-making of Social Contagion Management with Task Migrations,2022,"Considering two decision-making tasks $A$ and $B$, each of which wishes to compute an effective \textit{decision} $Y$ for a given \textit{query} $X$, {can we solve task $B$ by using query-decision pairs $(X, Y)$ of $A$ without knowing the latent decision-making model?} Such problems, called \textit{inverse decision-making with task migrations}, are of interest in that the complex and stochastic nature of real-world applications often prevents the agent from completely knowing the underlying system. In this paper, we introduce such a new problem with formal formulations and present a generic framework for addressing decision-making tasks in social contagion management. On the theory side, we present a generalization analysis for justifying the learning performance of our framework. In empirical studies, we perform a sanity check and compare the presented method with other possible learning-based and graph-based methods. We have acquired promising experimental results, confirming for the first time that it is possible to solve one decision-making task by using the solutions associated with another one.",10.48550/arXiv.2209.10493
014d0d142b82173e588ee812820e40a543cc2f4c,Mining Emotions on Plutchik's Wheel,2020,"Tweets embed rich information about users' moods, emotions and feelings. Mining for these latent emotions can offer clues about users' affective state on a broad range of topics ranging from their mental health to political opinions. This paper proposes a supervised machine learning approach to detect emotions from tweets. The approach is built around a Crowdfiower data set of 40,000 tweets labeled with 13 distinct emotions. These 13 labels were mapped to emotions guided by the Plutchik's wheel, and are further organized into pairs of polar opposites leading to four binary classification problems: Love vs. Hate, Joy vs. Sadness, Trust vs. Disgust, and Anticipation vs. Surprise. For each classification problem, five supervised machine learning models were trained on a combination of linguistic and metadata features extracted from the tweets. The performance of these models is evaluated using sensitivity, specificity, accuracy and AUC. These results suggest that Random Forest and Support Vector Machine classifiers show close to highest accuracy in distinguishing between pair-wise emotions. Although the emotions in each pair are polar opposites on the Plutchik's wheel, their classification performance differs widely; distinguishing between Love vs. Hate and Anticipation vs Surprise show the highest (87%) and lowest (77%) accuracy respectively. Feature importance splits the discriminating power 60% - 40% over linguistic and metadata features. Our results thus suggest that every pair of polar opposite emotions are not equally differentiable, and using both linguistic and metadata features leads to better accuracy over exclusively using text-based or sentiment features.",10.1109/SNAMS52053.2020.9336534
da3fadeb9b55efdf8d7ac1b163e7114f3d952565,"Deep Learning Through A Telescoping Lens: A Simple Model Provides Empirical Insights On Grokking, Gradient Boosting & Beyond",2024,"Deep learning sometimes appears to work in unexpected ways. In pursuit of a deeper understanding of its surprising behaviors, we investigate the utility of a simple yet accurate model of a trained neural network consisting of a sequence of first-order approximations telescoping out into a single empirically operational tool for practical analysis. Across three case studies, we illustrate how it can be applied to derive new empirical insights on a diverse range of prominent phenomena in the literature -- including double descent, grokking, linear mode connectivity, and the challenges of applying deep learning on tabular data -- highlighting that this model allows us to construct and extract metrics that help predict and understand the a priori unexpected performance of neural networks. We also demonstrate that this model presents a pedagogical formalism allowing us to isolate components of the training process even in complex contemporary settings, providing a lens to reason about the effects of design choices such as architecture&optimization strategy, and reveals surprising parallels between neural network learning and gradient boosting.",10.48550/arXiv.2411.00247
014f3b9030d97bd18c5e732f2d7e54aa9b2c7d15,Pathloss Prediction using Deep Learning with Applications to Cellular Optimization and Efficient D2D Link Scheduling,2020,"In this paper we propose a highly efficient and very accurate method for estimating the propagation pathloss from a point x to all points y on the 2D plane. Our method, termed RadioUNet, is a deep neural network. For applications such as user-cell site association and device-to-device (D2D) link scheduling, an accurate knowledge of the pathloss function for all pairs of locations is very important. Commonly used statistical models approximate the pathloss as a decaying function of the distance between the points. However, in realistic propagation environments characterized by the presence of buildings, street canyons, and objects at different heights, such radial-symmetric functions yield very misleading results. In this paper we show that properly designed and trained deep neural networks are able to learn how to estimate the pathloss function, given an urban environment, very accurately and extremely quickly. Our proposed method generates pathloss estimations that are very close to estimations given by physical simulation, but much faster. Moreover, experimental results show that our method significantly outperforms previously proposed methods based on radial basis function interpolation and tensor completion.",10.1109/ICASSP40776.2020.9053347
01508f386eb2ca5181fde7bb6da4920e250d7498,Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing,2020,"We propose the use of a sequence-to-sequence paraphraser for automatic machine translation evaluation. The paraphraser takes a human reference as input and then force-decodes and scores an MT system output. We propose training the aforementioned paraphraser as a multilingual NMT system, treating paraphrasing as a zero-shot ""language pair"" (e.g., Russian to Russian). We denote our paraphraser ""unbiased"" because the mode of our model's output probability is centered around a copy of the input sequence, which in our case represent the best case scenario where the MT system output matches a human reference. Our method is simple and intuitive, and our single model (trained in 39 languages) outperforms or statistically ties with all prior metrics on the WMT19 segment-level shared metrics task in all languages, excluding Gujarati where the model had no training data. We also explore using our model conditioned on the source instead of the reference, and find that it outperforms every quality estimation as a metric system from the WMT19 shared task on quality estimation by a statistically significant margin in every language pair.",10.18653/v1/2020.emnlp-main.8
015639f092128b23af4f5db5c28d50ac0bcd742f,Contrastive Learning for Weakly Supervised Phrase Grounding,2020,"Phrase grounding, the problem of associating image regions to caption words, is a crucial component of vision-language tasks. We show that phrase grounding can be learned by optimizing word-region attention to maximize a lower bound on mutual information between images and caption words. Given pairs of images and captions, we maximize compatibility of the attention-weighted regions and the words in the corresponding caption, compared to non-corresponding pairs of images and captions. A key idea is to construct effective negative captions for learning through language model guided word substitutions. Training with our negatives yields a $\sim10\%$ absolute gain in accuracy over randomly-sampled negatives from the training data. Our weakly supervised phrase grounding model trained on COCO-Captions shows a healthy gain of $5.7\%$ to achieve $76.7\%$ accuracy on Flickr30K Entities benchmark.",10.1007/978-3-030-58580-8_44
0156ba27dc5a09eb7ce757291709b6d47c00fb13,Lossy Medical Image Compression using Residual Learning-based Dual Autoencoder Model,2020,"In this work, we propose a two-stage autoencoder based compressor-decompressor framework for compressing malaria RBC cell image patches. We know that the medical images used for disease diagnosis are around multiple gigabytes size, which is quite huge. The proposed residual-based dual autoencoder network is trained to extract the unique features which are then used to reconstruct the original image through the decompressor module. The two latent space representations (first for the original image and second for the residual image) are used to rebuild the final original image. Color-SSIM has been exclusively used to check the quality of the chrominance part of the cell images after decompression. The empirical results indicate that the proposed work outperformed other neural network related compression technique for medical images by approximately 35%, 10% and 5% in PSNR, Color SSIM and MS-SSIM respectively. The algorithm exhibits a significant improvement in bit savings of 76%, 78%, 75% & 74% over JPEG-LS, JP2K-LM, CALIC and recent neural network approach respectively, making it a good compression-decompression technique.",10.1109/UPCON50219.2020.9376417
863eedf42803549b9853a0dcaec7c13ed90ddf7e,ROI and Non-ROI Image Compression Using Optimal Zero Tree Wavelet and Enhanced Convolutional Neural Network for MRI Images,2023,,10.1007/s42979-023-02335-6
ea44cd42a3377a25ab8c08c598af7c4fe4cd70ef,A Novel Implicit Neural Representation for Volume Data,2023,"The storage of medical images is one of the challenges in the medical imaging field. There are variable works that use implicit neural representation (INR) to compress volumetric medical images. However, there is room to improve the compression rate for volumetric medical images. Most of the INR techniques need a huge amount of GPU memory and a long training time for high-quality medical volume rendering. In this paper, we present a novel implicit neural representation to compress volume data using our proposed architecture, that is, the Lanczos downsampling scheme, SIREN deep network, and SRDenseNet high-resolution scheme. Our architecture can effectively reduce training time, and gain a high compression rate while retaining the final rendering quality. Moreover, it can save GPU memory in comparison with the existing works. The experiments show that the quality of reconstructed images and training speed using our architecture is higher than current works which use the SIREN only. Besides, the GPU memory cost is evidently decreased.",10.3390/app13053242
c8198282b45ca3f539f5dc4d323ed739afd29f21,An Improved Medical Image Compression Method Based on Wavelet Difference Reduction,2023,"Advanced microscopic techniques such as high-throughput, high-content, multispectral, and 3D imaging could include many images per experiment requiring hundreds of gigabytes (GBs) of memory. Efficient lossy image-compression methods such as joint photographic experts group (JPEG) and JPEG 2000 are crucial to managing these large amounts of data. However, these methods can get visual quality with high compression ratios but do not necessarily maintain the medical data and information integrity. This paper proposes a novel and improved medical image compression method based on color wavelet difference reduction. Specifically, the proposed method is an extension of the standard wavelet difference reduction (WDR) method using mean co-located pixel difference to select the optimum quantity of color images that present the highest similarity in the spatial and temporal domain. The images with large spatiotemporal coherence are encoded as one volume and evaluated regarding the peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM). The proposed method is evaluated in the challenging histopathological microscopy image analysis field using 31 slides of colorectal cancer. It is found that the perceptual quality of the medical image is remarkably high. The results indicate that the PSNR improvement over existing schemes may reach up to 22.65 dB compared to JPEG 2000. Also, it can reach up to 10.33dB compared to a method utilizing discrete wavelet transform (DWT), leading us to implement a mobile and web platform that can be used for compressing and transmitting microscopic medical images in real time.",10.1109/ACCESS.2023.3246948
015e57a87d400a8c5d83a5854529e5cd52c87a02,Securing Bring-Your-Own-Device (BYOD) Programming Exams,2020,"Traditional pen and paper exams are inadequate for modern university programming courses as they are misaligned with pedagogies and learning objectives that target practical coding ability. Unfortunately, many institutions lack the resources or space to be able to run assessments in dedicated computer labs. This has motivated the development of bring-your-own-device~(BYOD) exam formats, allowing students to program in a similar environment to how they learnt, but presenting instructors with significant additional challenges in preventing plagiarism and cheating. In this paper, we describe a BYOD exam solution based on lockdown browsers, software which temporarily turns students' laptops into secure workstations with limited system or internet access. We combine the use of this technology with a learning management system and cloud-based programming tool to facilitate conceptual and practical programming questions that can be tackled in an interactive but controlled environment. We reflect on our experience of implementing this solution for a major undergraduate programming course, highlighting our principal lesson that policies and support mechanisms are as important to consider as the technology itself.",10.1145/3328778.3366907
da86cbe6ea4571301ab0a3bdfaea5df84c2912e3,Assessing Software Development Competences Constructively Aligned in an Open-Web Format,2024,"To assess in a constructively aligned way those programming competences that are relevant for the professional practice of future software developers, an assessment format would be suitable where students actively program, within the integrated development environment (IDE) that they are individually used to, and with unrestricted access for researching in the web. A format that accommodates these needs well is “Bring Your Own Device, Open Book, Open Web”, where students work on their own devices against given git repositories, with full access on knowledge bases and the internet. In this work, we share experiences and suggest well established practices for executing this exam type. As well, we discuss how the ready availability of large language models impacts this assessment type.",10.1109/CSEET62301.2024.10662997
0d03c69b7ff2c6894c94a44295847a91c0471d34,Comparing the Security of Three Proctoring Regimens for Bring-Your-Own-Device Exams,2024,"We compare the exam security of three proctoring regimens of Bring-Your-Own-Device, synchronous, computer-based exams in a computer science class: online un-proctored, online proctored via Zoom, and in-person proctored. We performed two randomized crossover experiments to compare these proctoring regimens. The first study measured the score advantage students receive while taking un-proctored online exams over Zoom-proctored online exams. The second study measured the score advantage of students taking Zoom-proctored online exams over in-person proctored exams. In both studies, students took six 50-minute exams using their own devices, which included two coding questions and 8--10 non-coding questions. We find that students score 2.3% higher on non-coding questions when taking exams in the un-proctored format compared to Zoom proctoring. No statistically significant advantage was found for the coding questions. While most of the non-coding questions had randomization such that students got different versions, for the few questions where all students received the same exact version, the score advantage escalated to 5.2%. From the second study, we find no statistically significant difference between students' performance on Zoom-proctored vs. in-person proctored exams. With this, we recommend educators incorporate some form of proctoring along with question randomization to mitigate cheating concerns in BYOD exams.",10.1145/3626252.3630809
c948f8ac91630f5b894de8b7623235e7d8fc44a3,"Executable Exams: Taxonomy, Implementation and Prospects",2023,"Traditionally exams in introductory programming courses have tended to be multiple choice, or ""paper-based"" coding exams in which students hand write code. This does not reflect how students typically write and are assessed on programming assignments in which they write code on a computer and are able to validate and assess their code using an auto-grading system. Executable exams are exams in which students are given programming problems, write code using a computer within a development environment and submissions are digitally validated or executed. This format is far more consistent with how students engage in programming assignments. This paper explores the executable exam format and attempts to gauge the state-of-the-practice and how prevalent it is. First, we formulate a taxonomy of characteristics of executable exams, identifying common aspects and various levels of flexibility. then give two case studies: one in which executable exams have been utilized for nearly 10 years and another in which they've been recently adopted. Finally, we provide results from faculty surveys providing evidence that, though not standard practice, the use of executable exams is not uncommon and appears to be on the rise.",10.1145/3545945.3569724
015fa16f05f3989f1664a9a91eb5b48a4c1173a5,Multivariate Adaptive Gaussian Mixture for Scene Level Anomaly Modeling,2020,"Scene changes that typically occur in a real-world setting degrade anomaly detection performance over the long run. Most of the existing methods ignore the challenge of temporal concept drift in video surveillance. In this paper, we propose an unsupervised end-to-end framework for adaptive scene level anomaly detection. We utilize multivariate Gaussian mixtures for adaptive scene learning. The mixture represents the possible distribution of normal and abnormal events shown till now. The distribution adapts itself according to the slow scene changes. We introduce a Mahalanobis distance-based contribution factor to update mixture parameters on the arrival of each new event. A detailed discussion and experiments are conducted to decide optimum local as well as global temporal context. The existing public datasets for anomaly detection are of very short duration (maximum of 1.5 hours) to be used for evaluating adaptive approaches. Therefore we also collected a longer duration dataset of continuous 10 hours duration. We achieved a promising performance of 85.14% AUC and 21.26% EER on this data.",10.1109/BigMM50055.2020.00018
1bf528547bf19f36993a5786b86de43ee11b5c05,Concept Drift Challenge in Multimedia Anomaly Detection: A Case Study with Facial Datasets,2022,"Anomaly detection in multimedia datasets is a widely studied area. Yet, the concept drift challenge in data has been ignored or poorly handled by the majority of the anomaly detection frameworks. The state-of-the-art approaches assume that the data distribution at training and deployment time will be the same. However, due to various real-life environmental factors, the data may encounter drift in its distribution or can drift from one class to another in the late future. Thus, a one-time trained model might not perform adequately. In this paper, we systematically investigate the effect of concept drift on various detection models and propose a modified Adaptive Gaussian Mixture Model (AGMM) based framework for anomaly detection in multimedia data. In contrast to the baseline AGMM, the proposed extension of AGMM remembers the past for a longer period in order to handle the drift better. Extensive experimental analysis shows that the proposed model better handles the drift in data as compared with the baseline AGMM. Further, to facilitate research and comparison with the proposed framework, we contribute three multimedia datasets constituting faces as samples. The face samples of individuals correspond to the age difference of more than ten years to incorporate a longer temporal context.",10.48550/arXiv.2207.13430
90ffa7f8c3754a136d2a343e5a1aed0182bcc73a,Situational Anomaly Detection in Multimedia Data under Concept Drift,2021,"Anomaly detection has been a very challenging and active area of research for decades, particularly for video surveillance. However, most of the works detect predefined anomaly classes using static models. These frameworks have limited applicability for real-life surveillance where the data have concept drift. Under concept drift, the distribution of both normal and anomaly classes changes over time. An event may change its class from anomaly to normal or vice-versa. The non-adaptive frameworks do not handle this drift. Additionally, the focus has been on detecting local anomalies, such as a region of an image. In contrast, in CCTV-based monitoring, flagging unseen anomalous situations can be of greater interest. Utilizing multiple sensory information for anomaly detection has also received less attention. This extended abstract discusses these gaps and possible solutions.",10.1145/3474085.3481033
01637f04eac8523b6c4887d419bd718f65860982,Removing the Background by Adding the Background: Towards Background Robust Self-supervised Video Representation Learning,2020,"Self-supervised learning has shown great potentials in improving the video representation ability of deep neural networks by getting supervision from the data itself. However, some of the current methods tend to cheat from the background, i.e., the prediction is highly dependent on the video background instead of the motion, making the model vulnerable to background changes. To mitigate the model reliance towards the background, we propose to remove the background impact by adding the background. That is, given a video, we randomly select a static frame and add it to every other frames to construct a distracting video sample. Then we force the model to pull the feature of the distracting video and the feature of the original video closer, so that the model is explicitly restricted to resist the background influence, focusing more on the motion changes. We term our method as Background Erasing (BE). It is worth noting that the implementation of our method is so simple and neat and can be added to most of the SOTA methods without much efforts. Specifically, BE brings 16.4% and 19.1% improvements with MoCo on the severely biased datasets UCF101 and HMDB51, and 14.5% improvement on the less biased dataset Diving48.",10.1109/CVPR46437.2021.01163
01671bddce80f01ade92a06d9e469ca23ac6e72e,Improving Monocular Depth Estimation by Leveraging Structural Awareness and Complementary Datasets,2020,"Monocular depth estimation plays a crucial role in 3D recognition and understanding. One key limitation of existing approaches lies in their lack of structural information exploitation, which leads to inaccurate spatial layout, discontinuous surface, and ambiguous boundaries. In this paper, we tackle this problem in three aspects. First, to exploit the spatial relationship of visual features, we propose a structure-aware neural network with spatial attention blocks. These blocks guide the network attention to global structures or local details across different feature layers. Second, we introduce a global focal relative loss for uniform point pairs to enhance spatial constraint in the prediction, and explicitly increase the penalty on errors in depth-wise discontinuous regions, which helps preserve the sharpness of estimation results. Finally, based on analysis of failure cases for prior methods, we collect a new Hard Case (HC) Depth dataset of challenging scenes, such as special lighting conditions, dynamic objects, and tilted camera angles. The new dataset is leveraged by an informed learning curriculum that mixes training examples incrementally to handle diverse data distributions. Experimental results show that our method outperforms state-of-the-art approaches by a large margin in terms of both prediction accuracy on NYUDv2 dataset and generalization performance on unseen datasets.",10.1007/978-3-030-58568-6_6
1064f3cac8b9477afe86ea7f9c10f9e499a3e3db,Deep Learning-based Depth Estimation Methods from Monocular Image and Videos: A Comprehensive Survey,2024,"Estimating depth from single RGB images and videos is of widespread interest due to its applications in many areas, including autonomous driving, 3D reconstruction, digital entertainment, and robotics. More than 500 deep learning-based papers have been published in the past 10 years, which indicates the growing interest in the task. This paper presents a comprehensive survey of the existing deep learning-based methods, the challenges they address, and how they have evolved in their architecture and supervision methods. It provides a taxonomy for classifying the current work based on their input and output modalities, network architectures, and learning methods. It also discusses the major milestones in the history of monocular depth estimation, and different pipelines, datasets, and evaluation metrics used in existing methods.",10.1145/3677327
016b0854e8dce422d77297d1bec1ef0ba43ef0e0,AsymNVM: An Efficient Framework for Implementing Persistent Data Structures on Asymmetric NVM Architecture,2020,"The byte-addressable non-volatile memory (NVM) is a promising technology since it simultaneously provides DRAM-like performance, disk-like capacity, and persistency. The current NVM deployment with byte-addressability is \em symmetric, where NVM devices are directly attached to servers. Due to the higher density, NVM provides much larger capacity and should be shared among servers. Unfortunately, in the symmetric setting, the availability of NVM devices is affected by the specific machine it is attached to. High availability can be achieved by replicating data to NVM on a remote machine. However, it requires full replication of data structure in local memory --- limiting the size of the working set. This paper rethinks NVM deployment and makes a case for the \em asymmetric byte-addressable non-volatile memory architecture, which decouples servers from persistent data storage. In the proposed \em \anvm architecture, NVM devices (i.e., back-end nodes) can be shared by multiple servers (i.e., front-end nodes) and provide recoverable persistent data structures. The asymmetric architecture, which follows the industry trend of \em resource disaggregation, is made possible due to the high-performance network (e.g., RDMA). At the same time, \anvm leads to a number of key problems such as, still relatively long network latency, persistency bottleneck, and simple interface of the back-end NVM nodes. We build \em \anvm framework based on \anvm architecture that implements: 1) high performance persistent data structure update; 2) NVM data management; 3) concurrency control; and 4) crash-consistency and replication. The key idea to remove persistency bottleneck is the use of \em operation log that reduces stall time due to RDMA writes and enables efficient batching and caching in front-end nodes. To evaluate performance, we construct eight widely used data structures and two transaction applications based on \anvm framework. In a 10-node cluster equipped with real NVM devices, results show that \anvm achieves similar or better performance compared to the best possible symmetric architecture while enjoying the benefits of disaggregation. We found the speedup brought by the proposed optimizations is drastic, --- 5$\sim$12× among all benchmarks.",10.1145/3373376.3378511
eb6f5ab4f46a38203dc80bbd9b39449ed8dedbba,TrEnv: Transparently Share Serverless Execution Environments Across Different Functions and Nodes,2024,,10.1145/3694715.3695967
9d3cc8aad60aafd120ff285d6b5f82a16831f7f1,SWARM: Replicating Shared Disaggregated-Memory Data in No Time,2024,"Memory disaggregation is an emerging data center architecture that improves resource utilization and scalability. Replication is key to ensure the fault tolerance of applications, but replicating shared data in disaggregated memory is hard. We propose SWARM (Swift WAit-free Replication in disaggregated Memory), the first replication scheme for in-disaggregated-memory shared objects to provide (1) single-roundtrip reads and writes in the common case, (2) strong consistency (linearizability), and (3) strong liveness (wait-freedom). SWARM makes two independent contributions. The first is Safe-Guess, a novel wait-free replication protocol with single-roundtrip operations. The second is In-n-Out, a novel technique to provide conditional atomic update and atomic retrieval of large buffers in disaggregated memory in one roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly consistent and highly available disaggregated key-value store. We evaluate SWARM-KV and find that it has marginal latency overhead compared to an unreplicated key-value store, and that it offers much lower latency and better availability than FUSEE, a state-of-the-art replicated disaggregated key-value store.",10.1145/3694715.3695945
a7755e689249cfb1a2cced699e85d7c84e57b2e5,Scaling Up Memory Disaggregated Applications with SMART,2024,"Recent developments in RDMA networks are leading to the trend of memory disaggregation. However, the performance of each compute node is still limited by the network, especially when it needs to perform a large number of concurrent fine-grained remote accesses. According to our evaluations, existing IOPS-bound disaggregated applications do not scale well beyond 32 cores, and therefore do not take full advantage of today's many-core machines. After an in-depth analysis of the internal architecture of RNIC, we found three major scale-up bottlenecks that limit the throughput of today's disaggregated applications: (1) implicit contention of doorbell registers, (2) cache trashing caused by excessive outstanding work requests, and (3) wasted IOPS from unsuccessful CAS retries. However, the solutions to these problems involve many low-level details that are not familiar to application developers. To ease the burden on developers, we propose Smart, an RDMA programming framework that hides the above details by providing an interface similar to one-sided RDMA verbs. We take 44 and 16 lines of code to refactor the state-of-the-art disaggregated hash table (RACE) and persistent transaction processing system (FORD) with Smart, improving their throughput by up to 132.4× and 5.2×, respectively. We have also refactored Sherman (a recent disaggregated B+Tree) with Smart and an additional speculative lookup optimization (48 lines of code changed), which changes its memory access pattern from bandwidth-bound to IOPS-bound and leads to a speedup of 2.0×. Smart is publicly available at https://github.com/madsys-dev/smart.",10.1145/3617232.3624857
016ca039d9f5220c96b26f15d90d82064c361bfa,Learning from Task Descriptions,2020,"Typically, machine learning systems solve new tasks by training on thousands of examples. In contrast, humans can solve new tasks by reading some instructions, with perhaps an example or two. To take a step toward closing this gap, we introduce a framework for developing NLP systems that solve new tasks after reading their descriptions, synthesizing prior work in this area. We instantiate this framework with a new English language dataset, ZEST, structured for task-oriented evaluation on unseen tasks. Formulating task descriptions as questions, we ensure each is general enough to apply to many possible inputs, thus comprehensively evaluating a model's ability to solve each task. Moreover, the dataset's structure tests specific types of systematic generalization. We find that the state-of-the-art T5 model achieves a score of 12% on ZEST, leaving a significant challenge for NLP researchers.",10.18653/v1/2020.emnlp-main.105
349dde7c0b8d45e7aab6516588e4ca764d8a4bda,From Instance Training to Instruction Learning: Task Adapters Generation from Instructions,2024,"Large language models (LLMs) have acquired the ability to solve general tasks by utilizing instruction finetuning (IFT). However, IFT still relies heavily on instance training of extensive task data, which greatly limits the adaptability of LLMs to real-world scenarios where labeled task instances are scarce and broader task generalization becomes paramount. Contrary to LLMs, humans acquire skills and complete tasks not merely through repeated practice but also by understanding and following instructional guidelines. This paper is dedicated to simulating human learning to address the shortcomings of instance training, focusing on instruction learning to enhance cross-task generalization. Within this context, we introduce Task Adapters Generation from Instructions (TAGI), which automatically constructs the task-specific model in a parameter generation manner based on the given task instructions without retraining for unseen tasks. Specifically, we utilize knowledge distillation to enhance the consistency between TAGI developed through Learning with Instruction and task-specific models developed through Training with Instance, by aligning the labels, output logits, and adapter parameters between them. TAGI is endowed with cross-task generalization capabilities through a two-stage training process that includes hypernetwork pretraining and finetuning. We evaluate TAGI on the Super-Natural Instructions and P3 datasets. The experimental results demonstrate that TAGI can match or even outperform traditional meta-trained models and other hypernetwork models, while significantly reducing computational requirements.",10.48550/arXiv.2406.12382
f5ef3d2be60157c7791f2a137d53ee8e7d1431eb,Dual Alignment Unsupervised Domain Adaptation for Video-Text Retrieval,2023,"Video-text retrieval is an emerging stream in both computer vision and natural language processing communities, which aims to find relevant videos given text queries. In this paper, we study the notoriously challenging task, i.e., Unsupervised Domain Adaptation Video-text Retrieval (UDAVR), wherein training and testing data come from different distributions. Previous works merely alleviate the domain shift, which however overlook the pairwise misalignment issue in target domain, i.e., there exist no semantic relationships between target videos and texts. To tackle this, we propose a novel method named Dual Alignment Domain Adaptation (DADA). Specifically, we first introduce the cross-modal semantic embedding to generate discriminative source features in a joint embedding space. Besides, we utilize the video and text domain adaptations to smoothly balance the minimization of the domain shifts. To tackle the pairwise misalignment in target domain, we propose the Dual Alignment Consistency (DAC) to fully exploit the semantic information of both modalities in target domain. The proposed DAC adaptively aligns the video-text pairs which are more likely to be relevant in target domain, enabling that positive pairs are increasing progressively and the noisy ones will potentially be aligned in the later stages. To that end, our method can generate more truly aligned target pairs and ensure the discriminability of target features. Compared with the state-of-the-art methods, DADA achieves 20.18% and 18.61% relative improvements on R@1 under the setting of TGIF→MSR-VTT and TGIF→MSVD respectively, demonstrating the superiority of our method.",10.1109/CVPR52729.2023.01818
0174d608bd44ad7148e1c44612f8f31cfd9f3c42,Simple or Complex? Learning to Predict Readability of Bengali Texts,2020,"Determining the readability of a text is the first step to its simplification. In this paper, we present a readability analysis tool capable of analyzing text written in the Bengali language to provide in-depth information on its readability and complexity. Despite being the 7th most spoken language in the world with 230 million native speakers, Bengali suffers from a lack of fundamental resources for natural language processing. Readability related research of the Bengali language so far can be considered to be narrow and sometimes faulty due to the lack of resources. Therefore, we correctly adopt document-level readability formulas traditionally used for U.S. based education system to the Bengali language with a proper age-to-age comparison. Due to the unavailability of large-scale human-annotated corpora, we further divide the document-level task into sentence-level and experiment with neural architectures, which will serve as a baseline for the future works of Bengali readability prediction. During the process, we present several human-annotated corpora and dictionaries such as a document-level dataset comprising 618 documents with 12 different grade levels, a large-scale sentence-level dataset comprising more than 96K sentences with simple and complex labels, a consonant conjunct count algorithm and a corpus of 341 words to validate the effectiveness of the algorithm, a list of 3,396 easy words, and an updated pronunciation dictionary with more than 67K words. These resources can be useful for several other tasks of this low-resource language.",10.1609/aaai.v35i14.17495
fd965454b40c455b7251c8e9a25089dddfaf15c6,BasahaCorpus: An Expanded Linguistic Resource for Readability Assessment in Central Philippine Languages,2023,"Current research on automatic readability assessment (ARA) has focused on improving the performance of models in high-resource languages such as English. In this work, we introduce and release BasahaCorpus as part of an initiative aimed at expanding available corpora and baseline models for readability assessment in lower resource languages in the Philippines. We compiled a corpus of short fictional narratives written in Hiligaynon, Minasbate, Karay-a, and Rinconada -- languages belonging to the Central Philippine family tree subgroup -- to train ARA models using surface-level, syllable-pattern, and n-gram overlap features. We also propose a new hierarchical cross-lingual modeling approach that takes advantage of a language's placement in the family tree to increase the amount of available training data. Our study yields encouraging results that support previous work showcasing the efficacy of cross-lingual models in low-resource settings, as well as similarities in highly informative linguistic features for mutually intelligible languages.",10.48550/arXiv.2310.11584
0177437b4a97a182a923dfdb0aa7f68cdec4f031,Multipath Parallel Hybrid Deep Neural Networks Framework for Remaining Useful Life Estimation,2020,"The paper introduces a multi-path parallel hybrid deep neural design for remaining useful life (RUL) estimation of critical infrastructure, referred to as the MPHD. The proposed framework integrates three noisy deep learning structures in parallel: (a) A noisy path uses Long Short-Term Memory (LSTM), (b) A noisy path uses Gated Recurrent Unit (GRU), and; (c) A noisy path uses Convolutional Neural Network (CNN), The proposed framework aims to collect different types of features from the most popular deep neural networks architectures and then utilizing a fusion center consists of noisy fully connected multilayer neural network, to combine the collected features of the three parallel paths and predict the RLU. The MPHD framework utilizes noisy training to improve accuracy, enhance robustness, and mitigate the overfitting problem associated with neural networks. The proposed model is evaluated by utilizing (CMAPSS) dataset, which is provided by NASA.",10.1109/ICPHM49022.2020.9187040
1739f86aa8529ff22d3dd2e21cedcb6765359f20,Dynamic predictive maintenance for multiple components using data-driven probabilistic RUL prognostics: The case of turbofan engines,2023,,10.1016/j.ress.2023.109199
017775e08a74cb2e9ba8117f96694df5d0818670,Multi-Step Recurrent Q-Learning for Robotic Velcro Peeling,2020,"Learning object manipulation is a critical skill for robots to interact with their environment. Even though there has been significant progress in robotic manipulation of rigid objects, interacting with non-rigid objects remains challenging for robots. In this work, we introduce velcro peeling as a new application for robotic manipulation of non-rigid objects in complex environments. We present a method of learning force-based manipulation from noisy and incomplete sensor inputs in partially observable environments by modeling long term dependencies between measurements with a multi-step deep recurrent network. We present experiments on a real robot to show the necessity of modeling these long term dependencies and validate our approach in simulation and robot experiments. Our results show that using tactile input enables the robot to overcome geometric uncertainties present in the environment with high fidelity in ∼ 90% of all cases, outperforming the baselines by a large margin.",10.1109/ICRA48506.2021.9561030
e9c3513331262c3bca112d250ea926fb7898f645,A Transfer Approach Using Graph Neural Networks in Deep Reinforcement Learning,2024,"Transfer learning (TL) has shown great potential to improve Reinforcement Learning (RL) efficiency by leveraging prior knowledge in new tasks. However, much of the existing TL research focuses on transferring knowledge between tasks that share the same state-action spaces. Further, transfer from multiple source tasks that have different state-action spaces is more challenging and needs to be solved urgently to improve the generalization and practicality of the method in real-world scenarios. This paper proposes TURRET (Transfer Using gRaph neuRal nETworks), to utilize the generalization capabilities of Graph Neural Networks (GNNs) to facilitate efficient and effective multi-source policy transfer learning in the state-action mismatch setting. TURRET learns a semantic representation by accounting for the intrinsic property of the agent through GNNs, which leads to a unified state embedding space for all tasks. As a result, TURRET achieves more efficient transfer with strong generalization ability between different tasks and can be easily combined with existing Deep RL algorithms. Experimental results show that TURRET significantly outperforms other TL methods on multiple continuous action control tasks, successfully transferring across robots with different state-action spaces.",10.1609/aaai.v38i15.29571
1fdbf33eaf586b14cd1afd10eb721999ca460a6a,Support Vector Machine Based Lane-Changing Behavior Recognition and Lateral Trajectory Prediction,2022,"With the development of technology, vehicle trajectory prediction and safety decision technology has become an important part of active safety technology. Among them, the vehicle trajectory prediction technology can predict the vehicle position, speed, and other motion states in the predicted period according to the current and historical vehicle running state, and the prediction results can provide support for judging the vehicle safety in the predicted period. In order to analyze the above problems, this study fully extracted the main feature information from the vehicle lane change track data with the help of the powerful nonlinear learning and high pattern recognition ability of support vector machine, and conducted identification modeling for the actual lane change process of the vehicle and predictive analysis of the vehicle lateral movement track. First, the lane-changing behavior of vehicles was analyzed, and the vehicle lane-changing execution stage and 10 influencing factors that could characterize lane-changing behavior were determined based on NGSIM to extract the data of lane-change-related variables. Then, a lane-changing recognition model based on gridsearch-PSO is proposed. In the Matlab environment, the model has a test accuracy of 97.68%, while the SVM model without optimization parameters has a recognition accuracy of only 80.87%. The results show that the model has strong classification ability and robustness. Finally, by using the polynomial model for lateral movement trajectory fitting, K-fold cross-validation method is used for fitting polynomial model fitting test.",10.1155/2022/3632333
e9f37e9986a7323a5372e399476a76a749acb235,PetFace: A Large-Scale Dataset and Benchmark for Animal Identification,2024,"Automated animal face identification plays a crucial role in the monitoring of behaviors, conducting of surveys, and finding of lost animals. Despite the advancements in human face identification, the lack of datasets and benchmarks in the animal domain has impeded progress. In this paper, we introduce the PetFace dataset, a comprehensive resource for animal face identification encompassing 257,484 unique individuals across 13 animal families and 319 breed categories, including both experimental and pet animals. This large-scale collection of individuals facilitates the investigation of unseen animal face verification, an area that has not been sufficiently explored in existing datasets due to the limited number of individuals. Moreover, PetFace also has fine-grained annotations such as sex, breed, color, and pattern. We provide multiple benchmarks including re-identification for seen individuals and verification for unseen individuals. The models trained on our dataset outperform those trained on prior datasets, even for detailed breed variations and unseen animal families. Our result also indicates that there is some room to improve the performance of integrated identification on multiple animal families. We hope the PetFace dataset will facilitate animal face identification and encourage the development of non-invasive animal automatic identification methods.",10.48550/arXiv.2407.13555
018079a2120906abada89982a420cf856bcd8061,A Survey of Deep Learning and Its Application in Distribution Network,2020,"With the availability of data and improvement of computation ability, deep learning methods are promising in many areas. Due to the wide deployment of sensors and smart meters in distribution network, abundant data can be obtained, which provides the potential for the application of deep learning in distribution network. In this paper, concept and typical structures of deep learning networks are discussed. Meanwhile, several applications of deep learning in distribution network are summarized. Finally, challenges and prospects are proposed.",10.1109/ICAIIC48513.2020.9065235
8388875ce48b8517c8987a4a7096f6b14dfb548c,Deep Learning Techniques for Smart Meter Data Analytics: A Review,2022,,10.1007/s42979-022-01161-6
1be49eaed90f1ac896f7ae545b2c02f242f0d967,Fault Calculation Method of Distribution Network Based on Deep Learning,2021,"Under the low voltage ride through (LVRT) control strategy, the inverter interfaced distributed generation (IIDG) needs to change the output mode of the inverter according to the voltage of the connected nodes. The short-circuit current is related to the system rated capacity, network short-circuit impedance, and distributed power output. So, based on the deep learning algorithm, a predicting method of the voltage drop is proposed. By predicting the voltage of connected nodes, the output mode of IIDG can be determined based on the LVRT control. Thus, the fault calculation model of IIDG is accurately established. Compared with the three-phase asymmetric Gaussian fault calculation method, the proposed method can achieve fault calculation accurately. Finally, a case study is built to verify the effectiveness of the proposed method. The results indicate that the proposed method can make accurate voltage prediction and improve the computation speed of the fault calculation.",10.3390/sym13061086
faac8b44cd93260d59b4ee26a8266f8f119252d2,"Review of low voltage load forecasting: Methods, applications, and recommendations",2021,,10.1016/j.apenergy.2021.117798
17f8c9acd5ad20723784f3b6d11cc37bdb2b9179,CLGNet: A New Network for Human Pose Estimation using Commodity Millimeter Wave Radar,2020,"This paper introduces a new network (CLGNet: Combined Local and Global information encoding Network) for human pose estimation based on commodity millimeter wave (mmWave) radar. Based on the benchmark model of ResNet, a global spatial information encoding module is introduced at the early stage of the network. This new module is expected to help learning the relationship between sparsely distributed human pose keypoints with internal relations. Our experimental results show that the addition of this structural module improves the prediction accuracy of human pose keypoints, especially on minor body parts, such as hands and feet.",10.1145/3446132.3446421
01813111200df46133afb937df726c11f286c9ce,A New Feature Selection Method for Enhancing Cancer Diagnosis Based on DNA Microarray,2020,"Accurately classifying medical data is critical for improving diagnostic prediction system and identifying threptic targets for treatments. Analysing gene expression data has a major challenge in extracting disease-related genes from the large number of genes output from next generation sequencing technology. Therefore, eliminating irrelevant and redundant genes is a major step to process data for prediction. Our objective is to predict more accurately the presence of cancer disease in a sample cell from the gene expression.In this paper, we create a function called Classification Technique as Feature Selection (CTFS) as a new feature selection (FS) method to extract a subset (small number) of genes from classified big number of genes expression to improve cancer prediction result. The enrolled classification techniques in CTFS function for selection are K-Nearest Neighbors (K-NN) and Extreme Gradient Boosting (XGBoosting) optimized by Bayesian Parameter Tuning (BPT). The feature selection methods used to investigate the performance of CTFS function are Univariate Feature Selection (UFS) and Feature Importance (FI). The classification stage is carried out after the feature selection stage using three machine learning (ML) algorithms, Naïve Bayes (NB), Linear Support Vector Machine (LSVM), and Random Forest (RF). Results shows that, using XGBoosting optimized by BPT for FS outperforms FI method in terms of increasing the prediction accuracies along with minimum number of features but with higher running time. The performance of K-NN in FS outperforms all other FS methods in terms of accuracies providing an accuracy that is up to 100% when applied with LSVM on simulation dataset.",10.1109/NRSC49500.2020.9235095
b810c8d141eb814a9b23c5b91285147bdd5188c4,Using Feature Selection from XGBoost to Predict MIC Values with Neural Networks,2023,"The minimum inhibitory concentration (MIC) is the lowest concentration of an antimicrobial agent (commonly known as an antibiotic) that inhibits bacterial growth. The interpretation of an MIC value for a bacterial organism, recovered from an infection guides the treatment of that infection by informing the physician if the treatment has a chance to be successful or not. Methods to generate the MIC values depend on bacterial growth and could take 24 hours to several days, delaying the implementation of appropriate therapy and increasing rates of morbidity and mortality. In this study, XGBoost and Neural Network (NN) models were used to predict the MIC values for 13 antimicrobial agents in parallel. The XGBoost model was then used for feature selection, and those features were used as input to the NN. The 29 selected features from the XGBoost model gave an F1 score of 0.808 when applied to a NN model. That NN model has a +-1 2-fold dilution accuracy of ׅ89 for all antibiotics. The results prove that the XGBoost model is capable of selecting important features from the dataset and those selected features can be used to generate a highly effective NN model that can predict MIC results.",10.1109/IJCNN54540.2023.10191713
1cde1e585c9b7ad5f87162bd21ab9448321f7abb,F2UL: Fairness-Aware Federated Unlearning for Data Trading,2024,"Federated learning (FL) offers a credible solution for distributed data trading since it could train machine learning models in a distributed manner thereby enhancing data privacy without sharing local data. However, it is still challenging to trade data through FL due to unfair model allocation issues arising from the unreliability of user-provided data. To tackle it, we propose F2UL, a Fairness-aware Federated UnLearning solution that distributes models to users commensurate with their data quality. F2UL is trained in a two-stage (TST) way and contains three main components: 1) Label-free model quality assessment (LMQA) promotes fairness by evaluating models without user-specific data, ensuring uniform assessment standards. 2) Fair model distribution (FMD) addresses the issue of unfair model distribution by allocating models with feature mapping deviation, ensuring that users who contribute low-quality models do not receive enhanced models. 3) User data federated unlearning (UDFU) ensures fairness in model distribution by employing rapid recovery federated unlearning, safeguarding regular users from the adverse effects of low-quality data on model performance. In experiments on CIFAR10, F2UL reduces the low-quality data user accuracy to 9.09% and increases regular users’ accuracy by 3.02%, thereby demonstrating F2UL's capacity to ensure fairness in data trading.",10.1109/TMC.2024.3429228
2dcfd82df670da37a7e13610bc1c83c809f767fb,Addressing unreliable local models in federated learning through unlearning,2024,,10.1016/j.neunet.2024.106688
a323b1c18fc82a7fa550408f19d315ca7d63187b,Mitigating Poor Data Quality Impact with Federated Unlearning for Human-Centric Metaverse,2024,"Federated Learning (FL), which has been employed to train machine learning models on the data with a distributed manner, could enhance the immersive user experience for the human-centric metaverse. However, it’s challenging to train machine learning models accurately and promptly with FL for the human-centric metaverse due to massive data communication and user unreliability. User experience could be negatively affected by using low-quality machine learning models for human-centric metaverse, e.g., it cannot scrutinize and arrive at decisions accurately and timely. To resolve this pressing issue, we propose MetaFul a federated unlearning solution which reduces the negative influences of low-quality data with no data transmission by removing low-quality training models at the server side. To be specific, MetaFul includes three main components. (i) Low-throughput federated learning (LT-FL) addresses the issue of large model transmission in FL by decreasing the dimension and the number of transmitted model parameters. (ii) Loss-based model quality assessment (LM-QA) utilizes the model loss generated in LT-FL to estimate user data quality. (iii) Non-communicative federated unlearning (NC-FUL) revokes the low-quality data impact on the FL model with careful designed federated unlearning at the server side. Both LM-QA and NC-FUL have no communications with clients. Finally, extensive evaluations are conducted to show MetaFul could improve the model accuracy by at least 2.5% and decrease the user perception time by at least 19.3% in human-centric metaverse compared to benchmarks.",10.1109/JSAC.2023.3345388
01831fef2819279de24dbbd91e548d59ce8d6e44,Intrinsic Point Cloud Interpolation via Dual Latent Space Navigation,2020,"We present a learning-based method for interpolating and manipulating 3D shapes represented as point clouds, that is explicitly designed to preserve intrinsic shape properties. Our approach is based on constructing a dual encoding space that enables shape synthesis and, at the same time, provides links to the intrinsic shape information, which is typically not available on point cloud data. Our method works in a single pass and avoids expensive optimization, employed by existing techniques. Furthermore, the strong regularization provided by our dual latent space approach also helps to improve shape recovery in challenging settings from noisy point clouds across different datasets. Extensive experiments show that our method results in more realistic and smoother interpolations compared to baselines.",10.1007/978-3-030-58536-5_39
11f3810118c7a8d9bda99fe54e36c565595b88a6,Constructive Solid Geometry on Neural Signed Distance Fields,2023,"Signed Distance Fields (SDFs) parameterized by neural networks have recently gained popularity as a fundamental geometric representation. However, editing the shape encoded by a neural SDF remains an open challenge. A tempting approach is to leverage common geometric operators (e.g., boolean operations), but such edits often lead to incorrect non-SDF outputs (which we call Pseudo-SDFs), preventing them from being used for downstream tasks. In this paper, we characterize the space of Pseudo-SDFs, which are eikonal yet not true distance functions, and derive the closest point loss, a novel regularizer that encourages the output to be an exact SDF. We demonstrate the applicability of our regularization to many operations in which traditional methods cause a Pseudo-SDF to arise, such as CSG and swept volumes, and produce a true (neural) SDF for the result of these operations.",10.1145/3610548.3618170
efce14bc2a14f490cc28c608ba40a631018f3852,GeoLatent: A Geometric Approach to Latent Space Design for Deformable Shape Generators,2023,"We study how to optimize the latent space of neural shape generators that map latent codes to 3D deformable shapes. The key focus is to look at a deformable shape generator from a differential geometry perspective. We define a Riemannian metric based on as-rigid-as-possible and as-conformal-as-possible deformation energies. Under this metric, we study two desired properties of the latent space: 1) straight-line interpolations in latent codes follow geodesic curves; 2) latent codes disentangle pose and shape variations at different scales. Strictly enforcing the geometric interpolation property, however, only applies if the metric matrix is a constant. We show how to achieve this property approximately by enforcing that geodesic interpolations are axis-aligned, i.e., interpolations along coordinate axis follow geodesic curves. In addition, we introduce a novel approach that decouples pose and shape variations via generalized eigendecomposition. We also study efficient regularization terms for learning deformable shape generators, e.g., that promote smooth interpolations. Experimental results on benchmark datasets show that our approach leads to interpretable latent codes, improves the generalizability of synthetic shapes, and enhances performance in geodesic interpolation and geodesic shooting.",10.1145/3618371
46e9889a14afa55bb882545455a7954ceb7b29d7,SpecTrHuMS: Spectral transformer for human mesh sequence learning,2023,,10.1016/j.cag.2023.07.001
0614bb1558dcf5f2024b7d4076cacec70a81cb4f,GenCorres: Consistent Shape Matching via Coupled Implicit-Explicit Shape Generative Models,2023,"This paper introduces GenCorres, a novel unsupervised joint shape matching (JSM) approach. Our key idea is to learn a mesh generator to fit an unorganized deformable shape collection while constraining deformations between adjacent synthetic shapes to preserve geometric structures such as local rigidity and local conformality. GenCorres presents three appealing advantages over existing JSM techniques. First, GenCorres performs JSM among a synthetic shape collection whose size is much bigger than the input shapes and fully leverages the datadriven power of JSM. Second, GenCorres unifies consistent shape matching and pairwise matching (i.e., by enforcing deformation priors between adjacent synthetic shapes). Third, the generator provides a concise encoding of consistent shape correspondences. However, learning a mesh generator from an unorganized shape collection is challenging, requiring a good initialization. GenCorres addresses this issue by learning an implicit generator from the input shapes, which provides intermediate shapes between two arbitrary shapes. We introduce a novel approach for computing correspondences between adjacent implicit surfaces, which we use to regularize the implicit generator. Synthetic shapes of the implicit generator then guide initial fittings (i.e., via template-based deformation) for learning the mesh generator. Experimental results show that GenCorres considerably outperforms state-of-the-art JSM techniques. The synthetic shapes of GenCorres also achieve salient performance gains against state-of-the-art deformable shape generators.",10.48550/arXiv.2304.10523
477c521cbd4aeb8acef58133ab066b308d57c460,A Margin-Maximizing Fine-Grained Ensemble Method,2024,"Ensemble learning has achieved remarkable success in machine learning, but its reliance on numerous base learners limits its application in resource-constrained environments. This paper introduces an innovative""Margin-Maximizing Fine-Grained Ensemble Method""that achieves performance surpassing large-scale ensembles by meticulously optimizing a small number of learners and enhancing generalization capability. We propose a novel learnable confidence matrix, quantifying each classifier's confidence for each category, precisely capturing category-specific advantages of individual learners. Furthermore, we design a margin-based loss function, constructing a smooth and partially convex objective using the logsumexp technique. This approach improves optimization, eases convergence, and enables adaptive confidence allocation. Finally, we prove that the loss function is Lipschitz continuous, based on which we develop an efficient gradient optimization algorithm that simultaneously maximizes margins and dynamically adjusts learner weights. Extensive experiments demonstrate that our method outperforms traditional random forests using only one-tenth of the base learners and other state-of-the-art ensemble methods.",10.48550/arXiv.2409.12849
018b19a26d596222dced050a9e6ccc3f895478c1,Learning Continuous Object Representations from Point Cloud Data,2020,"Continuous representations of objects have always been used in robotics in the form of geometric primitives and surface models. Recently, learning techniques have emerged which allow more complex continuous representations to be learned from data, but these learning techniques require training data in the form of watertight meshes which restricts their application as meshes of this form are difficult to obtain from real data. This paper proposes a modification to existing methods that allows real world point cloud data to be used for training these surface representations allowing the techniques to be used in broader applications. The modification is evaluated on ModelNet10 to quantify the difference between the existing and the proposed methods as well as on a novel precision agriculture dataset that has been released publicly to show the modification's applicability to new areas. The proposed method enables obtaining training data from real world sensors that produce point clouds rather than requiring an expensive meshing step which may not be possible for some applications. This opens the possibility of using techniques like this for complex shapes in areas like grasping and agricultural data collection.",10.1109/IROS45743.2020.9341765
061116d951524b0f3cce48f7c2438ed2ac467502,RayMVSNet++: Learning Ray-Based 1D Implicit Fields for Accurate Multi-View Stereo,2023,"Learning-based multi-view stereo (MVS) has by far centered around 3D convolution on cost volumes. Due to the high computation and memory consumption of 3D CNN, the resolution of output depth is often considerably limited. Different from most existing works dedicated to adaptive refinement of cost volumes, we opt to directly optimize the depth value along each camera ray, mimicking the range (depth) finding of a laser scanner. This reduces the MVS problem to ray-based depth optimization which is much more light-weight than full cost volume optimization. In particular, we propose RayMVSNet which learns sequential prediction of a 1D implicit field along each camera ray with the zero-crossing point indicating scene depth. This sequential modeling, conducted based on transformer features, essentially learns the epipolar line search in traditional multi-view stereo. We devise a multi-task learning for better optimization convergence and depth accuracy. We found the monotonicity property of the SDFs along each ray greatly benefits the depth estimation. Our method ranks top on both the DTU and the Tanks & Temples datasets over all previous learning-based methods, achieving an overall reconstruction score of 0.33 mm on DTU and an F-score of 59.48% on Tanks & Temples. It is able to produce high-quality depth estimation and point cloud reconstruction in challenging scenarios such as objects/scenes with non-textured surface, severe occlusion, and highly varying depth range. Further, we propose RayMVSNet++ to enhance contextual feature aggregation for each ray through designing an attentional gating unit to select semantically relevant neighboring rays within the local frustum around that ray. This improves the performance on datasets with more challenging examples (e.g., low-quality images caused by poor lighting conditions or motion blur). RayMVSNet++ achieves state-of-the-art performance on the ScanNet dataset. In particular, it attains an AbsRel of 0.058m and produces accurate results on the two subsets of textureless regions and large depth variation.",10.1109/TPAMI.2023.3296163
4689d123d0b1099914f8dff8e3c352fb9114faa5,Learning to Detect 3D Symmetry From Single-View RGB-D Images With Weak Supervision,2022,"3D symmetry detection is a fundamental problem in computer vision and graphics. Most prior works detect symmetry when the object model is fully known, few studies symmetry detection on objects with partial observation, such as single RGB-D images. Recent work addresses the problem of detecting symmetries from incomplete data with a deep neural network by leveraging the dense and accurate symmetry annotations. However, due to the tedious labeling process, full symmetry annotations are not always practically available. In this work, we present a 3D symmetry detection approach to detect symmetry from single-view RGB-D images without using symmetry supervision. The key idea is to train the network in a weakly-supervised learning manner to complete the shape based on the predicted symmetry such that the completed shape be similar to existing plausible shapes. To achieve this, we first propose a discriminative variational autoencoder to learn the shape prior in order to determine whether a 3D shape is plausible or not. Based on the learned shape prior, a symmetry detection network is present to predict symmetries that produce shapes with high shape plausibility when completed based on those symmetries. Moreover, to facilitate end-to-end network training and multiple symmetry detection, we introduce a new symmetry parametrization for the learning-based symmetry estimation of both reflectional and rotational symmetry. The proposed approach, coupled symmetry detection with shape completion, essentially learns the symmetry-aware shape prior, facilitating more accurate and robust symmetry detection. Experiments demonstrate that the proposed method is capable of detecting reflectional and rotational symmetries accurately, and shows good generality in challenging scenarios, such as objects with heavy occlusion and scanning noise. Moreover, it achieves state-of-the-art performance, improving the F1-score over the existing supervised learning method by 2%-11% on the ShapeNet and ScanNet datasets.",10.1109/TPAMI.2022.3186876
ae033080d452915e647ddb69b1189a70e2f2397d,RayMVSNet: Learning Ray-based 1D Implicit Fields for Accurate Multi-View Stereo,2022,"Learning-based multi-view stereo (MVS) has by far cen-tered around 3D convolution on cost volumes. Due to the high computation and memory consumption of 3D CNN, the resolution of output depth is often considerably limited. Differentfrom most existing works dedicated to adaptive re-finement of cost volumes, we opt to directly optimize the depth value along each camera ray, mimicking the range (depth) finding of a laser scanner. This reduces the MVS problem to ray-based depth optimization which is much more light-weight than full cost volume optimization. In particular, we propose RayMVSNet which learns sequen-tial prediction of aID implicit field along each camera ray with the zero-crossing point indicating scene depth. This sequential modeling, conducted based on transformer features, essentially learns the epipolar line search in traditional multi-view stereo. We also devise a multi-task learning for better optimization convergence and depth accuracy. Our method ranks top on both the DTU and the Tanks & Temples datasets over all previous learning-based methods, achieving overall reconstruction score of 0.33mm on DTU andf-score of59.48% on Tanks & Temples.",10.1109/CVPR52688.2022.00840
018b9347e4a3e53506d781275db15b66105f1f03,Normalized Residue Analysis for Deep Learning Based Probabilistic Forecasting of Photovoltaic Generations,2020,"In this study, probabilistic forecasting schemes of day-ahead photovoltaic (PV) generations are investigated with the auto-regressive recurrent neural network model named DeepAR, and are evaluated based on the normalized residues. For PV generations, probabilistic outcomes should be helpful for efficient grid managements to account uncertainties such as sudden changes in the local weather. The tightness of the prediction interval for local PV generations is investigated with DeepAR models with varying input data like the local weather forecasts of the day and historical records of the PV generations. For performance measure, normalized residue with the mean and standard deviation of the predicted traces is compared to the standard normal distribution. For evaluation, local PV generation data captured at Hadong, Korea is tested by the DeepAR models with optional input of local weather forecasts data. The evaluation results of the PV generation tests show that the local weather data provides extra tightness of the prediction interval with the normalized residues close to the standard normal distribution.",10.1109/BigComp48618.2020.00-20
097a0ed5340f1fdf28a9189b1cbc1d5a2b9e6757,HDTO-DeepAR: A novel hybrid approach to forecast surface water quality indicators.,2024,,10.1016/j.jenvman.2024.120091
726ea9bf07ae6ebb457ea26cb8eee5eec38734f8,Intelligent prediction of rolling bearing remaining useful life based on probabilistic DeepAR-Transformer model,2023,"Remaining useful life (RUL) prediction for rolling bearings requires highly accurate and stable long-term prediction capabilities in equipment health management, which demands that the prediction model has strong data reasoning and regression performance. However, it is difficult to accurately capture long-term dependencies via traditional convolutional neural network because the information loss and insufficient analysis are unavoidable during the feature extraction process. An end-to-end time series forecasting method called D-former for RUL prediction of rolling bearings is proposed in this paper. The method mainly consists of DeepAR and a multi-layer encoder, so it is able to extract degradation features directly from the original signal. This method has the following salient features: (1) the designed multi-head attention mechanism can highlight important feature information and realize parallel computing, so the method is extremely suitable for processing long-term time series; (2) the important time feature information is rearranged through DeepAR, so the method has the excellent domain adaptability, and it can achieve accurate prediction of rolling bearing RUL under different working conditions. The verification experiment was implemented on the IEEE PHM 2012 dataset and the XJTU-SY bearing dataset. The experimental results show that the proposed D-former method is actually superior to the existing mainstream RUL prediction methods.",10.1088/1361-6501/acf874
86147323127155e47dac2d1dd95be81b7e75d3a3,The synergistic effect of operational research and big data analytics in greening container terminal operations: A review and future directions,2022,,10.1016/j.ejor.2022.11.054
7293c83bcdfadb7a48914b9fef588e6fb04b5f64,Image-based time series forecasting: A deep convolutional neural network approach,2022,,10.1016/j.neunet.2022.10.006
275e4d2ad0040dd8ef01173bb0aab65befa4be04,Navigating learners towards technology-enhanced learning during post COVID-19 semesters,2022,,10.1016/j.tine.2022.100189
8fc2668d975845c3eea81bc41b61015da38b186b,"Augmented Reality, a Review of a Way to Represent and Manipulate 3D Chemical Structures",2022,"Augmented reality (AR) is a mixed technology that superimposes three-dimensional (3D) digital data onto an image of reality. This technology enables users to represent and manipulate 3D chemical structures. In spite of its potential, the use of these tools in chemistry is still scarce. The aim of this work is to identify the real situation of AR developments and its potential for 3D visualization of molecules. A descriptive analysis of a selection of 143 research publications (extracted from Web of Science between 2018 and 2020) highlights some significant AR examples that had been implemented in chemistry, in both education and research environments. Although the traditional 2D screen visualization is still preferred when teaching chemistry, the application of AR in early education has shown potential to facilitate the understanding and visualization of chemical structures. The increasing connectivity of the AR technology to web platforms and scientific networks should translate into new opportunities for teaching and learning strategies.",10.1021/acs.jcim.1c01255
73f341d5924730eba71e001c76e25bd8c9ec9498,The Urgency of Digital Literacy for Generation Z Students in Chemistry Learning,2021,"Chemistry learning is faced with problems that emphasize students' difficulty in learning the basic chemistry concepts to become increasingly difficult and less meaningful. Digital literacy can understand chemistry concepts and develop critical thinking skills because digital literacy emphasizes reading, writing, understanding, evaluating, communicating, and using the information in different formats. This research aims to determine the role and urgency of digital literacy for generation Z students in chemistry learning. This research is descriptive qualitative research. The research subjects were chemistry education students of FSTT of Universitas Pendidikan Mandalika. In this study, there were two types of data; primary and secondary data. The primary data was collected through direct observation and interviews. Secondary data was collected through the literature review. The results showed that digital literacy is needed in chemistry learning by focusing on the needs of Generation Z students in accessing the internet as a medium to promises ease of community connectivity, starting from how to communicate, collaborate, be creative, solve problems, make decisions, and consume information. In chemistry learning, digital technology can help improve chemistry teaching quality in technical, cognitive, and social aspects. Also, digital technology has become necessary in chemistry learning because it can help students learn better with various ICT-enabled features that can motivate understanding of chemistry concepts and reduce cognitive memory load when studying ICT-based chemistry. Thus, digital literacy is urgent for Generation Z students in chemistry learning.",10.3991/IJET.V16I11.19871
0195901ab3b390a61788da82a0ae3b1086725248,Automated Measurement of Competencies and Generation of Feedback in Object-Oriented Programming Courses,2020,"To overcome the shortage of computer specialists, there is an increased need for correspondent study and training offers, in particular for learning programming. The automated assessment of solutions to programming tasks could relieve teachers of time-consuming corrections and provide individual feedback even in online courses without any personal teacher. The e-assessment system JACK has been successfully applied for more than 12 years up to now, e.g., in a CS1 lecture. However, there are only few solid research results on competencies and competence models for object-oriented programming (OOP), which could be used as a foundation for high-quality feedback.In a joint research project of research groups at two universities, we aim to empirically define competencies for OOP using a mixed-methods approach. In a first step, we performed a qualitative content analysis of source code (sample solutions and students’ solutions) and as a result identified a set of suitable competency components that forms the core of further investigations. Semi-structured interviews with learners will be used to identify difficulties and misconceptions of the learners and to adapt the set of competency components. Based on that we will use Item Response Theory (IRT) to develop an automatically evaluable test instrument for the implementation of abstract data types. We will further develop empirically founded and competency-based feedback that can be used in e-assessment systems and MOOCs.",10.1109/EDUCON45650.2020.9125323
c886d0e3bffa478bf5e01f2b9f4231d1d5e3fbd0,How ChatGPT Will Change Software Engineering Education,2023,"This position paper discusses the potential for using generative AIs like ChatGPT in software engineering education. Currently, discussions center around potential threats emerging from student's use of ChatGPT. For instance, generative AI will limit the usefulness of graded homework dramatically. However, there exist potential opportunities as well. For example, ChatGPT's ability to understand and generate human language allows providing personalized feedback to students, and can thus accompany current software engineering education approaches. This paper highlights the potential for enhancing software engineering education. The availability of generative AI will improve the individualization of education approaches. In addition, we discuss the need to adapt software engineering curricula to the changed profiles of software engineers. Moreover, we point out why it is important to provide guidance for using generative AI and, thus, integrate it in courses rather than accepting the unsupervised use by students, which can negatively impact the students' learning.",10.1145/3587102.3588815
768403710ec37fe612257384b19bf8f1c7bcce72,Formative assessment strategies for students' conceptions - The potential of learning analytics,2022,"Formative assessment is considered to be helpful in students' learning support and teaching design. Following Aufschnaiter's and Alonzo's framework, formative assessment practices of teachers can be subdivided into three practices: eliciting evidence, interpreting evidence and responding. Since students' conceptions are judged to be important for meaningful learning across disciplines, teachers are required to assess their students' conceptions. The focus of this article lies on the discussion of learning analytics for supporting the assessment of students' conceptions in class. The existing and potential contributions of learning analytics are discussed related to the named formative assessment framework in order to enhance the teachers' options to consider individual students' conceptions. We refer to findings from biology and computer science education on existing assessment tools and identify limitations and potentials with respect to the assessment of students' conceptions.",10.1111/bjet.13288
55c926859b1611ec9352b1bc5fe971b3479bbc65,Automatic Feedback in the Teaching of Programming in Undergraduate Courses: a Literature Mapping,2022,"Teaching programming in the early years of undergraduate courses has been a challenge for students, institutions, and professors. In view of this, Learning Management Systems (LMSs) and other teaching platforms have emerged to address some of the difficulties in this process. In this context, the present work intends to answer the following research question (RQ): What does the literature tell us about the use of automatic feedback in teaching programming in undergraduate courses? To answer this question, a literature mapping was conducted based on 119 articles published between 2017 and 2021. The mapping showed that the research area is expanding and has related studies from all over the world. The papers have different origins, and 37 countries are represented in this survey. The main programming languages used are Java, Python, C and C++. Another finding was that it is common practice to develop specific platforms for automatic feedback in programming courses. This paper presents the findings and results obtained.",10.1109/FIE56618.2022.9962723
e4042bf1f370e323520d76374494aa3f4ce8d32c,Automated Assessment in Computer Science Education: A State-of-the-Art Review,2022,"Practical programming competencies are critical to the success in computer science (CS) education and go-to-market of fresh graduates. Acquiring the required level of skills is a long journey of discovery, trial and error, and optimization seeking through a broad range of programming activities that learners must perform themselves. It is not reasonable to consider that teachers could evaluate all attempts that the average learner should develop multiplied by the number of students enrolled in a course, much less in a timely, deep, and fair fashion. Unsurprisingly, exploring the formal structure of programs to automate the assessment of certain features has long been a hot topic among CS education practitioners. Assessing a program is considerably more complex than asserting its functional correctness, as the proliferation of tools and techniques in the literature over the past decades indicates. Program efficiency, behavior, and readability, among many other features, assessed either statically or dynamically, are now also relevant for automatic evaluation. The outcome of an evaluation evolved from the primordial Boolean values to information about errors and tips on how to advance, possibly taking into account similar solutions. This work surveys the state of the art in the automated assessment of CS assignments, focusing on the supported types of exercises, security measures adopted, testing techniques used, type of feedback produced, and the information they offer the teacher to understand and optimize learning. A new era of automated assessment, capitalizing on static analysis techniques and containerization, has been identified. Furthermore, this review presents several other findings from the conducted review, discusses the current challenges of the field, and proposes some future research directions.",10.1145/3513140
0198b1bcfad3862984a1d45bda11b8c46504c022,SCOUT: Self-Aware Discriminant Counterfactual Explanations,2020,"The problem of counterfactual visual explanations is considered. A new family of discriminant explanations is introduced. These produce heatmaps that attribute high scores to image regions informative of a classifier prediction but not of a counter class. They connect attributive explanations, which are based on a single heat map, to counterfactual explanations, which account for both predicted class and counter class. The latter are shown to be computable by combination of two discriminant explanations, with reversed class pairs. It is argued that self-awareness, namely the ability to produce classification confidence scores, is important for the computation of discriminant explanations, which seek to identify regions where it is easy to discriminate between prediction and counter class. This suggests the computation of discriminant explanations by the combination of three attribution maps. The resulting counterfactual explanations are optimization free and thus much faster than previous methods. To address the difficulty of their evaluation, a proxy task and set of quantitative metrics are also proposed. Experiments under this protocol show that the proposed counterfactual explanations outperform the state of the art while achieving speeds much faster, for popular networks. In a human-learning machine teaching experiment, they are also shown to improve mean student accuracy from chance level to 95%.",10.1109/CVPR42600.2020.00900
01a543bfaa14acbcac07656b875132bd05be3a96,Incremental Semi-Supervised Learning for Multi-Genre Speech Recognition,2020,"In this work, we explore a data scheduling strategy for semi-supervised learning (SSL) for acoustic modeling in automatic speech recognition. The conventional approach uses a seed model trained with supervised data to automatically recognize the entire set of unlabeled (auxiliary) data to generate new labels for subsequent acoustic model training. In this paper, we propose an approach in which the unlabelled set is divided into multiple equal-sized subsets. These subsets are processed in an incremental fashion: for each iteration a new subset is added to the data used for SSL, starting from only one subset in the first iteration. The acoustic model from the previous iteration becomes the seed model for the next one. This scheduling strategy is compared to the approach employing all unlabeled data in one-shot for training. Experiments using lattice-free maximum mutual information based acoustic model training on Fisher English gives 80% word error recovery rate. On the multi-genre evaluation sets on Lithuanian and Bulgarian relative improvements of up to 17.2% in word error rate are observed.",10.1109/ICASSP40776.2020.9054309
5f94d07dae733efe61eda174206259738e3b78d6,XLSR-Transducer: Streaming ASR for Self-Supervised Pretrained Models,2024,"Self-supervised pretrained models exhibit competitive performance in automatic speech recognition on finetuning, even with limited in-domain supervised data. However, popular pretrained models are not suitable for streaming ASR because they are trained with full attention context. In this paper, we introduce XLSR-Transducer, where the XLSR-53 model is used as encoder in transducer setup. Our experiments on the AMI dataset reveal that the XLSR-Transducer achieves 4% absolute WER improvement over Whisper large-v2 and 8% over a Zipformer transducer model trained from scratch. To enable streaming capabilities, we investigate different attention masking patterns in the self-attention computation of transformer layers within the XLSR-53 model. We validate XLSR-Transducer on AMI and 5 languages from CommonVoice under low-resource scenarios. Finally, with the introduction of attention sinks, we reduce the left context by half while achieving a relative 12% improvement in WER.",10.1109/icassp49660.2025.10888110
01a7b0fa501e0319f3f3bbaf5d8caaee159663cc,Multi-Objective Interpolation Training for Robustness to Label Noise,2020,"Deep neural networks trained with standard cross-entropy loss memorize noisy labels, which degrades their performance. Most research to mitigate this memorization proposes new robust classification loss functions. Conversely, we propose a Multi-Objective Interpolation Training (MOIT) approach that jointly exploits contrastive learning and classification to mutually help each other and boost performance against label noise. We show that standard supervised contrastive learning degrades in the presence of label noise and propose an interpolation training strategy to mitigate this behavior. We further propose a novel label noise detection method that exploits the robust feature representations learned via contrastive learning to estimate per-sample soft-labels whose disagreements with the original labels accurately identify noisy samples. This detection allows treating noisy samples as unlabeled and training a classifier in a semi-supervised manner to prevent noise memorization and improve representation learning. We further propose MOIT+, a refinement of MOIT by fine-tuning on detected clean samples. Hyperparameter and ablation studies verify the key components of our method. Experiments on synthetic and real-world noise benchmarks demonstrate that MOIT/MOIT+ achieves state-of-the-art results. Code is available at https://git.io/JI40X.",10.1109/CVPR46437.2021.00654
01a9ffd52c39158ff1ee87986f53a79d9bdaf3ad,Generative Adversarial Training for Weakly Supervised Nuclei Instance Segmentation,2020,"Nuclei segmentation occupies an important position in medical image analysis, which helps to predict and diagnose diseases. With the further research of deep learning, the task of nuclei segmentation has been automated. However, most existing methods require a great deal of manually marked full masks for training, which is time-consuming and labor-intensive, and can only be done by professional personnel. For the purpose of reducing the cost of labeling, we propose a weakly supervised method using generative adversarial training for segmentation of nucleus. In the case of no boundary, but only the centroid of the nucleus, the proposed method segmented the nucleus region with blurred boundaries. We first use the generative adversarial network(GAN) to generate the likelihood map of the nuclear centroid, then use Guided Backpropagation to visualize the pixels that contributes to the detection of the centroid of each nucleus, and finally obtain the segmentation mask of the nucleus by graph-cut. In addition, for the purpose of training the network better, we performed stain normalization on each pathological image. We have verified the proposed method on a multi-organ nuclei dataset. The final experiment results show that our advanced method achieves better segmentation performance than other weakly supervised methods, and can even reach the level of full supervision.",10.1109/SMC42975.2020.9283412
77c0922ee2738ea2cc43c4fcb9f3aae281e9bd2a,EOFD-Net: Edge Optimization and Feature Denoising for Weakly Supervised Deep Nuclei Segmentation with Point Annotations,2024,"Nuclei segmentation is a fundamental and critical step in digital pathological image analysis. Fully supervised nuclei segmentation requires a lot of pixel-by-pixel manual annotation by pathologists, which is very time-consuming and laborious. To minimize the labeling burden of pathologists, this paper uses only point annotations of nuclei data for weakly supervised learning. Specifically, a two-stage model named EOFD-Net with feature denoising and edge optimization is proposed. In the first stage, three weak labels (K-means cluster labels, Voronoi labels, and superpixel labels) with complementary information are used to train the encoder-decoder network to achieve coarse segmentation of nuclei. A feature denoising module(FDM) is designed in the encoder part, which can effectively reduce noise interference. In the second stage, we designed an edge optimization strategy using the prior knowledge of the trained model in the first stage. Confident learning is employed to denoise pseudo-label and rectify the mislabel. These optimized labels are input into the second stage to obtain the final segmentation results. The performance of our method outperforms current state-of-the-art methods on two publicly nuclei segmentation datasets, MoNuSeg and TNBC.",10.1109/ICASSP48485.2024.10448142
2cb137db7e485fcad147ff27acc80e35ae897484,A Loopback Network for Explainable Microvascular Invasion Classification,2023,"Microvascular invasion (MVI) is a critical factor for prognosis evaluation and cancer treatment. The current diagnosis of MVI relies on pathologists to manually find out cancerous cells from hundreds of blood vessels, which is time-consuming, tedious, and subjective. Recently, deep learning has achieved promising results in medical image analysis tasks. However, the unexplainability of black box models and the requirement of massive annotated samples limit the clinical application of deep learning based diagnostic methods. In this paper, aiming to develop an accurate, objective, and explainable diagnosis tool for MVI, we propose a Loopback Network (LoopNet) for classifying MVI efficiently. With the image-level category annotations of the collected Pathologic Vessel Image Dataset (PVID), LoopNet is devised to be composed binary classification branch and cell locating branch. The latter is devised to locate the area of cancerous cells, regular non-cancerous cells, and background. For healthy samples, the pseudo masks of cells supervise the cell locating branch to distinguish the area of regular non-cancerous cells and background. For each MVI sample, the cell locating branch predicts the mask of cancerous cells. Then the masked cancerous and non-cancerous areas of the same sample are input back to the binary classification branch separately. The loopback between two branches enables the category label to supervise the cell locating branch to learn the locating ability for cancerous areas. Experiment results show that the proposed LoopNet achieves 97.5% accuracy on MVI classification. Surprisingly, the proposed loopback mechanism not only enables LoopNet to predict the cancerous area but also facilitates the classification backbone to achieve better classification performance.",10.1109/CVPR52729.2023.00719
01b74158a1f8a0a7089ec8eb9a72e30f0a52a325,Efficient Implicit Unsupervised Text Hashing using Adversarial Autoencoder,2020,"Searching for documents with semantically similar content is a fundamental problem in the information retrieval domain with various challenges, primarily, in terms of efficiency and effectiveness. Despite the promise of modeling structured dependencies in documents, several existing text hashing methods lack an efficient mechanism to incorporate such vital information. Additionally, the desired characteristics of an ideal hash function, such as robustness to noise, low quantization error and bit balance/uncorrelation, are not effectively learned with existing methods. This is because of the requirement to either tune additional hyper-parameters or optimize these heuristically and explicitly constructed cost functions. In this paper, we propose a Denoising Adversarial Binary Autoencoder (DABA) model which presents a novel representation learning framework that captures structured representation of text documents in the learned hash function. Also, adversarial training provides an alternative direction to implicitly learn a hash function that captures all the desired characteristics of an ideal hash function. Essentially, DABA adopts a novel single-optimization adversarial training procedure that minimizes the Wasserstein distance in its primal domain to regularize the encoder’s output of either a recurrent neural network or a convolutional autoencoder. We empirically demonstrate the effectiveness of our proposed method in capturing the intrinsic semantic manifold of the related documents. The proposed method outperforms the current state-of-the-art shallow and deep unsupervised hashing methods for the document retrieval task on several prominent document collections.",10.1145/3366423.3380150
0d4fc9e3013ab7d39520ff563be89d831819f43a,POLISH: Adaptive Online Cross-Modal Hashing for Class Incremental Data,2024,"In recent years, hashing-based online cross-modal retrieval has garnered growing attention. This trend is motivated by the fact that web data is increasingly delivered in a streaming manner as opposed to batch processing. Simultaneously, the sheer scale of web data sometimes makes it impractical to fully load for the training of hashing models. Despite the evolution of online cross-modal hashing techniques, several challenges remain: 1) Most existing methods learn hash codes by considering the relevance among newly arriving data or between new data and the existing data, often disregarding valuable global semantic information. 2) A common but limiting assumption in many methods is that the label space remains constant, implying that all class labels should be provided within the first data chunk. This assumption does not hold in real-world scenarios, and the presence of new labels in incoming data chunks can severely degrade or even break these methods. To tackle these issues, we introduce a novel supervised online cross-modal hashing method named adaPtive Online cLass-Incremental haSHing (POLISH). Leveraging insights from language models, POLISH generates representations for new class label from multiple angles. Meanwhile, POLISH treats label embeddings, which remain unchanged once learned, as stable global information to produce high-quality hash codes. POLISH also puts forward an efficient optimization algorithm for hash code learning. Extensive experiments on two real-world benchmark datasets show the effectiveness of the proposed POLISH for class incremental data in the cross-modal hashing domain.",10.1145/3589334.3645716
3475f513e7f97cfc612cef13f38f7d6753ac8bc4,A novel robust black-box fingerprinting scheme for deep classification neural networks,2024,,10.1016/j.eswa.2024.124201
1552afa096a16226ab91043484ef4bb0325a356d,Bit-mask Robust Contrastive Knowledge Distillation for Unsupervised Semantic Hashing,2024,"Unsupervised semantic hashing has emerged as an indispensable technique for fast image search, which aims to convert images into binary hash codes without relying on labels. Recent advancements in the field demonstrate that employing large-scale backbones (e.g., ViT) in unsupervised semantic hashing models can yield substantial improvements. However, the inference delay has become increasingly difficult to overlook. Knowledge distillation provides a means for practical model compression to alleviate this delay. Nevertheless, the prevailing knowledge distillation approaches are not explicitly designed for semantic hashing. They ignore the unique search paradigm of semantic hashing, the inherent necessities of the distillation process, and the property of hash codes. In this paper, we propose an innovative Bit-mask Robust Contrastive knowledge Distillation (BRCD) method, specifically devised for the distillation of semantic hashing models. To ensure the effectiveness of two kinds of search paradigms in the context of semantic hashing, BRCD first aligns the semantic spaces between the teacher and student models through a contrastive knowledge distillation objective. Additionally, to eliminate noisy augmentations and ensure robust optimization, a cluster-based method within the knowledge distillation process is introduced. Furthermore, through a bit-level analysis, we uncover the presence of redundancy bits resulting from the bit independence property. To mitigate these effects, we introduce a bit mask mechanism in our knowledge distillation objective. Finally, extensive experiments not only showcase the noteworthy performance of our BRCD method in comparison to other knowledge distillation methods but also substantiate the generality of our methods across diverse semantic hashing models and backbones. The code for BRCD is available at https://github.com/hly1998/BRCD.",10.1145/3589334.3645440
63035fc655bbaadd2fd71057fcb3aeeb2b9c2711,Synthesis of model features for fake news detection using large language models,2024,"In recent years, the issue of fake news has become exceedingly pertinent due to its rapid dissemination through social media and online platforms. Detecting fake news requires the utilization of various methods and resources. This work proposes an approach to building a model for fake news detection using generative artificial intelligence and natural language processing. The main focus of the proposed approach is the synthesis of model features for fake news detection using generative artificial intelligence and Natural Language Processing. The use of this proposed approach not only facilitates the detection of fake news but also renders the detection process transparent and customizable by the user. Additionally, through the proposed method, users gain the ability to expand features and train the system to adapt to new types of fake news and their variations. Experimental results, presented both qualitatively through visual analytics and quantitatively through statistical indicators, convincingly demonstrate the effectiveness of the proposed approach in detecting fake news with satisfactory accuracy levels reaching 90% and provide users with sufficient interpretability of the obtained decisions. Overall, this research aims to create an approach for detecting fake news that may have a significant impact on addressing this issue in contemporary society.",10.31110/colins/2024-4/005
01c304c4c731705f371e0d0024a95b136a805d41,The Next Generation of Neural Networks,2020,"The most important unsolved problem with artificial neural networks is how to do unsupervised learning as effectively as the brain. There are currently two main approaches to unsupervised learning. In the first approach, exemplified by BERT and Variational Autoencoders, a deep neural network is used to reconstruct its input. This is problematic for images because the deepest layers of the network need to encode the fine details of the image. An alternative approach, introduced by Becker and Hinton in 1992, is to train two copies of a deep neural network to produce output vectors that have high mutual information when given two different crops of the same image as their inputs. This approach was designed to allow the representations to be untethered from irrelevant details of the input. The method of optimizing mutual information used by Becker and Hinton was flawed (for a subtle reason that I will explain) so Pacannaro and Hinton (2001) replaced it by a discriminative objective in which one vector representation must select a corresponding vector representation from among many alternatives. With faster hardware, contrastive learning of representations has recently become very popular and is proving to be very effective, but it suffers from a major flaw: To learn pairs of representation vectors that have N bits of mutual information we need to contrast the correct corresponding vector with about 2N incorrect alternatives. I will describe a novel and effective way of dealing with this limitation. I will also show that this leads to a simple way of implementing perceptual learning in cortex.",10.1145/3397271.3402425
96e96731fe8d1a27263d0ce29640bd1f77f6546e,DADR: Deep adaptive dimensionality reduction architecture based on the coefficient of variation,2022,"The traditional deep dimensionality reduction methods corresponding to the depth of the model and the dimensionality of each layer depend on empirical settings. In this paper, we propose a deep adaptive dimensionality reduction architecture (DADR) based on the coefficient of variation and Gaussian restricted Boltzmann machine (GRBM) for achieving adaptivity of depth and dimensionality in the dimensionality reduction process. To verily the validity of the proposed model, we introduce two unsupervised algorithms, K-means and spectral clustering (SC), to compare the DADR architecture with all original features, shallow GRBM model, PCA and two advanced feature selection-based dimensionality reduction algorithms (CNAFS and UFSwithOL), respectively. The final experimental results show the performance of the proposed DADR architecture is demonstrated to be superior to the other algorithmic models. The source code is available at https://github.com/dingxm99/DADR.",10.1109/ccis57298.2022.10016418
25f2216a6e44d78a3ad12551b76432281b831e21,Extraction of descriptive driving patterns from driving data using unsupervised algorithms,2021,,10.1016/J.YMSSP.2020.107589
01c36603be4de55e5166f07203ad1f8f5047e3de,Robust Adversarial Objects against Deep Learning Models,2020,"Previous work has shown that Deep Neural Networks (DNNs), including those currently in use in many fields, are extremely vulnerable to maliciously crafted inputs, known as adversarial examples. Despite extensive and thorough research of adversarial examples in many areas, adversarial 3D data, such as point clouds, remain comparatively unexplored. The study of adversarial 3D data is crucial considering its impact in real-life, high-stakes scenarios including autonomous driving. In this paper, we propose a novel adversarial attack against PointNet++, a deep neural network that performs classification and segmentation tasks using features learned directly from raw 3D points. In comparison to existing works, our attack generates not only adversarial point clouds, but also robust adversarial objects that in turn generate adversarial point clouds when sampled both in simulation and after construction in real world. We also demonstrate that our objects can bypass existing defense mechanisms designed especially against adversarial 3D data.",10.1609/AAAI.V34I01.5443
e93fef07980c96190c1bd8eb29bba2620cc4c066,ANF: Crafting Transferable Adversarial Point Clouds via Adversarial Noise Factorization,2025,"Transfer-based adversarial attacks involve generating adversarial point clouds in surrogate models and transferring them to other models to assess 3D model robustness. However, current methods rely too much on surrogate model parameters, limiting transferability. In this work, we use Shapley value to identify positive and negative features, guiding optimization of adversarial noise in feature space. To effectively mislead the 3D classifier, we factorize the adversarial noise into positive and negative noise, with the former keeping the features of the adversarial point cloud close to the negative features, and the latter and the adversarial noise moving it away from the positive features. Finally, a novel adversarial point cloud attack method with Adversarial Noise Factorization is proposed, which is abbreviated as <bold>ANF</bold>. ANF simultaneously optimizes the adversarial noise and its positive and negative noise in the feature space, only relying on partial network parameters, which significantly reduces the reliance on the surrogate model and improves the transferability of the adversarial point cloud. Experiments on well-recognized benchmark datasets show that the transferability of adversarial point clouds generated by ANF could be improved by more than 26.7<inline-formula><tex-math notation=""LaTeX"">$\%$</tex-math><alternatives><mml:math><mml:mo>%</mml:mo></mml:math><inline-graphic xlink:href=""qian-ieq1-3436593.gif""/></alternatives></inline-formula> on average over state-of-the-art transfer-based adversarial attack methods.",10.1109/TBDATA.2024.3436593
01c4d1079cac74a93cefd460ee002aa068bf402b,Market Manipulation: An Adversarial Learning Framework for Detection and Evasion,2020,We propose an adversarial learning framework to capture the evolving game between a regulator who develops tools to detect market manipulation and a manipulator who obfuscates actions to evade detection. The model includes three main parts: (1) a generator that learns to adapt original manipulation order streams to resemble trading patterns of a normal trader while preserving the manipulation intent; (2) a discriminator that differentiates the adversarially adapted manipulation order streams from normal trading activities; and (3) an agent-based simulator that evaluates the manipulation effect of adapted outputs. We conduct experiments on simulated order streams associated with a manipulator and a market-making agent respectively. We show examples of adapted manipulation order streams that mimic a specified market maker's quoting patterns and appear qualitatively different from the original manipulation strategy we implemented in the simulator. These results demonstrate the possibility of automatically generating a diverse set of (unseen) manipulation strategies that can facilitate the training of more robust detection algorithms.,10.24963/ijcai.2020/638
cb2bb8df3d2bb35d0123e4ff11c2e13bdc23683e,The Effect of Liquidity on the Spoofability of Financial Markets,2024,"We investigate the relationship between market liquidity and spoofing, a manipulative practice involving the submission of deceptive orders aimed at misleading other traders. Utilizing an agent-based market simulator, we model markets with varying levels of liquidity, adjusting the spread and intervals of a market maker’s orders to control liquidity. Within these simulated markets, we evaluate the effectiveness of two novel spoofing strategies against a benchmark approach. Our experiments show that in high-liquidity markets, spoofing is substantially less profitable and less detrimental to other traders compared to their low-liquidity counterparts. Additionally, we identify two distinct spoofing behavior regimes based on liquidity, each of which employ drastically different profit-making strategies. Finally, building on our quantitative findings, we identify and expound upon the mechanisms through which liquidity mitigates market manipulation.",10.1145/3677052.3698634
b0938a65f10cbe9121a1b91bc4da762e43fe0696,Learning to Manipulate a Financial Benchmark,2023,"Financial benchmarks estimate market values or reference rates used in a wide variety of contexts, but are often calculated from data generated by parties who have incentives to manipulate these benchmarks. Since the LIBOR scandal in 2011, market participants, scholars, and regulators have scrutinized financial benchmarks and the ability of traders to manipulate them. We study the impact on market welfare of manipulating transaction-based benchmarks in a simulated market environment. Our market consists of a single benchmark manipulator with external holdings dependent on the benchmark, and numerous background traders unaffected by the benchmark. Background traders use standard zero intelligence (ZI) strategies. We explore two types of manipulative trading strategies: manually adjusted ZI, and strategies generated by deep reinforcement learning. We find that manipulation decreases market surplus for the manipulator but increases it (to a lesser degree) for the background traders. It also decreases the quality of market information. Including the benchmark holdings, aggregate profits for the manipulator substantially increase. The negative impacts of manipulation, therefore, fall to the external counterparties to the manipulator’s benchmark holdings, as well as anyone relying on benchmark information for decision making.",10.1145/3604237.3626847
01c663a80e486575e0d6f5d44e9c95f8d659577f,Utility Optimization for Resource Allocation in Edge Network Slicing Using DRL,2020,"Network slicing and Multi-access Edge Computing (MEC) have been envisioned as promising technique in the fifth generation mobile communication (5G). In this work, we study joint optimization of radio and computation resource in network slicing with MEC to maximize utility of Mobile Virtual Network Operator (MVNO), while meeting slice Quality of Service (QoS) requirements. On account of the dynamic change of slice demands and environment information, it is hard to solve resource allocation problems with conventional methods. Inspired by the superiority of deep reinforcement learning (DRL) in decision-making problems with the high state space and continuous action space. We formulate the utility maximization problem as a markov decision process (MDP). With an MVNO controller, the problem can be solved utilizing deep deterministic policy gradient (DDPG) algorithm to execute the dynamic resource allocation scheme. Simulation results show that utility performance of the proposed algorithm outperforms than the benchmark algorithms and enables dynamic resource allocation scheme.",10.1109/GLOBECOM42002.2020.9322481
43df24beb5c0890645f23ad86e1f3414148fbf2a,A Priori Quantification of Transfer Learning Performance on Time Series Classification for Cyber-Physical Health Systems,2022,"Cyber-Physical Systems fully encompass the intelligent system from signal acquisition through to physical computing and computation - it requires consideration of the deep entanglement between computational and physical elements. Human health and performance is increasingly being measured and analyzed using machine learning to identify complex relationships using wearable and pervasive computing. This combination defines the focused area of Cyber-Physical Health Systems. Modern deep learning algorithms, such as one-dimensional convolutional neural networks, have demonstrated excellent performance in classifying time series data because of the ability to identify time-invariant features. A primary challenge of deep learning for time series classification is the large amount of data required for training and many application domains, such as medicine, have challenges obtaining sufficient data. Transfer learning is a deep learning method used to apply feature knowledge from one deep learning model to another; this is a powerful tool when both training datasets are similar and offers smaller datasets the power of more robust larger datasets. This makes it vital that the best source dataset is selected when performing transfer learning and presently there is no a priori metric defined for this purpose. Analyzing time-series data from public human-activity-recognition datasets a neural network autoencoder was used to first transform the source and target datasets into a time-independent feature space. To quantify the suitability of transfer learning datasets the average embedded signal from each dataset was used to calculate the distance between each dataset centroid. Our metric was then applied to predict the success of transfer learning from one dataset to another for the purpose of general time series classification.",10.1109/CCECE49351.2022.9918279
954cf4ffe5d4c0e64f76c029c32e2f2ac6a2eb26,Semi-supervised and Unsupervised Privacy-Preserving Distributed Transfer Learning Approach in HAR Systems,2020,,10.1007/s11277-020-07891-1
3c143cbc26bc75f7b66b74e10b759081ab3d0eff,Impact of COVID-19 on education in India: stakeholders' voice,2023,"PurposeThis exploratory study aims to discover the different forms of challenges encountered by school stakeholders, including students, teachers, parents and management due to the coronavirus disease 2019 (COVID-19) pandemic.Design/methodology/approachQualitative methodology was deployed for the study. A purposive sampling technique was used to select the respondents for a semi-structured interview. Data were examined using interpretative phenomenological analysis (IPA).FindingsIt was found that each stakeholder faced four different challenges: mental distress, physical immobility, financial crunches and technological concerns. Findings suggest that teachers are experiencing higher financial, technological and physical challenges as compared to other stakeholders followed by parents.Originality/valueThis paper discusses the major challenges faced by each stakeholder along with the opportunities. These findings will be useful for educationists, regulatory authorities, policymakers and management of educational institutions in developing countries to revisit their policy frameworks to develop new strategies and processes for the smooth implementation of remote learning during a period of uncertainty.",10.1108/bij-11-2021-0665
ed469359d6fac98e620a8ff9caacb9efa3ccbb36,The Impact of Digital Literacy and Social Presence on Teachers’ Acceptance of Online Professional Development,2022,"The global COVID-19 pandemic has created the urgent need for online instruction throughout all levels, including teacher professional development. As we move beyond the survival phase of remote teacher professional development, it is critical to well understand teacher acceptance and continued use of online professional development. Digital literacy and social presence (SP) have been widely studied to understand online teaching and learning process. However, there is a dearth of studies that examine the impact of digital literacy and SP on the acceptance of online teacher professional development (OTPD). This study aimed to examine if digital literacy and SP affected secondary school teachers’ acceptance and continued use of OTPD. A quantitative method was employed with two hundred and thirty-two Indonesian secondary school teachers completed a 48-item questionnaire based on an extended technology acceptance model and teacher digital literacy framework. Data were analyzed by structural equation modeling. The findings showed that digital literacy and SP significantly affected teachers’ acceptance of OTPD. Therefore, this study suggests that the proposed model is valid to explain teachers’ engagement in OTPD. The results have implications for educational leaders, designers, and facilitators who want to promote online professional development. © 2022 by authors.",10.30935/cedtech/12329
f30f33b0b13b624360a619151d1f4e2b55de14ad,Pose-Invariant Face Recognition via Adaptive Angular Distillation,2022,"Pose-invariant face recognition is a practically useful but challenging task. This paper introduces a novel method to learn pose-invariant feature representation without normalizing profile faces to frontal ones or learning disentangled features. We first design a novel strategy to learn pose-invariant feature embeddings by distilling the angular knowledge of frontal faces extracted by teacher network to student network, which enables the handling of faces with large pose variations. In this way, the features of faces across variant poses can cluster compactly for the same person to create a pose-invariant face representation. Secondly, we propose a Pose-Adaptive Angular Distillation loss to mitigate the negative effect of uneven distribution of face poses in the training dataset to pay more attention to the samples with large pose variations. Extensive experiments on two challenging benchmarks (IJB-A and CFP-FP) show that our approach consistently outperforms the existing methods.",10.1609/aaai.v36i3.20249
01d2e44ffe01d1a4d55151055352b33cd9614927,Product Insights: Analyzing Product Intents in Web Search,2020,"Web search engines are frequently used to access information about products. This has increased in recent times with the rising popularity of e-commerce. However, there is limited understanding of what users search for and their intents when it comes to product search on the web. In this work, we study search logs from Bing web search engine to characterize user intents and study user behavior for product search. We propose a taxonomy of product intents by analyzing product search queries. This itself is a challenging task given that only 15%-17% of queries in the web refer to products. We train machine learning classifiers with query log features to classify queries based on intent with an overall F1-score of 78%. We further analyze various characteristics of product search queries in terms of search metrics like dwell time, success, popularity and session-specific information.",10.1145/3340531.3412090
01d78495d85360230e120bfbd3a5b47e7c943d54,Optimal Margin Distribution Learning in Dynamic Environments,2020,"Recently a promising research direction of statistical learning has been advocated, i.e., the optimal margin distribution learning with the central idea that instead of the minimal margin, the margin distribution is more crucial to the generalization performance. Although the superiority of this new learning paradigm has been verified under batch learning settings, it remains open for online learning settings, in particular, the dynamic environments in which the underlying decision function varies over time. In this paper, we propose the dynamic optimal margin distribution machine and theoretically analyze its regret. Although the obtained bound has the same order with the best known one, our method can significantly relax the restrictive assumption that the function variation should be given ahead of time, resulting in better applicability in practical scenarios. We also derive an excess risk bound for the special case when the underlying decision function only evolves several discrete changes rather than varying continuously. Extensive experiments on both synthetic and real data sets demonstrate the superiority of our method.",10.1609/AAAI.V34I04.6162
31d8c240bc6d32bddadb96e1839aabce0f155017,Adapting to Online Label Shift with Provable Guarantees,2022,"The standard supervised learning paradigm works effectively when training data shares the same distribution as the upcoming testing samples. However, this stationary assumption is often violated in real-world applications, especially when testing data appear in an online fashion. In this paper, we formulate and investigate the problem of \emph{online label shift} (OLaS): the learner trains an initial model from the labeled offline data and then deploys it to an unlabeled online environment where the underlying label distribution changes over time but the label-conditional density does not. The non-stationarity nature and the lack of supervision make the problem challenging to be tackled. To address the difficulty, we construct a new unbiased risk estimator that utilizes the unlabeled data, which exhibits many benign properties albeit with potential non-convexity. Building upon that, we propose novel online ensemble algorithms to deal with the non-stationarity of the environments. Our approach enjoys optimal \emph{dynamic regret}, indicating that the performance is competitive with a clairvoyant who knows the online environments in hindsight and then chooses the best decision for each round. The obtained dynamic regret bound scales with the intensity and pattern of label distribution shift, hence exhibiting the adaptivity in the OLaS problem. Extensive experiments are conducted to validate the effectiveness and support our theoretical findings.",10.48550/arXiv.2207.02121
4181dd768fcae61010b59001adfc9b116f58a516,Posistive-Unlabeled Learning via Optimal Transport and Margin Distribution,2022,"Positive-unlabeled (PU) learning deals with the circumstances where only a portion of positive instances are labeled, while the rest and all negative instances are unlabeled, and due to this confusion, the class prior can not be directly available. Existing PU learning methods usually estimate the class prior by training a nontraditional probabilistic classifier, which is prone to give an overestimation. Moreover, these methods learn the decision boundary by optimizing the minimum margin, which is not suitable in PU learning due to its sensitivity to label noise. In this paper, we enhance PU learning methods from the above two aspects. More specifically, we first explicitly learn a transformation from unlabeled data to positive data by entropy regularized optimal transport to achieve a much more precise estimation for class prior. Then we switch to optimizing the margin distribution, rather than the minimum margin, to obtain a label noise insensitive classifier. Extensive empirical studies on both synthetic and real-world data sets demonstrate the superiority of our proposed method.",10.24963/ijcai.2022/393
1006ec77842d3ff34b924f3a8f09cdf2a9074d8d,Improving generalization of deep neural networks by leveraging margin distribution,2018,,10.1016/j.neunet.2022.03.019.
b7b3d2695da710cd8fa8bb490218c49516b03cc6,Verus: A Practical Foundation for Systems Verification,2024,"Formal verification is a promising approach to eliminate bugs at compile time, before they ship. Indeed, our community has verified a wide variety of system software. However, much of this success has required heroic developer effort, relied on bespoke logics for individual domains, or sacrificed expressiveness for powerful proof automation. Building on prior work on Verus, we aim to enable faster, cheaper verification of rich properties for realistic systems. We do so by integrating and optimizing the best choices from prior systems, tuning our design to overcome barriers encountered in those systems, and introducing novel techniques. We evaluate Verus’s effectiveness with a wide variety of case-study systems, including distributed systems, an OS page table, a library for NUMA-aware concurrent data structure replication, a crash-safe storage system, and a concurrent memory allocator, together comprising 6.1K lines of implementation and 31K lines of proof. Verus verifies code 3–61 × faster and with less effort than the state of the art. Our results suggest that Verus offers a platform for exploring the next frontiers in system-verification research. Because Verus builds on Rust, Verus is also positioned for wider use in production by developers who have already adopted Rust in the pursuit of more robust systems.",10.1145/3694715.3695952
01dd3853c321cfc68c8e0a458018049ce1e83462,Smooth Variational Graph Embeddings for Efficient Neural Architecture Search,2020,"Neural architecture search (NAS) has recently been addressed from various directions, including discrete, sampling-based methods and efficient differentiable approaches. While the former are notoriously expensive, the latter suffer from imposing strong constraints on the search space. Architecture optimization from a learned embedding space for example through graph neural network based variational autoencoders builds a middle ground and leverages advantages from both sides. Such approaches have recently shown good performance on several benchmarks. Yet, their stability and predictive power heavily depends on their capacity to reconstruct networks from the embedding space. In this paper, we propose a two-sided variational graph autoencoder, which allows to smoothly encode and accurately reconstruct neural architectures from various search spaces. We evaluate the proposed approach on neural architectures defined by the ENAS approach, the NAS-Bench-101 and the NAS-Bench-201 search space and show that our smooth embedding space allows to directly extrapolate the performance prediction to architectures outside the seen domain (e.g. with more operations). Thus, it facilitates to predict good network architectures even without expensive Bayesian optimization or reinforcement learning.",10.1109/IJCNN52387.2021.9534092
33762e1c423ece2e466b58ef4a9d6faf6681e9b7,Efficient Data Subset Selection to Generalize Training Across Models: Transductive and Inductive Networks,2024,"Existing subset selection methods for efficient learning predominantly employ discrete combinatorial and model-specific approaches which lack generalizability. For an unseen architecture, one cannot use the subset chosen for a different model. To tackle this problem, we propose $\texttt{SubSelNet}$, a trainable subset selection framework, that generalizes across architectures. Here, we first introduce an attention-based neural gadget that leverages the graph structure of architectures and acts as a surrogate to trained deep neural networks for quick model prediction. Then, we use these predictions to build subset samplers. This naturally provides us two variants of $\texttt{SubSelNet}$. The first variant is transductive (called as Transductive-$\texttt{SubSelNet}$) which computes the subset separately for each model by solving a small optimization problem. Such an optimization is still super fast, thanks to the replacement of explicit model training by the model approximator. The second variant is inductive (called as Inductive-$\texttt{SubSelNet}$) which computes the subset using a trained subset selector, without any optimization. Our experiments show that our model outperforms several methods across several real datasets",10.48550/arXiv.2409.12255
01e23a22031d4e8ac528a191257ae7adb0f8e3c9,Stock Price Prediction using Bi-Directional LSTM based Sequence to Sequence Modeling and Multitask Learning,2020,"The stock market is a dynamic and volatile platform which provides an environment for traders to invest and trade in shares. The price of a stock is dependent on numerous static and dynamic features. Predicting the future price of a particular company’s stock can be extremely beneficial for traders. Seq2Seq modelling helps map an input sequence to an output sequence. In this paper, we propose a system to predict the future Open, High, Close, Low (OHCL) value of a stock using a Bi-Directional LSTM based Sequence to Sequence Modelling. Each OHCL price is an independent sequence and multitask learning helps map the interrelations between them. A multitask system is also proposed which uses sub tasks and shared tasks to model the prices. Stock prices of Tata Consumer Products Limited from the National Stock Exchange (NSE) of India is used. To evaluate the efficiency of the proposed systems, they are compared against various machine learning algorithms. The proposed Seq2Seq and multitask systems comfortably outperform the existing algorithms with RMSE values of 3.98 and 7.87 respectively.",10.1109/UEMCON51285.2020.9298066
396d02524403c4d897a47121ea2029bcd4735b27,Plant disease management: a fine-tuned enhanced CNN approach with mobile app integration for early detection and classification,2024,"Farmers face the formidable challenge of meeting the increasing demands of a rapidly growing global population for agricultural products, while plant diseases continue to wreak havoc on food production. Despite substantial investments in disease management, agriculturists are increasingly turning to advanced technology for more efficient disease control. This paper addresses this critical issue through an exploration of a deep learning-based approach to disease detection. Utilizing an optimized Convolutional Neural Network (E-CNN) architecture, the study concentrates on the early detection of prevalent leaf diseases in Apple, Corn, and Potato crops under various conditions. The research conducts a thorough performance analysis, emphasizing the impact of hyperparameters on plant disease detection across these three distinct crops. Multiple machine learning and pre-trained deep learning models are considered, comparing their performance after fine-tuning their parameters. Additionally, the study investigates the influence of data augmentation on detection accuracy. The experimental results underscore the effectiveness of our fine-tuned enhanced CNN model, achieving an impressive 98.17% accuracy in fungal classes. This research aims to pave the way for more efficient plant disease management and, ultimately, to enhance agricultural productivity in the face of mounting global challenges. To improve accessibility for farmers, the developed model seamlessly integrates with a mobile application, offering immediate results upon image upload or capture. In case of a detected disease, the application provides detailed information on the disease, its causes, and available treatment options.",10.1007/s10462-024-10809-z
01e94a4cab4ac4fc9a727a799b9a2a9a4c99a76a,From relative azimuth to absolute location: pushing the limit of PIR sensor based localization,2020,"Pyroelectric infrared (PIR) sensors are considered to be promising devices for device-free localization due to its advantages of low cost, energy efficiency, and the immunity from multi-path fading. However, most of the existing PIR-based localization systems only utilize the binary information of PIR sensors and therefore require a large number of carefully deployed PIR sensors. A few works directly map the raw data of PIR sensors to one's location using machine learning approaches. However, these data-driven approaches require abundant training data and suffer from environmental change. In this paper, we propose PIRATES, a PIR-based device-free localization system based on the raw data of PIR sensors. The key of PIRATES is to extract a new type of location information called azimuth change. The extraction of the azimuth change relies on the physical properties of PIR sensors. Therefore, no abundant training data are needed and the system is robust to environmental change. Through experiments, we demonstrate that PIRATES can achieve higher localization accuracy than the state-of-the-art approaches. In addition, the information of the azimuth change can be easily incorporated with other information of PIR signals (e.g. amplitude) to improve the localization accuracy.",10.1145/3372224.3380878
01ec21b137f052d80a5501fe725a72f0ad2ccc7e,Speech2Video Synthesis with 3D Skeleton Regularization and Expressive Body Poses,2020,,10.1007/978-3-030-69541-5_19
38bd9f98d405e7b7a56f2788e5d7771759008e20,EmoTalk3D: High-Fidelity Free-View Synthesis of Emotional 3D Talking Head,2024,"We present a novel approach for synthesizing 3D talking heads with controllable emotion, featuring enhanced lip synchronization and rendering quality. Despite significant progress in the field, prior methods still suffer from multi-view consistency and a lack of emotional expressiveness. To address these issues, we collect EmoTalk3D dataset with calibrated multi-view videos, emotional annotations, and per-frame 3D geometry. By training on the EmoTalk3D dataset, we propose a \textit{`Speech-to-Geometry-to-Appearance'} mapping framework that first predicts faithful 3D geometry sequence from the audio features, then the appearance of a 3D talking head represented by 4D Gaussians is synthesized from the predicted geometry. The appearance is further disentangled into canonical and dynamic Gaussians, learned from multi-view videos, and fused to render free-view talking head animation. Moreover, our model enables controllable emotion in the generated talking heads and can be rendered in wide-range views. Our method exhibits improved rendering quality and stability in lip motion generation while capturing dynamic facial details such as wrinkles and subtle expressions. Experiments demonstrate the effectiveness of our approach in generating high-fidelity and emotion-controllable 3D talking heads. The code and EmoTalk3D dataset are released at https://nju-3dv.github.io/projects/EmoTalk3D.",10.48550/arXiv.2408.00297
01efcaf2debf8091b360f7d813693a03ef986c72,When to Use Convolutional Neural Networks for Inverse Problems,2020,"Reconstruction tasks in computer vision aim fundamentally to recover an undetermined signal from a set of noisy measurements. Examples include super-resolution, image denoising, and non-rigid structure from motion, all of which have seen recent advancements through deep learning. However, earlier work made extensive use of sparse signal reconstruction frameworks (e.g. convolutional sparse coding). While this work was ultimately surpassed by deep learning, it rested on a much more developed theoretical framework. Recent work by Papyan et. al. provides a bridge between the two approaches by showing how a convolutional neural network (CNN) can be viewed as an approximate solution to a convolutional sparse coding (CSC) problem. In this work we argue that for some types of inverse problems the CNN approximation breaks down leading to poor performance. We argue that for these types of problems the CSC approach should be used instead and validate this argument with empirical evidence. Specifically we identify JPEG artifact reduction and non-rigid trajectory reconstruction as challenging inverse problems for CNNs and demonstrate state of the art performance on them using a CSC method. Furthermore, we offer some practical improvements to this model and its application, and also show how insights from the CSC model can be used to make CNNs effective in tasks where their naive application fails.",10.1109/cvpr42600.2020.00825
01f5ad533b38ece29721d99084e65d6f42763253,Variational Information Diffusion for Probabilistic Cascades Prediction,2020,"Understanding in-network information diffusion is a fundamental problem in many application domains and one of the primary challenges is to predict the size of the information cascade. Most of the existing models rely either on hypothesized point process (e.g., Poisson and Hawkes process), or simply predict the information propagation via deep neural networks. However, they fail to simultaneously capture the underlying structure of a cascade graph and the propagation of uncertainty in the diffusion, which may result in unsatisfactory prediction performance.To address these, in this work we propose a novel probabilistic cascade prediction framework: Variational Cascade (VaCas) graph learning networks. VaCas allows a non-linear information diffusion inference and models the information diffusion process by learning the latent representation of both the structural and temporal information. It is a pattern-agnostic model leveraging variational inference to learn the node-level and cascade-level latent factors in an unsupervised manner. In addition, VaCas is capable of capturing both the cascade representation uncertainty and node infection uncertainty, while enabling hierarchical pattern learning of information diffusion. Extensive experiments conducted on real-world datasets demonstrate that VaCas significantly improves the prediction accuracy, compared to state-of-the-art approaches, while also enabling interpretability.",10.1109/infocom41043.2020.9155349
a1e23cc5b8d054d01a30a67dc7c33cd0fd49df5b,Enhancing Multi-Scale Diffusion Prediction via Sequential Hypergraphs and Adversarial Learning,2024,"Information diffusion prediction plays a crucial role in understanding the propagation of information in social networks, encompassing both macroscopic and microscopic prediction tasks. Macroscopic prediction estimates the overall impact of information diffusion, while microscopic prediction focuses on identifying the next user to be influenced. While prior research often concentrates on one of these aspects, a few tackle both concurrently. These two tasks provide complementary insights into the diffusion process at different levels, revealing common traits and unique attributes. The exploration of leveraging common features across these tasks to enhance information prediction remains an underexplored avenue. In this paper, we propose an intuitive and effective model that addresses both macroscopic and microscopic prediction tasks. Our approach considers the interactions and dynamics among cascades at the macro level and incorporates the social homophily of users in social networks at the micro level. Additionally, we introduce adversarial training and orthogonality constraints to ensure the integrity of shared features. Experimental results on four datasets demonstrate that our model significantly outperforms state-of-the-art methods.",10.1609/aaai.v38i8.28701
01fe5038f9830369eb7874973a3b08439562af8f,Implementation of Human Face and Spoofing Detection Using Deep Learning on Embedded Hardware,2020,"Human face and spoofing detection are of prime importance in many security verification and law enforcement applications. This paper deals with human face detection and spoofing detection using deep learning. Essential feature extraction from images is the key to achieving considerable accuracy for classification and detection. Convolutional Neural Networks (CNN) are well-equipped to extract vital features on its own from images without the need to manually select and extract features from them. In this paper, a CNN based real human face detection classifier has been proposed. The CNN classifier was implemented on an embedded system device for real-time detection. It accurately predicts whether or not a real human face is present in the frame of the recognizing/detecting camera.",10.1109/ICCCNT49239.2020.9225495
0202845014ce483c57806a80b515e7e3c3a47c3b,Introducing a New Dataset for Event Detection in Cybersecurity Texts,2020,"Detecting cybersecurity events is necessary to keep us informed about the fast growing number of such events reported in text. In this work, we focus on the task of event detection (ED) to identify event trigger words for the cybersecurity domain. In particular, to facilitate the future research, we introduce a new dataset for this problem, characterizing the manual annotation for 30 important cybersecurity event types and a large dataset size to develop deep learning models. Comparing to the prior datasets for this task, our dataset involves more event types and supports the modeling of document-level information to improve the performance. We perform extensive evaluation with the current state-of-the-art methods for ED on the proposed dataset. Our experiments reveal the challenges of cybersecurity ED and present many research opportunities in this area for the future work.",10.18653/v1/2020.emnlp-main.433
0208b8aeed3fdc0199ec5f4bb7cf4207b8f93618,Learning Object Attributes with Category-Free Grounded Language from Deep Featurization,2020,"While grounded language learning, or learning the meaning of language with respect to the physical world in which a robot operates, is a major area in human-robot interaction studies, most research occurs in closed worlds or domain-constrained settings. We present a system in which language is grounded in visual percepts without using categorical constraints by combining CNN-based visual featurization with natural language labels. We demonstrate results comparable to those achieved using handcrafted features for specific traits, a step towards moving language grounding into the space of fully open world recognition.",10.1109/IROS45743.2020.9340824
c0c7c6a90dc5c256718b1f633cbae55fbc788940,Head Pose for Object Deixis in VR-Based Human-Robot Interaction,2022,"Modern robotics heavily relies on machine learning and has a growing need for training data. Advances and commercialization of virtual reality (VR) present an opportunity to use VR as a tool to gather such data for human-robot interactions. We present the Robot Interaction in VR simulator, which allows human participants to interact with simulated robots and environments in real-time. We are particularly interested in spoken interactions between the human and robot, which can be combined with the robot’s sensory data for language grounding. To demonstrate the utility of the simulator, we describe a study which investigates whether a user’s head pose can serve as a proxy for gaze in a VR object selection task. Participants were asked to describe a series of known objects, providing approximate labels for the focus of attention. We demonstrate that using a concept of gaze derived from head pose can be used to effectively narrow the set of objects that are the target of participants’ attention and linguistic descriptions.",10.1109/RO-MAN53752.2022.9900631
844a83ef632af2b6ab8a83e56055a1ee2c65c7d6,Bridging the Gap: Using Deep Acoustic Representations to Learn Grounded Language from Percepts and Raw Speech,2021,"Learning to understand grounded language, which connects natural language to percepts, is a critical research area. Prior work in grounded language acquisition has focused primarily on textual inputs. In this work, we demonstrate the feasibility of performing grounded language acquisition on paired visual percepts and raw speech inputs. This will allow human-robot interactions in which language about novel tasks and environments is learned from end-users, reducing dependence on textual inputs and potentially mitigating the effects of demographic bias found in widely available speech recognition systems. We leverage recent work in self-supervised speech representation models and show that learned representations of speech can make language grounding systems more inclusive towards specific groups while maintaining or even increasing general performance.",10.1609/aaai.v36i10.21335
ee2250b8577b6d680ee52db173c51858d5909f1f,Neural Variational Learning for Grounded Language Acquisition,2021,"We propose a learning system in which language is grounded in visual percepts without specific pre-defined categories of terms. We present a unified generative method to acquire a shared semantic/visual embedding that enables the learning of language about a wide range of real-world objects. We evaluate the efficacy of this learning by predicting the semantics of objects and comparing the performance with neural and non-neural inputs. We show that this generative approach exhibits promising results in language grounding without pre-specifying visual categories under low resource settings. Our experiments demonstrate that this approach is generalizable to multilingual, highly varied datasets.",10.1109/RO-MAN50785.2021.9515374
020d0b6eaa8d03326b7d0acbf4895ba6adbc7251,The Surprising Effectiveness of Linear Models for Visual Foresight in Object Pile Manipulation,2020,"In this paper, we tackle the problem of pushing piles of small objects into a desired target set using visual feedback. Unlike conventional single-object manipulation pipelines, which estimate the state of the system parametrized by pose, the underlying physical state of this system is difficult to observe from images. Thus, we take the approach of reasoning directly in the space of images, and acquire the dynamics of visual measurements in order to synthesize a visual-feedback policy. We present a simple controller using an image-space Lyapunov function, and evaluate the closed-loop performance using three different class of models for image prediction: deep-learning-based models for image-to-image translation, an object-centric model obtained from treating each pixel as a particle, and a switched-linear system where an action-dependent linear map is used. Through results in simulation and experiment, we show that for this task, a linear model works surprisingly well -- achieving better prediction error, downstream task performance, and generalization to new environments than the deep models we trained on the same amount of data. We believe these results provide an interesting example in the spectrum of models that are most useful for vision-based feedback in manipulation, considering both the quality of visual prediction, as well as compatibility with rigorous methods for control design and analysis.",10.1007/978-3-030-66723-8_21
020fe34f1e5c89bdfb22261f1faf016e677ddd1f,3D Printed Brain-Controlled Robot-Arm Prosthetic via Embedded Deep Learning From sEMG Sensors,2020,"In this paper, we present our work on developing robot arm prosthetic via deep learning. Our work proposes to use transfer learning techniques applied to the Google Inception model to retrain the final layer for surface electromyography (sEMG) classification. Data have been collected using the Thalmic Labs Myo Armband and used to generate graph images comprised of 8 subplots per image containing sEMG data captured from 40 data points per sensor, corresponding to the array of 8 sEMG sensors in the armband. Data captured were then classified into four categories (Fist, Thumbs Up, Open Hand, Rest) via using a deep learning model, Inception-v3, with transfer learning to train the model for accurate prediction of each on real-time input of new data. This trained model was then downloaded to the ARM processor based embedding system to enable the brain-controlled robot-arm prosthetic manufactured from our 3D printer. Testing of the functionality of the method, a robotic arm was produced using a 3D printer and off-the-shelf hardware to control it. SSH communication protocols are employed to execute python files hosted on an embedded Raspberry Pi with ARM processors to trigger movement on the robot arm of the predicted gesture.",10.1109/ICMLC51923.2020.9469532
787a9720a9d9c91f0a977dcdc6954e90bac185d4,An Improved Approach for Grasp Force Sensing and Control of Upper Limb Soft Robotic Prosthetics,2023,"The following research proposes a closed loop force control system, which is implemented on a soft robotic prosthetic hand. The proposed system uses a force sensing approach that does not require any sensing elements to be embedded in the prosthetic’s fingers, therefore maintaining their monolithic structural integrity, and subsequently decreasing the cost and manufacturing complexity. This is achieved by embedding an aluminum test specimen with a full bridge strain gauge circuit directly inside the actuator’s housing rather than in the finger. The location of the test specimen is precisely at the location of the critical section of the bending moment on the actuator housing due to the tension in the driving tendon. Therefore, the resulting loadcell can acquire a signal proportional to the prosthetic’s grasping force. A PI controller is implemented and tested using this force sensing approach. The experiment design includes a flexible test object, which serves to visually demonstrate the force controller’s performance through the deformation that the test object experiences. Setpoints corresponding to “light”, “medium”, and “hard” grasps were tested with pinch, tripod, and full grasps and the results of these tests are documented in this manuscript. The developed controller was found to have an accuracy of ±2%. Additionally, the deformation of the test object increased proportionally with the given grasp force setpoint, with almost no deformation during the light grasp test, slight deformation during the medium grasp test, and relatively large deformation of the test object during the hard grasp test.",10.3390/mi14030596
0214eabf37cc0cf0956683d5867557e588abfcaa,Queueing Inference for Process Performance Analysis with Missing Life-Cycle Data,2020,"Measuring key performance indicators, such as queue lengths and waiting times, using event logs serve for improvement of resource-driven business processes. However, existing techniques assume the availability of complete life cycle information, including the time a case was scheduled for execution (aka arrival times). Yet, in practice, such information may be missing for a large portion of the recorded cases. In this paper, we propose a methodology to address missing life-cycle data by incorporating predicted information in business processes performance analysis. Our approach builds upon techniques from queueing theory and leverages supervised learning to accurately predict performance indicators based on an event log with missing data. Our experimental results using both synthetic and real-world data demonstrate the effectiveness of our approach.",10.1109/ICPM49681.2020.00019
8d066e13b32ca1683f968f3fba067babf4aea976,A framework for measuring the quality of business process simulation models,2024,,10.1016/j.is.2024.102447
80f6d7c052fcf662212d3bcd516dbc4472ced7e8,Machine learning in business process management: A systematic literature review,2024,,10.1016/j.eswa.2024.124181
3299227209081c5d294c27a9a1d83f1df2dcbd77,Why am I Waiting? Data-Driven Analysis of Waiting Times in Business Processes,2022,"Waiting times in a business process often arise when a case transitions from one activity to another. Accordingly, analyzing the causes of waiting times of activity transitions can help analysts to identify opportunities for reducing the cycle time of a process. This paper proposes a process mining approach to decompose the waiting time observed in each activity transition in a process into multiple direct causes and to analyze the impact of each identified cause on the cycle time efficiency of the process. An empirical evaluation shows that the proposed approach is able to discover different direct causes of waiting times. The applicability of the proposed approach is demonstrated on a real-life process.",10.48550/arXiv.2212.01392
021bbcefc993c389bad6c1daefd8ff92d0fc2441,Contrastive Code Representation Learning,2020,"Recent work learns contextual representations of source code by reconstructing tokens from their context. For downstream semantic understanding tasks like code clone detection, these representations should ideally capture program functionality. However, we show that the popular reconstruction-based RoBERTa model is sensitive to source code edits, even when the edits preserve semantics. We propose ContraCode: a contrastive pre-training task that learns code functionality, not form. ContraCode pre-trains a neural network to identify functionally similar variants of a program among many non-equivalent distractors. We scalably generate these variants using an automated source-to-source compiler as a form of data augmentation. Contrastive pre-training outperforms RoBERTa on an adversarial code clone detection benchmark by 39% AUROC. Surprisingly, improved adversarial robustness translates to better accuracy over natural code; ContraCode improves summarization and TypeScript type inference accuracy by 2 to 13 percentage points over competitive baselines. All source is available at https://github.com/parasj/contracode.",10.18653/v1/2021.emnlp-main.482
021e4c48728fcff2ca585cbdd079107c1c0cf994,ResNet-based Image Classification of Railway Shelling Defect,2020,"This paper highlights the ability of residual convolutional neural network (ResNet) at classifying railway shelling defect dataset without the requirement for handcrafted features. A 41-convolutional layers ResNet with the residual learning block is introduced. Bottleneck architecture of three convolutional layers is used in ResNet to decrease the computation cost. VGG convolutional neural network (VGGNet) classifier and Support Vector Machine (SVM) classifiers based on Histogram of Oriented Gradient (HOG), Local Binary Patterns (LBP) and Scale-invariant Feature Transform (SIFT) are compared with ResNet classifier at classifying our dataset. The performance of ResNet presented in this paper achieves 95% TOP-1 accuracy in testing dataset. In comparison, the result of ResNet is better than VGGNet with 92% TOP-1 accuracy and HOG (42.88%), LBP (52.26%), SIFT (60.69%). In the context of designing neural computing models for ResNet analysis, this paper shows that ResNet used in our experiment is able to not only achieve the high accuracy in railway shelling defect testing dataset, but also has a faster testing speed than the other classifiers.",10.23919/CCC50068.2020.9189112
76be8c22d952d00ec204e6454b246a8029c42b47,Intelligent Small Sample Defect Detection of Water Walls in Power Plants Using Novel Deep Learning Integrating Deep Convolutional GAN,2023,"Thermal power generation is one of the main forms of electricity generation in the world, and the share of thermal power generation in total electricity generation has long been maintained at over 80% in 2018. However, power plants are often shut down due to boiler accidents, which are mostly caused by water wall damage. At present, the detection method for water wall defects is still in the stage of manual detection, which has a high risk coefficient, long time-frame, and low efficiency. In this article, a deep learning method integrating deep convolutional generating adversarial networks (DCGAN) and a seam carving algorithm to solve the problem of small sample defect detection is proposed. The proposed method uses the seam carving algorithm to solve the overfitting of the DCGAN, for which the DCGAN generates high-quality images. Then, the intelligent small sample defect detection model is built by convolutional neural networks. Finally, the proposed method is used in the defect detection of water walls in the actual thermal power generation plant. To evaluate the performance of our proposed method, we conduct comparison experiments among different GANs and different detection networks integrating different processes used and not used the proposed data expansion method. The experimental results demonstrate that the proposed method can achieve a detection accuracy of 98.43%, which is higher than other methods, and has the best generalization ability.",10.1109/TII.2022.3159817
ee030dd975b478718740a4193e3e871aa1c5f704,Real-time Obstacles Avoidance for Crawler Crane based on DQN,2021,"Safety always is a crucial consideration for crawler cranes running with dozens of tons of load in its work site. However, around its work site, besides static obstacles, there are many dynamic obstacles, such as workers and engineering vehicles. These obstacles are potential risk for crane in congested work site. Therefore, risk probability and loss can be reduced in project of lifting engineering by dynamic lift-path planning system. However, presently, most proposed methods pay attention to search path in totally known environment. In this paper, we present a dynamic obstacles avoidance planner for crawler cranes in no-walk scenarios in partially known environment. This planner considers the lifted module as a 3DOF convex robot with discrete rotational and translational motions. First, we improved the structure of neural network of Deep Q-network by applying Resnet block for achieving real-time path planning for crawler crane. Thereafter, we also improved Artificial Potential Field Method (APFM) and apply it to search path for lifted module. And for shortening training time, we also apply the data generated by APFM to pre-train our neural network. Finally, we verified and compared the performance of APFM and DQN by simulation. The result of simulation can demonstrate the effectiveness of the presented planner.",10.1145/3474963.3474993
02219f187087a44cfe9064e199ef1f7970af9774,Classification and Concentration Prediction of VOC Gases Based on Sensor Array with Machine Learning Algorithms,2020,"In this work, a facile method is proposed to monitor the freshness of meat and fruits by combining a gas sensor array with machine learning algorithms, where the sensor array consists of four commercial metal oxide-based gas sensors. Back-propagation neural network (BPNN) is used for gas classification, and the average accuracy can reach 98.8%. To obtain more effective prediction of VOC gas concentration (ethanol, trimethylamine, and ammonia), four algorithms including BPNN, radial basis function neural network (RBFNN), support vector machine (SVM), and hybrid LDA-SVM, which is a combination of SVM and linear discriminant analysis (LDA) are implemented, which are trained with the same training set. By analyzing and comparing the prediction results of these four algorithm models, the RBFNN achieves the peak performance for the concentration predictions of ethanol and ammonia, and the average relative errors are less than 5% and 6.5%, respectively. For trimethylamine (TMA) concentration prediction, the average relative error of RBFNN is equal to 4.41%, which is better than 5.11% of SVM, while the mean absolute error of RBFNN is slightly inferior to SVM. Therefore, the classification accuracy of the gas type by BPNN and the prediction accuracy of gas concentration by RBFNN can meet the requirement of distinguishing the freshness of food.",10.1109/NEMS50311.2020.9265606
084549090c70e9d3c0116a9cbbd72ffc7363df0d,A review of MEMS-based metal oxide semiconductors gas sensor in Mainland China,2022,"With the growing demand for gas monitoring in various fields as the fast development of the internet of things, metal oxide semiconductors (MOSs) gas sensors based on the advanced microelectromechanical systems (MEMS) technology have achieved great developments in the past decades, especially in mainland China. This review summarizes the development of MEMS-based MOSs gas sensors in terms of the MEMS micro-hotplate, wafer-scale deposition and patterning methods for MOS materials, and several latest applications. Various designs of the micro-hotplates have been proposed, particularly, the suspended membrane type with low power consumption. By combining the ‘bottom up’ and the ‘top down’ strategies, MEMS provides a promising solution for wafer-scale fabrication process of MOSs based gas sensors, which have been successfully applied for the detection of ethanol, H2, H2S, toluene, HCHO, Freon etc. With the diversiform nano-structures of MOSs and emerging machine learning algorithm, great progress has been made recently on the aspects of the sensing performance, pulse heating and intelligent sensing systems.",10.1088/1361-6439/ac5b98
0223a52ad4bbb42cef8d890405d7ce24fb40e57a,Optimizing decision making in concolic execution using reinforcement learning,2020,"This paper presents an improvement to a new opensource testing tool capable of performing concolic execution on x86 binaries. The novelty is to use a reinforcement learning solution that reduces the number of symbolically executed states. It does so by learning a set of models that predict how efficiently it would be to change the conditions at various branch points. Thus, we first reinterpret the state-of-the-art concolic execution algorithm as a typical reinforcement learning environment, then we build estimation models used to prune states that do not look promising. The architecture of the base model is a Deep Q-Network used inside an LSTM that captures the patterns from the ordered set of branch points (path) resulted by executing the application under test with different inputs generated at runtime (experiments). Various reward functions can give automatic feedback from the concolic execution environment to define different policies. These are customizable in our open-source implementation, such that users can define their custom test targets.",10.1109/icstw50294.2020.00025
9b205c73db0f344cd968c774177dbb16d6824fdc,The role of Reinforcement Learning in software testing,2023,,10.1016/j.infsof.2023.107325
4332f91a0f7da18c25520ee1d75ca7697a19f427,RiverFuzzRL - an open-source tool to experiment with reinforcement learning for fuzzing,2021,"Combining fuzzing techniques and reinforcement learning could be an important direction in software testing. However, there is a gap in support for experimentation in this field, as there are no open-source tools to let academia and industry to perform experiments easily. The purpose of this paper is to fill this gap by introducing a new framework, named RiverFuzzRL, on top of our already mature frame-work for AI-guided fuzzing, River. We provide out-of-the-box implementations for users to choose from or customize for their test target. The work presented here is performed on testing binaries and does not require access to the source code, but it can be easily adapted to other types of software testing as well. We also discuss the challenges faced, opportunities, and factors that are important for performance, as seen in the evaluation.",10.1109/ICST49551.2021.00055
02285a0bec976ef65de77bcd9751b7a38261d1c5,Towards Backward-Compatible Representation Learning,2020,"We propose a way to learn visual features that are compatible with previously computed ones even when they have different dimensions and are learned via different neural network architectures and loss functions. Compatible means that, if such features are used to compare images, then ``new'' features can be compared directly to ``old'' features, so they can be used interchangeably. This enables visual search systems to bypass computing new features for all previously seen images when updating the embedding models, a process known as backfilling. Backward compatibility is critical to quickly deploy new embedding models that leverage ever-growing large-scale training datasets and improvements in deep learning architectures and training methods. We propose a framework to train embedding models, called backward-compatible training (BCT), as a first step towards backward compatible representation learning. In experiments on learning embeddings for face recognition, models trained with BCT successfully achieve backward compatibility without sacrificing accuracy, thus enabling backfill-free model updates of visual embeddings.",10.1109/cvpr42600.2020.00640
0e3ffaf73eaf0e5579b0418118b1e7ffb1b91c92,FlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization,2024,"Recent breakthroughs in preference alignment have significantly improved Large Language Models' ability to generate texts that align with human preferences and values. However, current alignment metrics typically emphasize the post-hoc overall improvement, while overlooking a critical aspect: regression, which refers to the backsliding on previously correctly-handled data after updates. This potential pitfall may arise from excessive fine-tuning on already well-aligned data, which subsequently leads to over-alignment and degeneration. To address this challenge, we propose FlipGuard, a constrained optimization approach to detect and mitigate update regression with focal attention. Specifically, FlipGuard identifies performance degradation using a customized reward characterization and strategically enforces a constraint to encourage conditional congruence with the pre-aligned model during training. Comprehensive experiments demonstrate that FlipGuard effectively alleviates update regression while demonstrating excellent overall performance, with the added benefit of knowledge preservation while aligning preferences.",10.48550/arXiv.2410.00508
902104ace7c6b19fb582c42c344e4f423e5085ed,A New Meta-Learning Framework for Estimating Atmospheric Turbulence and Phase Noise in Optical Satellite Internet of Things Systems,2024,"With the advantages of super-high-transmission rate, anti-electromagnetic interference and good confidentiality, optical wireless communication (OWC) systems play an important role in satellite Internet of Things (IoT). In this article, we propose a new meta-learning-based channel estimation scheme (meta-CE) to address the challenges of atmospheric turbulence and phase distortion in satellite OWC links. A neural network-based channel estimator outputs channel parameters, rather than categories, and utilizes meta-learning to enhance its convergence speed and adaptability to new environments. The meta-CE exhibits superior estimation accuracy, fast convergence, and generalization, compared to baseline schemes, and even outperforms the minimum mean square error (MMSE) channel estimation, especially with short pilot symbols, low-signal-to-noise ratio (SNR) and severe turbulence. In a $4 \times 4$ multiple-input–multiple-output (MIMO) scheme with 8-bit pilot symbols and 0-dB SNR, the mean square error of the meta-CE is about 35% lower than that of the MMSE in a strong Gamma-Gamma turbulence channel.",10.1109/JIOT.2023.3329566
022d778a8681702b634d17fa3c7fc0c05574b1df,Evaluation of Algorithm-Based Fault Tolerance for Machine Learning and Computer Vision under Neutron Radiation,2020,"In the past decade, there has been a push for deployment of commercial-off-the-shelf (COTS) avionics due in part to cheaper costs and the desire for more performance. Traditional radiation-hardened processors are expensive and only provide limited processing power. With smaller mission budgets and the need for more computational power, low-cost and highperformance COTS solutions become more attractive for these missions. Due to the computational capacity enhancements provided by COTS technology, machine-learning and computer-vision applications are now being deployed on modern space missions. However, COTS electronics are highly susceptible to radiation environments. As a result, reliability in the underlying computations becomes a concern. Matrix multiplication is used in machine-learning and computer-vision applications as the main computation for decisions, making it a critical part of the application. Therefore, the large time and memory footprint of the matrix multiplication in machine-learning and computer-vision applications makes them even more susceptible to single-event upsets. In this paper, algorithm-based fault tolerance (ABFT) is investigated to mitigate silent data errors in machine learning and computer vision. ABFT is a methodology of data error detection and correction using information redundancy contained in separate data structures from the primary data. In matrix multiplication, ABFT consists of storing checksum data in vectors separate from the matrix to use for error detection and correction. Fault injection into a matrix-multiplication kernel was performed prior to irradiation. Irradiation was then performed on the kernel under wide-spectrum neutrons at Los Alamos Neutron Science Center to observe the mitigation effects of ABFT. Fault injections targeted towards the general-purpose registers show a $48 \times$ reduction in data errors using data-error mitigation with ABFT with a negligible change in run-time. Cross-section results from irradiation show a $5.3 \times$ improvement in reliability of using ABFT as opposed to no mitigation with a $> 99.9999$ confidence level. The results of this experiment demonstrate that ABFT is a viable solution for run-time error correction in matrix multiplication for machine-learning and computer-vision applications in future spacecraft.",10.1109/AERO47225.2020.9172799
64d880bb9218bbf84c20b5ee56c34f1f196dea07,Soft-Error Characterization and Mitigation Strategies for Edge Tensor Processing Units in Space,2024,"The Google Coral Edge Tensor Processing Unit (Edge TPU) offers low-power, high-performance capabilities ideal for enabling deep learning in space. However, as a commercial product, no reliability considerations are made in its design. As a device targeted by current and future space computing platforms, it is vital to mission success to understand the vulnerabilities and possible failure modes prior to flight. In this research, we evaluate the soft-error vulnerabilities of the Edge TPU and propose fault-mitigation techniques to improve device reliability. Several Edge TPUs were irradiated using a wide spectrum neutron beam at the Los Alamos Neutron Science Center to evaluate the reliability of two machine-learning applications with common use cases within the space domain: image classification and semantic segmentation. Through experimentation, a vulnerability within the onboard memory is identified. Responsible for caching model parameters for increased performance, the onboard memory represents a critical device area. Any upsets within the cache risk compromising data integrity and model determinism. Across a variety of models tested, fault accumulation and persistence are consistently observed, resulting in the degradation of model accuracy and confidence. To alleviate the impact of radiation, we propose two fault-mitigation techniques: Naive Refreshing (NR) and Golden Batch Refreshing (GBR). NR periodically reloads model parameters to clear corrupted data. GBR is proposed as an alternative method to reduce reload frequency and improve performance. By leveraging knowledge of the cache vulnerabilities and applying one or more mitigation strategies, Edge TPUs can be properly considered for integration into existing and future flight hardware.",10.1109/TAES.2024.3393929
022d85684754084672b5c0435cc45908b6e65e0a,Domain shift in computer vision models for MRI data analysis: an overview,2020,"Machine learning and computer vision methods are showing good performance in medical imagery analysis. Yet only a few applications are now in clinical use and one of the reasons for that is poor transferability of the models to data from different sources or acquisition domains. Development of new methods and algorithms for the transfer of training and adaptation of the domain in multi-modal medical imaging data is crucial for the development of accurate models and their use in clinics. In present work, we overview methods used to tackle the domain shift problem in machine learning and computer vision. The algorithms discussed in this survey include advanced data processing, model architecture enhancing and featured training, as well as predicting in domain invariant latent space. The application of the autoencoding neural networks and their domain-invariant variations are heavily discussed in a survey. We observe the latest methods applied to the magnetic resonance imaging (MRI) data analysis and conclude on their performance as well as propose directions for further research.",10.1117/12.2587872
022d9928c379c57c7982206da32c6527efae8eca,Attention and Memory-Augmented Networks for Dual-View Sequential Learning,2020,"In recent years, sequential learning has been of great interest due to the advance of deep learning with applications in time-series forecasting, natural language processing, and speech recognition. Recurrent neural networks (RNNs) have achieved superior performance in single-view and synchronous multi-view sequential learning comparing to traditional machine learning models. However, the method remains less explored in asynchronous multi-view sequential learning, and the unalignment nature of multiple sequences poses a great challenge to learn the inter-view interactions. We develop an AMANet (Attention and Memory-Augmented Networks) architecture by integrating both attention and memory to solve asynchronous multi-view learning problem in general, and we focus on experiments in dual-view sequences in this paper. Self-attention and inter-attention are employed to capture intra-view interaction and inter-view interaction, respectively. History attention memory is designed to store the historical information of a specific object, which serves as local knowledge storage. Dynamic external memory is used to store global knowledge for each view. We evaluate our model in three tasks: medication recommendation from a patient's medical records, diagnosis-related group (DRG) classification from a hospital record, and invoice fraud detection through a company's taxation behaviors. The results demonstrate that our model outperforms all baselines and other state-of-the-art models in all tasks. Moreover, the ablation study of our model indicates that the inter-attention mechanism plays a key role in the model and it can boost the predictive power by effectively capturing the inter-view interactions from asynchronous views.",10.1145/3394486.3403055
226479f53335fb0ebd0f07b1a6177da83dbbc164,Transformer-based medication recommendation with a multiple graph augmentation strategy,2024,,10.1016/j.eswa.2024.125091
022ed6daae861295fe7d48fffd7cffb5da91d9ec,ABC-LMPC: Safe Sample-Based Learning MPC for Stochastic Nonlinear Dynamical Systems with Adjustable Boundary Conditions,2020,"Sample-based learning model predictive control (LMPC) strategies have recently attracted attention due to their desirable theoretical properties and their good empirical performance on robotic tasks. However, prior analysis of LMPC controllers for stochastic systems has mainly focused on linear systems in the iterative learning control setting. We present a novel LMPC algorithm, Adjustable Boundary Condition LMPC (ABC-LMPC), which enables rapid adaptation to novel start and goal configurations and theoretically show that the resulting controller guarantees iterative improvement in expectation for stochastic nonlinear systems. We present results with a practical instantiation of this algorithm and experimentally demonstrate that the resulting controller adapts to a variety of initial and terminal conditions on 3 stochastic continuous control tasks.",10.1007/978-3-030-66723-8_1
8dbb1666d82bf62bf8b83c5602e04c513dfe40f4,Constrained Motion Control of an Electro- Hydraulic Actuator Under Multiple Time-Varying Constraints,2023,"The motion control technology of electro-hydraulic actuators has great significance in industrial applications. Constraints significantly limit the motion control performance of actuator motion control, in addition to the inherent nonlinearities and uncertainties of the electro-hydraulic systems. The constraints comprise kinematic and dynamic constraints, and they can be time-varying owing to variations in the system. If the constraints are not fulfilled, the control accuracy may be adversely affected, for instance by actuator vibration, cavitation, or even instability. This article proposes a constrained motion control strategy for a variable-speed pump-driven hydraulic actuator. To robustly track a desired trajectory under constraints, the control strategy combines a nonlinear filter-type trajectory planning strategy and an adaptive robust motion controller. The trajectory planning strategy is designed by considering dynamic and kinematic constraints of the electro-hydraulic system, and it synthesizes a trajectory that reaches the given reference in minimum time while fulfilling these multiple constraints. Meanwhile, the adaptive robust motion controller tracks the synthesized trajectory with guaranteed control accuracy in the presence of inherent nonlinearities of the electro-hydraulic actuator. In addition, the assignments of the multiple constraints are adjusted in real time, which further optimize the constrained motion control performance. Comparative experiments with various given references were conducted to verify the advantages of the proposed constrained control strategy.",10.1109/TII.2023.3249760
023cf59937ab2dff8ef4b7262fe33971c20574e1,TED-CDB: A Large-Scale Chinese Discourse Relation Dataset on TED Talks,2020,"As different genres are known to differ in their communicative properties and as previously, for Chinese, discourse relations have only been annotated over news text, we have created the TED-CDB dataset. TED-CDB comprises a large set of TED talks in Chinese that have been manually annotated according to the goals and principles of Penn Discourse Treebank, but adapted to features that are not present in English. It serves as a unique Chinese corpus of spoken discourse. Benchmark experiments show that TED-CDB poses a challenge for state-of-the-art discourse relation classifiers, whose F1 performance on 4-way classification is 60%. This is a dramatic drop of 35% from performance on the news text in the Chinese Discourse Treebank. Transfer learning experiments have been carried out with the TED-CDB for both same-language cross-domain transfer and same-domain cross-language transfer. Both demonstrate that the TED-CDB can improve the performance of systems being developed for languages other than Chinese and would be helpful for insufficient or unbalanced data in other corpora. The dataset and our Chinese annotation guidelines will be made freely available.",10.18653/v1/2020.emnlp-main.223
2e9990d63103fce47e2017c74201daf3d7b59073,AdaST: Dynamically Adapting Encoder States in the Decoder for End-to-End Speech-to-Text Translation,2025,"In end-to-end speech translation, acoustic representations learned by the encoder are usually fixed and static, from the perspective of the decoder, which is not desirable for dealing with the cross-modal and cross-lingual challenge in speech translation. In this paper, we show the benefits of varying acoustic states according to decoder hidden states and propose an adaptive speech-to-text translation model that is able to dynamically adapt acoustic states in the decoder. We concatenate the acoustic state and target word embedding sequence and feed the concatenated sequence into subsequent blocks in the decoder. In order to model the deep interaction between acoustic states and target hidden states, a speech-text mixed attention sublayer is introduced to replace the conventional cross-attention network. Experiment results on two widely-used datasets show that the proposed method significantly outperforms state-of-the-art neural speech translation models.",10.18653/v1/2021.findings-acl.224
024239664ca3a5c975d93174fdc513c5ac632570,GONET: A Deep Network to Annotate Proteins via Recurrent Convolution Networks,2020,"Finding out the functions of protein in life activities precisely is nontrivial, which is the core of current proteomics research. Gene Ontology standardizes the function of protein into a series of GO terms, each of which belongs to exactly one of the three subontologies: Biological Process (BP), Cellular Component (CC), and Molecular Function (MF). The prediction of protein function can be considered as a multi-label classification problem. Traditional methods often spend a lot of costs to extract handcrafted features and plenty of domain knowledge is needed when solving these tasks, while using deep learning technology can overcome these shortcomings. Here, we propose a deep model GONET based on recurrent convolutional neural networks, which annotates protein in an end-to-end manner. Our model combines protein sequences and protein-protein interaction (PPI) network data, and utilizes representation learning to learn distributed representation of proteins to overcome the sparse nature and semantic independence problem. Moreover, we adopt a quite deep CNNRNN-Attention model, which is able to effectively extract high-order features of protein sequences. We have carried out experiments on several datasets, which achieve the state-of-the-art in some metrics compared with the existing competitive methods.",10.1109/BIBM49941.2020.9313235
9af4f2599858d937d04c983ee6395ca5f678d11d,Evaluating the advancements in protein language models for encoding strategies in protein function prediction: a comprehensive review,2025,"Protein function prediction is crucial in several key areas such as bioinformatics and drug design. With the rapid progress of deep learning technology, applying protein language models has become a research focus. These models utilize the increasing amount of large-scale protein sequence data to deeply mine its intrinsic semantic information, which can effectively improve the accuracy of protein function prediction. This review comprehensively combines the current status of applying the latest protein language models in protein function prediction. It provides an exhaustive performance comparison with traditional prediction methods. Through the in-depth analysis of experimental results, the significant advantages of protein language models in enhancing the accuracy and depth of protein function prediction tasks are fully demonstrated.",10.3389/fbioe.2025.1506508
735ba6669ca79e0844e438e707f6545ed41d9104,"Employing Machine Learning Techniques to Detect Protein Function: A Survey, Experimental, and Empirical Evaluations",2024,"This review article delves deeply into the various machine learning (ML) methods and algorithms employed in discerning protein functions. Each method discussed is assessed for its efficacy, limitations, potential improvements, and future prospects. We present an innovative hierarchical classification system that arranges algorithms into intricate categories and unique techniques. This taxonomy is based on a tri-level hierarchy, starting with the methodology category and narrowing down to specific techniques. Such a framework allows for a structured and comprehensive classification of algorithms, assisting researchers in understanding the interrelationships among diverse algorithms and techniques. The study incorporates both empirical and experimental evaluations to differentiate between the techniques. The empirical evaluation ranks the techniques based on four criteria. The experimental assessments rank: (1) individual techniques under the same methodology sub-category, (2) different sub-categories within the same category, and (3) the broad categories themselves. Integrating the innovative methodological classification, empirical findings, and experimental assessments, the article offers a well-rounded understanding of ML strategies in protein function identification. The paper also explores techniques for multi-task and multi-label detection of protein functions, in addition to focusing on single-task methods. Moreover, the paper sheds light on the future avenues of ML in protein function determination.",10.1109/TCBB.2024.3427381
9d4281083de70eb07d29bd632fd03c627b4f0858,A Deep Learning Framework for Predicting Protein Functions With Co-Occurrence of GO Terms,2022,"The understanding of protein functions is critical to many biological problems such as the development of new drugs and new crops. To reduce the huge gap between the increase of protein sequences and annotations of protein functions, many methods have been proposed to deal with this problem. These methods use Gene Ontology (GO) to classify the functions of proteins and consider one GO term as a class label. However, they ignore the co-occurrence of GO terms that is helpful for protein function prediction. We propose a new deep learning model, named DeepPFP-CO, which uses Graph Convolutional Network (GCN) to explore and capture the co-occurrence of GO terms to improve the protein function prediction performance. In this way, we can further deduce the protein functions by fusing the predicted propensity of the center function and its co-occurrence functions. We use Fmax and AUPR to evaluate the performance of DeepPFP-CO and compare DeepPFP-CO with state-of-the-art methods such as DeepGOPlus and DeepGOA. The computational results show that DeepPFP-CO outperforms DeepGOPlus and other methods. Moreover, we further analyze our model at the protein level. The results have demonstrated that DeepPFP-CO improves the performance of protein function prediction. DeepPFP-CO is available at https://csuligroup.com/DeepPFP/.",10.1109/TCBB.2022.3170719
02465597b3f8e8b154529f026b2e5cef21d4a1b6,A UAV-assisted Blockchain Based Secure Device-to-Device Communication in Internet of Military Things,2020,"The significant advancement of blockchain technology in the field of industrial networks has created a meaningful impact and for this, it comes to the field of the military where blockchain can have a greater impact. In the military environment, a lot of machines and smart devices are connected. Unmanned aerial vehicle (UAV) combined with mobile edge computing (MEC) can offer real-time services. However, through the UAV, smart devices and intelligent machines, attackers can easily access. This leads to disruption on operations and revealing the data confidentiality to an adversary. To prevent this issue an architecture is proposed based on blockchain technology. The architecture is applied to provide a solution for malicious cyber-attacks and prevent data loss to ensure the security on device-to-device (D2D) communication in the military network to provide maximum security. Simulation results illustrate that how efficaciously the applied method can work for restorative the security on D2D communication. Finally, in conclusion, how the proposed framework can be enhanced by applying other feasible techniques along with future research directions is discussed.",10.1109/ICTC49870.2020.9289282
1cc02386fbfb66a2e45dd2214549b37d75f62335,Secure UAV (Drone) and the Great Promise of AI,2024,"UAVs have found their applications in numerous applications from recreational activities to business in addition to military and strategic fields. However, research on UAVs is not going on as quickly as the technology. Especially, when it comes to the security of these devices, the academia is lagging behind the industry. This gap motivates our work in this paper as a stepping stone for future research in this area. A comprehensive survey on the security of UAVs and UAV-based systems can help the research community keep pace with, or even lead the industry. Although there are several reviews on UAVs or related areas, there is no recent survey broadly covering various aspects of security. Moreover, none of the existing surveys highlights current and future trends with a focus on the role of an omnipresent technology such as AI. This paper endeavors to overcome these shortcomings. We conduct a comprehensive review on security challenges of UAVs as well as the related security controls. Then we develop a future roadmap for research in this area with a focus on the role of AI. The future roadmap is established based on the identified current trends, under-researched topics, and a future look-ahead.",10.1145/3673225
0251f43b58e5d5a622e1a77da8a1f3c449779e8c,Interactive Exploration and Refinement of Facial Expression using Manifold Learning,2020,"Posing expressive 3D faces is extremely challenging. Typical facial rigs have upwards of 30 controllable parameters, that while anatomically meaningful, are hard to use due to redundancy of expression, unrealistic configurations, and many semantic and stylistic correlations between the parameters. We propose a novel interface for rapid exploration and refinement of static facial expressions, based on a data-driven face manifold of natural expressions. Rapidly explored face configurations are interactively projected onto this manifold of meaningful expressions. These expressions can then be refined using a 2D embedding of nearby faces, both on and off the manifold. Our validation is fourfold: we show expressive face creation using various devices; we verify that our learnt manifold transcends its training face, to expressively control very different faces; we perform a crowd-sourced study to evaluate the quality of manifold face expressions; and we report on a usability study that shows our approach is an effective interactive tool to author facial expression.",10.1145/3379337.3415877
572cd24f69f7cf5e2b25f55c83f2a41046476cb2,FlexComb: A Facial Landmark-Based Model for Expression Combination Generation,2023,"Facial expressions are a crucial but challenging aspect of animating in-game characters. They provide vital nonverbal communication cues, but given the high complexity and variability of human faces, the task of capturing the natural diversity and affective complexity of human faces can be a labour-intensive process for animators. This motivates the need for more accurate, realistic and lightweight methods for generating emotional expressions for in-game characters.
In this work, we introduce FlexComb, a Facial Landmark-based Expression Combination model, designed to generate a real-time space of realistic facial expression combinations. FlexComb leverages the highly varied CelebV-HQ dataset containing emotions in the wild, and a transformer-based architecture. The central component of the FlexComb system is an emotion recognition model that is trained on the facial dataset, and used to generate a larger dataset of tagged faces. The resulting system generates in-game facial expressions by sampling from this tagged dataset, including expressions that combine emotions in specified amounts. This allows in-game characters to take on variety of realistic facial expressions for a single emotion, which addresses this primary challenge of facial emotion modeling. FlexComb shows potential for expressive facial emotion simulation with applications that include animation, video game development, virtual reality, and human-computer interaction.",10.1609/aiide.v19i1.27529
025a4a0dc226bbe5f289a0de2d8ac49160f4647a,Experimentation with Local Intrusion Detection in IoT Networks Using Supervised Learning,2020,In this paper we are experimenting with an intrusion detection system (IDS) for IoT. The IDS under consideration is employing a machine learning techniques for detecting novel at-tacks in the IoT network. We examine detection based on Support Vector Machines (SVM). The detection models were trained and evaluated for Selective Forward and Blackhole network routing layer attacks using IoT-testbed data and achieved up to 99.8% Accuracy rates and 100% Recall values.,10.1109/DCOSS49796.2020.00073
52e579344816287df9f9a5e9ee60f1c061736283,Intelligent parameter-based in-network IDS for IoT using UNSW-NB15 and BoT-IoT datasets,2024,,10.1016/j.jfranklin.2024.107440
6358227cb519ac7fe9e5bd3316b6532b7a5be7b9,"A Systematic Review of IoT Security: Research Potential, Challenges, and Future Directions",2023,"The Internet of Things (IoT) encompasses a network of physical objects embedded with sensors, software, and data processing technologies that can establish connections and exchange data with other devices and systems via the Internet. IoT devices are incorporated into various products, ranging from ordinary household items to complex industrial appliances. Despite the increasing demand for IoT, security concerns have impeded its development. This article systematically reviews IoT security research, focusing on vulnerabilities, challenges, technologies, and future directions. It surveys 171 recent publications in the field, providing a comprehensive discussion on the development status, challenges, and solutions in IoT. The article outlines IoT architecture patterns and typical features, evaluates existing limitations, and explores strategies for enhancing IoT security. Additionally, the article delves into known IoT attacks and discusses the security countermeasures and mechanisms to address these challenges. It explores the functional requirements of IoT security and explores related technologies and standards. Finally, the article discusses potential future research directions in IoT security.",10.1145/3625094
26f79aa30001acf7d579d2f532ec6b890348f04e,Decision Tree with Pearson Correlation-based Recursive Feature Elimination Model for Attack Detection in IoT Environment,2022,"The industrial revolution in recent years made massive uses of Internet of Things (IoT) applications like smart cities’ growth. This leads to automation in real-time applications to make human life easier. These IoT-enabled applications, technologies, and communications enhance the quality of life, quality of service, people’s well-being, and operational efficiency. The efficiency of these smart devices may harm the end-users, misuse their sensitive information increase cyber-attacks and threats. This smart city expansion is difficult due to cyber attacks. Consequently, it is needed to develop an efficient system model that can protect IoT devices from attacks and threats. To enhance product safety and security, the IoT-enabled applications should be monitored in real-time. This paper proposed an efficient feature selection with a feature fusion technique for the detection of intruders in IoT.  The input IoT data is subjected to preprocessing to enhance the data. From the preprocessed data, the higher-order statistical features are selected using the proposed Decision tree-based Pearson Correlation Recursive Feature Elimination (DT-PCRFE) model. This method efficiently eliminates the redundant and uncorrelated features which will increase resource utilization and reduces the time complexity of the system. Then, the request from IoT devices is converted into word embedding using the feature fusion model to enhance the system robustness. Finally, a Deep Neural network (DNN) has been used to detect malicious attacks with the selected features. This proposed model experiments with the BoT-IoT dataset and the result shows the proposed model efficiency which outperforms other existing models with the accuracy of 99.2%.",10.5755/j01.itc.51.4.31818
025b22d5daa5d115a039bd4003ea52100198d6f5,Unsupervised Domain Adaptation in the Dissimilarity Space for Person Re-identification,2020,"Person re-identification (ReID) remains a challenging task in many real-word video analytics and surveillance applications, even though state-of-the-art accuracy has improved considerably with the advent of deep learning (DL) models trained on large image datasets. Given the shift in distributions that typically occurs between video data captured from the source and target domains, and absence of labeled data from the target domain, it is difficult to adapt a DL model for accurate recognition of target data. We argue that for pair-wise matchers that rely on metric learning, e.g., Siamese networks for person ReID, the unsupervised domain adaptation (UDA) objective should consist in aligning pair-wise dissimilarity between domains, rather than aligning feature representations. Moreover, dissimilarity representations are more suitable for designing open-set ReID systems, where identities differ in the source and target domains. In this paper, we propose a novel Dissimilarity-based Maximum Mean Discrepancy (D-MMD) loss for aligning pair-wise distances that can be optimized via gradient descent. From a person ReID perspective, the evaluation of D-MMD loss is straightforward since the tracklet information allows to label a distance vector as being either within-class or between-class. This allows approximating the underlying distribution of target pair-wise distances for D-MMD loss optimization, and accordingly align source and target distance distributions. Empirical results with three challenging benchmark datasets show that the proposed D-MMD loss decreases as source and domain distributions become more similar. Extensive experimental evaluation also indicates that UDA methods that rely on the D-MMD loss can significantly outperform baseline and state-of-the-art UDA methods for person ReID without the common requirement for data augmentation and/or complex networks.",10.1007/978-3-030-58583-9_10
ece45aeafac9d9e84822da68c1dc8578eb7b54f0,Pro-ReID: Producing reliable pseudo labels for unsupervised person re-identification,2024,,10.1016/j.imavis.2024.105244
025d142e712c5ddf97157e62765ff518da6ef9b2,Encoding formulas as deep networks: Reinforcement learning for zero-shot execution of LTL formulas,2020,"We demonstrate a reinforcement learning agent which uses a compositional recurrent neural network that takes as input an LTL formula and determines satisfying actions. The input LTL formulas have never been seen before, yet the network performs zero-shot generalization to satisfy them. This is a novel form of multi-task learning for RL agents where agents learn from one diverse set of tasks and generalize to a new set of diverse tasks. The formulation of the network enables this capacity to generalize. We demonstrate this ability in two domains. In a symbolic domain, the agent finds a sequence of letters that is accepted. In a Minecraft-like environment, the agent finds a sequence of actions that conform to the formula. While prior work could learn to execute one formula reliably given examples of that formula, we demonstrate how to encode all formulas reliably. This could form the basis of new multitask agents that discover sub-tasks and execute them without any additional training, as well as the agents which follow more complex linguistic commands. The structures required for this generalization are specific to LTL formulas, which opens up an interesting theoretical question: what structures are required in neural networks for zero-shot generalization to different logics?",10.1109/IROS45743.2020.9341325
025ed64cd3c3feb67acf8273c4da870438d946e8,Audio-based Near-Duplicate Video Retrieval with Audio Similarity Learning,2020,"In this work, we address the problem of audio-based near-duplicate video retrieval. We propose the Audio Similarity Learning (AuSiL) approach that effectively captures temporal patterns of audio similarity between video pairs. For the robust similarity calculation between two videos, we first extract representative audio-based video descriptors by leveraging transfer learning based on a Convolutional Neural Network (CNN) trained on a large scale dataset of audio events, and then we calculate the similarity matrix derived from the pairwise similarity of these descriptors. The similarity matrix is subsequently fed to a CNN network that captures the temporal structures existing within its content. We train our network following a triplet generation process and optimizing the triplet loss function. To evaluate the effectiveness of the proposed approach, we have manually annotated two publicly available video datasets based on the audio duplicity between their videos. The proposed approach achieves very competitive results compared to three state-of-the-art methods. Also, unlike the competing methods, it is very robust to the retrieval of audio duplicates generated with speed transformations.",10.1109/ICPR48806.2021.9413056
274e91538db084c3c299476778605a11146f2ae3,Audio-visual scene classification via contrastive event-object alignment and semantic-based fusion,2022,"Previous works on scene classification are mainly based on audio or visual signals, while humans perceive the environmental scenes through multiple senses. Recent studies on audio-visual scene classification separately fine-tune the large-scale audio and image pre-trained models on the target dataset, then either fuse the intermediate representations of the audio model and the visual model, or fuse the coarse-grained decision of both models at the clip level. Such methods ignore the detailed audio events and visual objects in audio-visual scenes (AVS), while humans often identify a scene through both audio events and visual objects within, and the congruence between them. To exploit the fine-grained information of audio events and visual objects in AVS, and coordinate the implicit relationship between audio events and visual objects, this paper proposes a multi-branch model equipped with contrastive event-object alignment (CEOA) and semantic-based fusion (SF) for AVSC. CEOA aims to align the learned embeddings of audio events and visual objects by comparing the difference between audio-visual event-object pairs. Then, visual objects associated with certain audio events and vice versa are accentuated by cross-attention and undergo SF for semantic-level fusion. Experiments show that: 1) the proposed AVSC model equipped with CEOA and SF outperforms the results of audio-only and visual-only models, i.e., the audio-visual results are better than the results from a single modality. 2) CEOA aligns the embeddings of audio events and related visual objects on a fine-grained level, and the SF effectively integrates both; 3) Compared with other large-scale integrated systems, the proposed model shows competitive performance, even without using additional datasets and data augmentation tricks.",10.1109/MMSP55362.2022.9949447
026080308c644ff56c4baa6220e22424d5b47819,Deshufflegan: A Self-Supervised Gan to Improve Structure Learning,2020,"Generative Adversarial Networks (GANs) triggered an increased interest in problem of image generation due to their improved output image quality and versatility for expansion towards new methods. Numerous GAN-based works attempt to improve generation by architectural and loss-based extensions. We argue that one of the crucial points to improve the GAN performance in terms of realism and similarity to the original data distribution is to be able to provide the model with a capability to learn the spatial structure in data. To that end, we propose the DeshuffleGAN to enhance the learning of the discriminator and the generator, via a self-supervision approach. Specifically, we introduce a deshuffling task that solves a puzzle of randomly shuffled image tiles, which in turn helps the DeshuffleGAN learn to increase its expressive capacity for spatial structure and realistic appearance. We provide experimental evidence for the performance improvement in generated images, compared to the baseline methods, which is consistently observed over two different datasets.",10.1109/ICIP40778.2020.9190774
d7ec2d79013491b6b0cf00d51e67e32e474a15ec,SS-CPGAN: Self-Supervised Cut-and-Pasting Generative Adversarial Network for Object Segmentation,2023,This paper proposes a novel self-supervised based Cut-and-Paste GAN to perform foreground object segmentation and generate realistic composite images without manual annotations. We accomplish this goal by a simple yet effective self-supervised approach coupled with the U-Net discriminator. The proposed method extends the ability of the standard discriminators to learn not only the global data representations via classification (real/fake) but also learn semantic and structural information through pseudo labels created using the self-supervised task. The proposed method empowers the generator to create meaningful masks by forcing it to learn informative per-pixel and global image feedback from the discriminator. Our experiments demonstrate that our proposed method significantly outperforms the state-of-the-art methods on the standard benchmark datasets.,10.3390/s23073649
9aa9eaf3744aa5c5eb38797098734036597a85d2,Exploring DeshuffleGANs in Self-Supervised Generative Adversarial Networks,2020,,10.1016/j.patcog.2021.108244
2eb429a9fa6b40d974688d586eb6468ae2231908,A Systematic Survey of Regularization and Normalization in GANs,2020,"Generative Adversarial Networks (GANs) have been widely applied in different scenarios thanks to the development of deep neural networks. The original GAN was proposed based on the non-parametric assumption of the infinite capacity of networks. However, it is still unknown whether GANs can fit the target distribution without any prior information. Due to the overconfident assumption, many issues remain unaddressed in GANs training, such as non-convergence, mode collapses, and gradient vanishing. Regularization and normalization are common methods of introducing prior information to stabilize training and improve discrimination. Although a handful number of regularization and normalization methods have been proposed for GANs, to the best of our knowledge, there exists no comprehensive survey that primarily focuses on objectives and development of these methods, apart from some incomprehensive and limited-scope studies. In this work, we conduct a comprehensive survey on the regularization and normalization techniques from different perspectives of GANs training. First, we systematically describe different perspectives of GANs training and thus obtain the different objectives of regularization and normalization. Based on these objectives, we propose a new taxonomy. Furthermore, we compare the performance of the mainstream methods on different datasets and investigate the applications of regularization and normalization techniques that have been frequently employed in state-of-the-art GANs. Finally, we highlight potential future directions of research in this domain. Code and studies related to the regularization and normalization of GANs in this work are summarized at https://github.com/iceli1007/GANs-Regularization-Review.",10.1145/3569928
0260c69e347ad2f47f65d43916425933bbde6b69,Deep Probabilistic Imaging: Uncertainty Quantification and Multi-modal Solution Characterization for Computational Imaging,2020,"Computational image reconstruction algorithms generally produce a single image without any measure of uncertainty or confidence. Regularized Maximum Likelihood (RML) and feed-forward deep learning approaches for inverse problems typically focus on recovering a point estimate. This is a serious limitation when working with under-determined imaging systems, where it is conceivable that multiple image modes would be consistent with the measured data. Characterizing the space of probable images that explain the observational data is therefore crucial. In this paper, we propose a variational deep probabilistic imaging approach to quantify reconstruction uncertainty. Deep Probabilistic Imaging (DPI) employs an untrained deep generative model to estimate a posterior distribution of an unobserved image. This approach does not require any training data; instead, it optimizes the weights of a neural network to generate image samples that fit a particular measurement dataset. Once the network weights have been learned, the posterior distribution can be efficiently sampled. We demonstrate this approach in the context of interferometric radio imaging, which is used for black hole imaging with the Event Horizon Telescope, and compressed sensing Magnetic Resonance Imaging (MRI).",10.1609/aaai.v35i3.16366
0edda12a5c98a3fdda4001f996618b9289044497,pcaGAN: Improving Posterior-Sampling cGANs via Principal Component Regularization,2024,"In ill-posed imaging inverse problems, there can exist many hypotheses that fit both the observed measurements and prior knowledge of the true image. Rather than returning just one hypothesis of that image, posterior samplers aim to explore the full solution space by generating many probable hypotheses, which can later be used to quantify uncertainty or construct recoveries that appropriately navigate the perception/distortion trade-off. In this work, we propose a fast and accurate posterior-sampling conditional generative adversarial network (cGAN) that, through a novel form of regularization, aims for correctness in the posterior mean as well as the trace and K principal components of the posterior covariance matrix. Numerical experiments demonstrate that our method outperforms contemporary cGANs and diffusion models in imaging inverse problems like denoising, large-scale inpainting, and accelerated MRI recovery. The code for our model can be found here: https://github.com/matt-bendel/pcaGAN.",10.48550/arXiv.2411.00605
02633cd97792eb483756f660c1fbb8dd327be319,Sentimental Analysis on voice using AWS Comprehend,2020,"Sentimental analysis plays an important role in these days because many start-ups have started with user-driven content [1]. Sentiment analysis is an important research area in natural language processing. Natural language processing has a wide range of applications like voice recognition, machine translation, product review, aspect-oriented product analysis, sentiment analysis and text classification etc [2]. This process will improve the business by analyse the emotions of the conversation. In this project author going to perform sentimental analysis using Amazon Comprehend. Amazon Comprehend is a natural language processing (NLP) service that uses machine learning to extract the content of the document. By using this service can extract the unstructured data like images, voice etc. Thus, will identify the emotions of the conversation and give the output whether the conversation is Positive, Negative, Neutral, or Mixed. To perform this author going to use some services from Aws like s3 which is used for the data store, Transcribe which is used for converting the audio to text, Aws Glue is used to generate the metadata from the comprehend file, Aws Comprehend is used to generate the sentiment file from the audio, Lambda is used to trigger from the data store s3, Aws Athena is used to convert text into structured data and finally there is quick sight where he can visualize the data from the given file.",10.1109/ICCCI48352.2020.9104105
0269b13ccbe3e3a81213e7ca2b049298d3a154f5,Learning to Cartoonize Using White-Box Cartoon Representations,2020,"This paper presents an approach for image cartoonization. By observing the cartoon painting behavior and consulting artists, we propose to separately identify three white-box representations from images: the surface representation that contains smooth surface of cartoon images, the structure representation that refers to the sparse color-blocks and flatten global content in the celluloid style workflow, and the texture representation that reflects high-frequency texture, contours and details in cartoon images. A Generative Adversarial Network (GAN) framework is used to learn the extracted representations and to cartoonize images. The learning objectives of our method are separately based on each extracted representations, making our framework controllable and adjustable. This enables our approach to meet artists' requirements in different styles and diverse use cases. Qualitative comparisons and quantitative analyses, as well as user studies, have been conducted to validate the effectiveness of this approach, and our method outperforms previous methods in all comparisons. Finally, the ablation study demonstrates the influence of each component in our framework.",10.1109/cvpr42600.2020.00811
702b70b2164978094d84dc3ac5689667fc7ccdec,Heterogeneous Graph Attention Network Based Statistical Timing Library Characterization with Parasitic RC Reduction,2024,"Statistical timing characterization for standard cell library poses significant challenge to accuracy and runtime cost. Prior analytical and machine learning-based methods neglect the profound influence induced by layout-dependent parasitic resistor and capacitor (RC) network in cell netlist as well as the timing correlation between topological structures of cells and process, voltage, and temperature (PVT) corners, resulting in tremendous simulation effort and/or poor accuracy. In this work, an accurate and efficient statistical cell timing library characterization framework is proposed based on heterogeneous graph attention network (HGAT) assisted with parasitic RC reduction approach, where the transistors and parasitic RC in cell are represented as heterogeneous nodes for graph learning and redundant RC nodes are removed to alleviate node imbalance issue and improve prediction accuracy. The proposed framework was validated with TSMC 22nm standard cells under multiple PVT corners to predict the standard deviation of cell delay with the error of 2.67% on average for all validated cells in terms of relative Root Mean Squared Error (rRMSE) with $3 \times $ characterization runtime speedup, achieving $2.7 \sim 6.9 \times $ accuracy improvement compared with prior works. The predicted statistical timing libraries were further validated with ISCAS’89 benchmark circuits for statistical static timing analysis (SSTA), where the critical path delay at $3 \sigma$ percentile point is reported with the average mismatch of $1.34 ps$ compared with foundry-provided library, showing $10.7 \sim 14.5 \times $ better accuracy than the competitive approaches.",10.1109/ASP-DAC58780.2024.10473881
026b397fda58681f3758e3a0c6eaee118bab65ca,AirID: Injecting a Custom RF Fingerprint for Enhanced UAV Identification using Deep Learning,2020,"We propose a framework called AirID that identifies friendly/authorized UAVs using RF signals emitted by radios mounted on them through a technique called as RF fingerprinting. Our main contribution is a method of intentionally inserting ‘signatures’ in the transmitted I/Q samples from each UAV, which are detected through a deep convolutional neural network (CNN) at the physical layer, without affecting the ongoing UAV data communication process. Specifically, AirID addresses the challenge of how to overcome the channel-induced perturbations in the transmitted signal that lowers identification accuracy. AirID is implemented using Ettus B200mini Software Defined Radios (SDRs) that serve as both static ground UAV identifiers, as well as mounted on DJI Matrice M100 UAVs to perform the identification collaboratively as an aerial swarm. AirID tackles the well-known problem of low RF fingerprinting accuracy in ‘train on one day test on another day’ conditions as the aerial environment is constantly changing. Results reveal 98% identification accuracy for authorized UAVs, while maintaining a stable communication BER of 10 -4 for the evaluated cases.",10.1109/GLOBECOM42002.2020.9322561
e80ad584e04c53fbcaf39528ed1b86f46fa6e447,SignalFormer: Hybrid Transformer for Automatic Drone Identification Based on Drone RF Signals,2023,"With the growing integration of drones into various civilian applications, the demand for effective automatic drone identification (ADI) technology has become essential to monitor malicious drone flights and mitigate potential threats. While numerous convolutional neural network (CNN)-based methods have been proposed for ADI tasks, the inherent local connectivity of the convolution operator in CNN models severely constrains RF signal identification performance. In this paper, we propose an innovative hybrid transformer model featuring a CNN-based tokenization method that is capable of generating T-F tokens enriched with significant local context information, and complemented by an efficient gated self-attention mechanism to capture global time/frequency correlations among these T-F tokens. Furthermore, we underscore the substantial impact of incorporating phase information into the input of the SignalFormer model. We evaluated the proposed method on two public datasets under Gaussian white noise and co-frequency signal interference conditions, The SignalFormer model achieved impressive identification accuracy of 97.57% and 98.03% for coarse-grained identification tasks, and 97.48% and 98.16% for fine-grained identification tasks. Furthermore, we introduced a class-incremental learning evaluation to demonstrate SignalFormer’s competence in handling previously unseen categories of drone signals. The above results collectively demonstrate that the proposed method is a promising solution for supporting the ADI task in reliable ways.",10.3390/s23229098
026b4815783ff561668f9b4dbd923baecf624aab,Efficiently Guiding Imitation Learning Agents with Human Gaze,2020,"Human gaze is known to be an intention-revealing signal in human demonstrations of tasks. In this work, we use gaze cues from human demonstrators to enhance the performance of agents trained via three popular imitation learning methods -- behavioral cloning (BC), behavioral cloning from observation (BCO), and Trajectory-ranked Reward EXtrapolation (T-REX). Based on similarities between the attention of reinforcement learning agents and human gaze, we propose a novel approach for utilizing gaze data in a computationally efficient manner, as part of an auxiliary loss function, which guides a network to have higher activations in image regions where the human's gaze fixated. This work is a step towards augmenting any existing convolutional imitation learning agent's training with auxiliary gaze data. Our auxiliary coverage-based gaze loss (CGL) guides learning toward a better reward function or policy, without adding any additional learnable parameters and without requiring gaze data at test time. We find that our proposed approach improves the performance by 95% for BC, 343% for BCO, and 390% for T-REX, averaged over 20 different Atari games. We also find that compared to a prior state-of-the-art imitation learning method assisted by human gaze (AGIL), our method achieves better performance, and is more efficient in terms of learning with fewer demonstrations. We further interpret trained CGL agents with a saliency map visualization method to explain their performance. At last, we show that CGL can help alleviate a well-known causal confusion problem in imitation learning.",10.5555/3463952.3464081
026c4e3b7591bd24371e281ed07a12bacecc109f,DeepPM: Efficient Power Management in Edge Data Centers using Energy Storage,2020,"With the rapid development of the Internet of Things (IoT), computational workloads are gradually moving toward the internet edge for low latency. Due to significant workload fluctuations, edge data centers built in distributed locations suffer from resource underutilization and requires capacity underprovisioning to avoid wasting capital investment. The workload fluctuations, however, also make edge data centers more suitable for battery-assisted power management to counter the performance impact due to underprovisioning. In particular, the workload fluctuations allow the battery to be frequently recharged and made available for temporary capacity boosts. But, using batteries can overload the data center cooling system which is designed with a matching capacity of the power system. In this paper, we design a novel power management solution, DeepPM, that exploits the UPS battery and cold air inside the edge data center as energy storage to boost the performance. DeepPM uses deep reinforcement learning (DRL) to learn the data center thermal behavior online in a model-free manner and uses it on-the-fly to determine power allocation for optimum latency performance without overheating the data center. Our evaluation shows that DeepPM can improve latency performance by more than 50% compared to a power capping baseline while the server inlet temperature remains within safe operating limits (e.g., 32°C).",10.1109/CLOUD49709.2020.00058
3273aa597d150cf3589831e0da006f92f69e8739,Mobile Task Offloading Under Unreliable Edge Performance,2021,"Offloading resource-hungry tasks from mobile devices to an edge server has been explored recently to improve task com- pletion time as well as save battery energy. The low la- tency computing resource from edge servers are a perfect companion to realize such task offloading. However, edge servers may su er from unreliable performance due to its rapid workload variation and reliance on intermittent re- newable energy. Further, batteries in mobile devices make online optimum offloading decisions challenging since it in- tertwines offloading decisions across di erent tasks. In this paper, we propose a deep Q-learning based task offloading solution, DeepTO, for online task offloading. DeepTO learns edge server performance in a model-free manner and takes future battery needs of the mobile device into account. Us- ing a simulation-based evaluation, we show that DeepTO can perform close to the optimum solution that has com- plete future knowledge.",10.1145/3466826.3466838
026c61fbf90b80a8c612548de90528adcd94bd19,Finding a Choice in a Haystack: Automatic Extraction of Opt-Out Statements from Privacy Policy Text,2020,"Website privacy policies sometimes provide users the option to opt-out of certain collections and uses of their personal data. Unfortunately, many privacy policies bury these instructions deep in their text, and few web users have the time or skill necessary to discover them. We describe a method for the automated detection of opt-out choices in privacy policy text and their presentation to users through a web browser extension. We describe the creation of two corpora of opt-out choices, which enable the training of classifiers to identify opt-outs in privacy policies. Our overall approach for extracting and classifying opt-out choices combines heuristics to identify commonly found opt-out hyperlinks with supervised machine learning to automatically identify less conspicuous instances. Our approach achieves a precision of 0.93 and a recall of 0.9. We introduce Opt-Out Easy, a web browser extension designed to present available opt-out choices to users as they browse the web. We evaluate the usability of our browser extension with a user study. We also present results of a large-scale analysis of opt-outs found in the text of thousands of the most popular websites.",10.1145/3366423.3380262
d2d032ba40dd2700a83de80c1fdb4b26ceb9ab3c,Large Language Models: A New Approach for Privacy Policy Analysis at Scale,2024,"The number and dynamic nature of web sites and mobile applications present regulators and app store operators with significant challenges when it comes to enforcing compliance with applicable privacy and data protection laws. Over the past several years, people have turned to Natural Language Processing (NLP) techniques to automate privacy compliance analysis (e.g., comparing statements in privacy policies with analysis of the code and behavior of mobile apps) and to answer people’s privacy questions. Traditionally, these NLP techniques have relied on labor-intensive and potentially error-prone manual annotation processes to build the corpora necessary to train them. This article explores and evaluates the use of Large Language Models (LLMs) as an alternative for effectively and efficiently identifying and categorizing a variety of data practice disclosures found in the text of privacy policies. Specifically, we report on the performance of ChatGPT and Llama 2, two particularly popular LLM-based tools. This includes engineering prompts and evaluating different configurations of these LLM techniques. Evaluation of the resulting techniques on well-known corpora of privacy policy annotations yields an F1 score exceeding 93%. This score is higher than scores reported earlier in the literature on these benchmarks. This performance is obtained at minimal marginal cost (excluding the cost required to train the foundational models themselves). These results, which are consistent with those reported in other domains, suggest that LLMs offer a particularly promising approach to automated privacy policy analysis at scale.",10.48550/arXiv.2405.20900
026fe2c17c54d2e491545da42eb2912886a29657,PsyNet: Self-Supervised Approach to Object Localization Using Point Symmetric Transformation,2020,"Existing co-localization techniques significantly lose performance over weakly or fully supervised methods in accuracy and inference time. In this paper, we overcome common drawbacks of co-localization techniques by utilizing self-supervised learning approach. The major technical contributions of the proposed method are two-fold. 1) We devise a new geometric transformation, namely point symmetric transformation and utilize its parameters as an artificial label for self-supervised learning. This new transformation can also play the role of region-drop based regularization. 2) We suggest a heat map extraction method for computing the heat map from the network trained by self-supervision, namely class-agnostic activation mapping. It is done by computing the spatial attention map. Based on extensive evaluations, we observe that the proposed method records new state-of-the-art performance in three fine-grained datasets for unsupervised object localization. Moreover, we show that the idea of the proposed method can be adopted in a modified manner to solve the weakly supervised object localization task. As a result, we outperform the current state-of-the-art technique in weakly supervised object localization by a significant gap.",10.1609/AAAI.V34I07.6615
573fd9faf49ca9ee9d44d136d4a6684df7b74a76,Clustering-inspired channel selection method for weakly supervised object localization,2024,,10.1016/j.patrec.2024.04.005
027486a1f89ca51fb17632f52538090747c64a6a,Analysis of Multivariate Scoring Functions for Automatic Unbiased Learning to Rank,2020,"Leveraging biased click data for optimizing learning to rank systems has been a popular approach in information retrieval. Because click data is often noisy and biased, a variety of methods have been proposed to construct unbiased learning to rank (ULTR) algorithms for the learning of unbiased ranking models. Among them, automatic unbiased learning to rank (AutoULTR) algorithms that jointly learn user bias models (i.e., propensity models) with unbiased rankers have received a lot of attention due to their superior performance and low deployment cost in practice. Despite their differences in theories and algorithm design, existing studies on ULTR usually use uni-variate ranking functions to score each document or result independently. On the other hand, recent advances in context-aware learning-to-rank models have shown that multivariate scoring functions, which read multiple documents together and predict their ranking scores jointly, are more powerful than uni-variate ranking functions in ranking tasks with human-annotated relevance labels. Whether such superior performance would hold in ULTR with noisy data, however, is mostly unknown. In this paper, we investigate existing multivariate scoring functions and AutoULTR algorithms in theory and prove that permutation invariance is a crucial factor that determines whether a context-aware learning-to-rank model could be applied to existing AutoULTR framework. Our experiments with synthetic clicks on two large-scale benchmark datasets show that AutoULTR models with permutation-invariant multivariate scoring functions significantly outperform those with uni-variate scoring functions and permutation-variant multivariate scoring functions.",10.1145/3340531.3412128
fc54fee135b8a97bf952d9478395d3a6d83b8cb8,Unbiased Learning-to-Rank Needs Unconfounded Propensity Estimation,2024,,10.1145/3626772.3657772
80a1c371f9995b898a3b1aaee17f79a26e21ac56,Mitigating Exploitation Bias in Learning to Rank with an Uncertainty-aware Empirical Bayes Approach,2023,"Ranking is at the core of many artificial intelligence (AI) applications, including search engines, recommender systems, etc. Modern ranking systems are often constructed with learning-to-rank (LTR) models built from user behavior signals. While previous studies have demonstrated the effectiveness of using user behavior signals (e.g., clicks) as both features and labels of LTR algorithms, we argue that existing LTR algorithms that indiscriminately treat behavior and non-behavior signals in input features could lead to suboptimal performance in practice. Because user behavior signals often have strong correlations with the ranking objective and can only be collected on items that have already been shown to users, directly using behavior signals in LTR could create an exploitation bias that hurts the system performance in the long run. To address the exploitation bias, we propose an uncertainty-aware empirical Bayes based ranking algorithm, referred to as EBRank. Specifically, EBRank uses a sole non-behavior feature-based prior model to get a prior estimation of relevance. In the dynamic training and serving of ranking systems, EBRank uses the observed user behaviors to update posterior relevance estimation instead of concatenating behaviors as features in ranking models. Besides, EBRank additionally applies an uncertainty-aware exploration strategy to explore actively and collect user behaviors for empirical Bayesian modeling. Experiments on three public datasets show that EBRank is effective, practical and significantly outperforms state-of-the-art ranking algorithms.",10.1145/3589334.3645487
2398cea6d38816bc9de41880f7cbbb4ee768ec37,Metric-agnostic Ranking Optimization,2023,"Ranking is at the core of Information Retrieval. Classic ranking optimization studies often treat ranking as a sorting problem with the assumption that the best performance of ranking would be achieved if we rank items according to their individual utility. Accordingly, considerable ranking metrics have been developed and learning-to-rank algorithms that have been designed to optimize these simple performance metrics have been widely used in modern IR systems. As applications evolve, however, people's need for information retrieval have shifted from simply retrieving relevant documents to more advanced information services that satisfy their complex working and entertainment needs. Thus, more complicated and user-centric objectives such as user satisfaction and engagement have been adopted to evaluate modern IR systems today. Those objectives, unfortunately, are difficult to be optimized under existing learning-to-rank frameworks as they are subject to great variance and complicated structures that cannot be explicitly explained or formulated with math equations like those simple performance metrics. This leads to the following research question -- how to optimize result ranking for complex ranking metrics without knowing their internal structures? To address this question, we conduct formal analysis on the limitation of existing ranking optimization techniques and describe three research tasks in Metric-agnostic Ranking Optimization: (1) develop surrogate metric models to simulate complex online ranking metrics on offline data; (2) develop differentiable ranking optimization frameworks for list or session level performance metrics without fine-grained supervision signals; and (3) develop efficient parameter exploration and exploitation techniques for ranking optimization in metric-agnostic scenarios. Through the discussion of potential solutions to these tasks, we hope to encourage more people to look into the problem of ranking optimization in complex search and recommendation scenarios.",10.1145/3539618.3591935
4f6cd084f67f0a00cb2d4f44ec9be0334cdf3c12,Marginal-Certainty-Aware Fair Ranking Algorithm,2022,"Ranking systems are ubiquitous in modern Internet services, including online marketplaces, social media, and search engines. Traditionally, ranking systems only focus on how to get better relevance estimation. When relevance estimation is available, they usually adopt a user-centric optimization strategy where ranked lists are generated by sorting items according to their estimated relevance. However, such user-centric optimization ignores the fact that item providers also draw utility from ranking systems. It has been shown in existing research that such user-centric optimization will cause much unfairness to item providers, followed by unfair opportunities and unfair economic gains for item providers. To address ranking fairness, many fair ranking methods have been proposed. However, as we show in this paper, these methods could be suboptimal as they directly rely on the relevance estimation without being aware of the uncertainty (i.e., variance of the estimated relevance). To address this uncertainty, we propose a novel Marginal-Certainty-aware Fair algorithm named MCFair. MCFair jointly optimizes fairness and user utility, while relevance estimation is constantly updated in an online manner. In MCFair, we first develop a ranking objective that includes uncertainty, fairness, and user utility. Then we directly use the gradient of the ranking objective as the ranking score. We theoretically prove that MCFair based on gradients is optimal for the aforementioned ranking objective. Empirically, we find that on semi-synthesized datasets, MCFair is effective and practical and can deliver superior performance compared to state-of-the-art fair ranking methods. To facilitate reproducibility, we release our code.",10.1145/3539597.3570474
027b9cba96e5a444c85230e44cb7a2523b694d4f,Exploration of Machine Learning to Identify Community Dwelling Older Adults with Balance Dysfunction Using Short Duration Accelerometer Data,2020,"The incidence of fall-related injuries in older adults is high. Given the significant and adverse outcomes that arise from injurious falls in older adults, it is of the utmost importance to identify older adults at greater risk for falls as early as possible. Given that balance dysfunction provides a significant risk factor for falls, an automated and objective identification of balance dysfunction in community dwelling older adults using wearable sensor data when walking may be beneficial. In this study, we examine the feasibility of using wearable sensors, when walking, to identify older adults who have trouble with balance at an early stage using state-of-the-art machine learning techniques. We recruited 21 community dwelling older women. The experimental paradigm consisted of two tasks: Normal walking with a self-selected comfortable speed on an instrumented treadmill and a test of reflexive postural response, using the motor control test (MCT). Based on the MCT, identification of older women with low or high balance function was performed. Using short duration accelerometer data from sensors placed on the knee and hip while walking, supervised machine learning was carried out to classify subjects with low and high balance function. Using a Gradient Boosting Machine (GBM) algorithm, we classified balance function in older adults using 60 seconds of accelerometer data with an average cross validation accuracy of 91.5% and area under the receiver operating characteristic curve (AUC) of 0.97. Early diagnosis of balance dysfunction in community dwelling older adults through the use of user friendly and inexpensive wearable sensors may help in reducing future fall risk in older adults through earlier interventions and treatments, and thereby significantly reduce associated healthcare costs.",10.1109/EMBC44109.2020.9175871
72aaffe51c777ba921102c822e9473b5be9876c6,Accurate fall risk classification in elderly using one gait cycle data and machine learning.,2024,,10.1016/j.clinbiomech.2024.106262
028080312edffd7946672130b75a8e5591c74aee,Self-Supervised Person Detection in 2D Range Data using a Calibrated Camera,2020,"Deep learning is the essential building block of state-of-the-art person detectors in 2D range data. However, only a few annotated datasets are available for training and testing these deep networks, potentially limiting their performance when deployed in new environments or with different LiDAR models. We propose a method, which uses bounding boxes from an image-based detector (e.g. Faster R-CNN) on a calibrated camera to automatically generate training labels (called pseudo-labels) for 2D LiDAR-based person detectors. Through experiments on the JackRabbot dataset with two detector models, DROW3 and DR-SPAAM, we show that self-supervised detectors, trained or fine-tuned with pseudolabels, outperform detectors trained only on a different dataset. Combined with robust training techniques, the self-supervised detectors reach a performance close to the ones trained using manual annotations of the target dataset. Our method is an effective way to improve person detectors during deployment without any additional labeling effort, and we release our source code to support relevant robotic applications.",10.1109/ICRA48506.2021.9561699
7b1d1394255927b8672985e2c585b7233cb7ad31,Crowd-Aware Robot Navigation with Switching Between Learning-Based and Rule-Based Methods Using Normalizing Flows,2024,"Mobile robot navigation in crowded environments with pedestrians is a crucial challenge in realizing service robots that can assist people in their daily lives. Navigation methods for mobile robots in environments employing deep reinforcement learning have been extensively studied. However, addressing such unexpected situations is a significant challenge. This study presents an approach that discerns whether a situation has been supposed to utilize a normalizing flow and dynamically switches between learning- and rule-based methods. Specifically, the proposed method achieves a higher success rate than employing only a learning-based approach and reaches the destination faster than employing only a rule-based approach in unexpected situations. Experiments are conducted to validate the performance enhancement achieved with the proposed switching method in both simulated and real-world settings.",10.1109/IROS58592.2024.10802676
0289f01d519b70816e1c2da6df1ddf22910e4a12,A Deep Convolutional Neural Network based on Local Binary Patterns of Gabor Features for Classification of Hyperspectral Images,2020,"To date, various spatial-spectral methods are proposed for accurate classification of hyperspectral images (HSI). Gabor spatial features are the most prominent ones that can extract shallow features such as edges and structures. In recent years, convolutional neural networks (CNN) have been promising in the classification of HSI. Although in literature Gabor features are used as the input of deep models, it seems that the performance of CNN can be improved by two-stage textural features based on local binary patterns of Gabor features. In this paper, input features of CNN are obtained based on local binary patterns of Gabor features which are more discriminative than both Gabor features and local binary patterns features. The experiments performed on the famous Indian Pines HIS, proved the superiority of the proposed method over some other deep learning-based methods.",10.1109/MVIP49855.2020.9187486
028a470bd6f80a7bfc1b2de009619339a7d86b72,Free the Plural: Unrestricted Split-Antecedent Anaphora Resolution,2020,"Now that the performance of coreference resolvers on the simpler forms of anaphoric reference has greatly improved, more attention is devoted to more complex aspects of anaphora. One limitation of virtually all coreference resolution models is the focus on single-antecedent anaphors. Plural anaphors with multiple antecedents-so-called split-antecedent anaphors (as in John met Mary. They went to the movies) have not been widely studied, because they are not annotated in ONTONOTES and are relatively infrequent in other corpora. In this paper, we introduce the first model for unrestricted resolution of split-antecedent anaphors. We start with a strong baseline enhanced by BERT embeddings, and show that we can substantially improve its performance by addressing the sparsity issue. To do this, we experiment with auxiliary corpora where split-antecedent anaphors were annotated by the crowd, and with transfer learning models using element-of bridging references and single-antecedent coreference as auxiliary tasks. Evaluation on the gold annotated ARRAU corpus shows that the out best model uses a combination of three auxiliary corpora achieved F1 scores of 70% and 43.6% when evaluated in a lenient and strict setting, respectively, i.e., 11 and 21 percentage points gain when compared with our baseline.",10.18653/V1/2020.COLING-MAIN.538
541a214557b95aa35c5d564cee89d38c92c2da53,What does the Failure to Reason with “Respectively” in Zero/Few-Shot Settings Tell Us about Language Models?,2023,"Humans can effortlessly understand the coordinate structure of sentences such as “Niels Bohr and Kurt Cobain were born in Copenhagen and Seattle, *respectively*”. In the context of natural language inference (NLI), we examine how language models (LMs) reason with respective readings (Gawron and Kehler, 2004) from two perspectives: syntactic-semantic and commonsense-world knowledge. We propose a controlled synthetic dataset WikiResNLI and a naturally occurring dataset NatResNLI to encompass various explicit and implicit realizations of “respectively”. We show that fine-tuned NLI models struggle with understanding such readings without explicit supervision. While few-shot learning is easy in the presence of explicit cues, longer training is required when the reading is evoked implicitly, leaving models to rely on common sense inferences. Furthermore, our fine-grained analysis indicates models fail to generalize across different constructions. To conclude, we demonstrate that LMs still lag behind humans in generalizing to the long tail of linguistic constructions.",10.48550/arXiv.2305.19597
fb1f7ffef65341aa5aee2bb0b240d4ef51680fce,Few-Shot Anaphora Resolution in Scientific Protocols via Mixtures of In-Context Experts,2022,"Anaphora resolution is an important task for information extraction across a range of languages, text genres, and domains, motivating the need for methods that do not require large annotated datasets. In-context learning has emerged as a promising approach, yet there are a number of challenges in applying in-context learning to resolve anaphora. For example, encoding a single in-context demonstration that consists of: an anaphor, a paragraph-length context, and a list of corresponding antecedents, requires conditioning a language model on a long sequence of tokens, limiting the number of demonstrations per prompt. In this paper, we present MICE (Mixtures of In-Context Experts), which we demonstrate is effective for few-shot anaphora resolution in scientific protocols (Tamari et al., 2021). Given only a handful of training examples, MICE combines the predictions of hundreds of in-context experts, yielding a 30% increase in F1 score over a competitive prompt retrieval baseline. Furthermore, we show MICE can be used to train compact student models without sacrificing performance. As far as we are aware, this is the first work to present experimental results demonstrating the effectiveness of in-context learning on the task of few-shot anaphora resolution in scientific protocols.",10.48550/arXiv.2210.03690
0294bd2e6638c9a3619d4baaa63202a3c511dccc,SplitFed: When Federated Learning Meets Split Learning,2020,"Federated learning (FL) and split learning (SL) are two popular distributed machine learning approaches. Both follow a model-to-data scenario; clients train and test machine learning models without sharing raw data. SL provides better model privacy than FL due to the machine learning model architecture split between clients and the server. Moreover, the split model makes SL a better option for resource-constrained environments. However, SL performs slower than FL due to the relay-based training across multiple clients. In this regard, this paper presents a novel approach, named splitfed learning (SFL), that amalgamates the two approaches eliminating their inherent drawbacks, along with a refined architectural configuration incorporating differential privacy and PixelDP to enhance data privacy and model robustness. Our analysis and empirical results demonstrate that (pure) SFL provides similar test accuracy and communication efficiency as SL while significantly decreasing its computation time per global epoch than in SL for multiple clients. Furthermore, as in SL, its communication efficiency over FL improves with the number of clients. Besides, the performance of SFL with privacy and robustness measures is further evaluated under extended experimental settings.",10.1609/aaai.v36i8.20825
0297627de9c8118db9a2d52562c15ea6c42c3682,Newspaper Navigator: Open Faceted Search for 1.5 Million Images,2020,"Despite the marked improvements in image classification and recognition tasks over the past decade, affordances and interfaces for searching images have remained largely unchanged on the web. In this demo, we present Newspaper Navigator, an open faceted search system for 1.5 million historic newspaper photographs. In contrast to standard faceted search, which requires facets to be pre-selected and applied to images, Newspaper Navigator empowers users to specify their own facets in an open-domain fashion during the search process by selecting relevant examples and iteratively training a machine learner. With Newspaper Navigator, users can quickly sort 1.5 million images according to dynamically-specified facets such as ""baseball player'' and ""oval-shaped portrait.'' Newspaper Navigator also drives facet exploration by suggesting related keyword search queries for a user to perform. Our demo walks through examples of searching with Newspaper Navigator and highlights the facet learning and exploration affordances.",10.1145/3379350.3416143
c3a7cb38e3e6d4a38f79927b2b239ce197c9f798,A Study of Explainability Features to Scrutinize Faceted Filtering Results,2021,"Faceted search systems enable users to filter results by selecting values along different dimensions or facets. Traditionally, facets have corresponded to properties of information items that are part of the document metadata. Recently, faceted search systems have begun to use machine learning to automatically associate documents with facet-values that are more subjective and abstract. Examples include search systems that support topic-based filtering of research articles, concept-based filtering of medical documents, and tag-based filtering of images. While machine learning can be used to infer facet-values when the collection is too large for manual annotation, machine-learned classifiers make mistakes. In such cases, it is desirable to have a scrutable system that explains why a filtered result is relevant to a facet-value. Such explanations are missing from current systems. In this paper, we investigate how explainability features can help users interpret results filtered using machine-learned facets. We consider two explainability features: (1) showing prediction confidence values and (2) highlighting rationale sentences that played an influential role in predicting a facet-value. We report on a crowdsourced study involving 200 participants. Participants were asked to scrutinize movie plot summaries predicted to satisfy multiple genres and indicate their agreement or disagreement with the system. Participants were exposed to four interface conditions. We found that both explainability features had a positive impact on participants' perceptions and performance. While both features helped, the sentence-highlighting feature played a more instrumental role in enabling participants to reject false positive cases. We discuss implications for designing tools to help users scrutinize automatically assigned facet-values.",10.1145/3459637.3482409
0298819645849ca5e2ee8b99849dbfa9502d7ac7,Learning Sequences of Approximations for Hierarchical Motion Planning,2020,"The process of designing hierarchical motion planners typically involves problem-specific intuition and implementations. This process is sub-optimal both in terms of solution space (amount of possibilities for search-space approximations, choice of planner parameters, etc) and amount of human labour. In this paper we show that the design of hierarchical motion planners does not have to be manual. We present a method for parameterizing and then optimizing sequences of problem approximations used in hierarchical motion planning. We define these as a specific kind of graph with intermediate state-spaces and solutions as nodes, and costs and planner parameters as edge properties. These properties become a continuous optimization variable that changes the sequence and parameters of sub-planners in the hierarchy. Using Pareto-front estimation, our method automatically discovers multiple designs of optimal computation-time/motion-cost trade-offs. We evaluate the method on a set of legged robot motion planning problems where hand-designed hierarchies are abundant. Our method discovers sequences of problem approximations which achieve similar—though slightly higher—performance than the best human-designed hierarchies. The performance gain significantly increases on new problems, yielding 12x faster computation times and 10% higher success rates.",10.1609/icaps.v30i1.6746
8e8f52c340f0b2041d4f37ab61790d2d1117221b,Towards providing explanations for robot motion planning,2021,"Recent research in AI ethics has put forth explainability as an essential principle for AI algorithms. However, it is still unclear how this is to be implemented in practice for specific classes of algorithms—such as motion planners. In this paper we unpack the concept of explanation in the context of motion planning, introducing a new taxonomy of kinds and purposes of explanations in this context. We focus not only on explanations of failure (previously addressed in motion planning literature) but also on contrastive explanations—which explain why a trajectory A was returned by a planner, instead of a different trajectory B expected by the user. We develop two explainable motion planners, one based on optimization, the other on sampling, which are capable of answering failure and constrastive questions. We use simulation experiments and a user study to motivate a technical and social research agenda.",10.1109/ICRA48506.2021.9562003
2e913c1f1fc6549138192fa5289c564201cdc8a7,Sparse Multilevel Roadmaps for High-Dimensional Robotic Motion Planning,2020,"Sparse roadmaps are important to compactly represent state spaces, to determine problems to be infeasible and to terminate in finite time. However, sparse roadmaps do not scale well to high-dimensional planning problems. In prior work, we showed improved planning performance on high-dimensional planning problems by using multilevel abstractions to simplify state spaces. In this work, we generalize sparse roadmaps to multilevel abstractions by developing a novel algorithm, the sparse multilevel roadmap planner (SMLR). To this end, we represent multilevel abstractions using the language of fiber bundles, and generalize sparse roadmap planners by using the concept of restriction sampling with visibility regions. We argue SMLR to be probabilistically complete and asymptotically near-optimal by inheritance from sparse roadmap planners. In evaluations, we outperform sparse roadmap planners on challenging planning problems, in particular problems which are high-dimensional, contain narrow passages or are infeasible. We thereby demonstrate sparse multilevel roadmaps as an efficient tool for feasible and infeasible high-dimensional planning problems.",10.1109/ICRA48506.2021.9562053
b3eb2569d2498b6b96c500082c100895ff52c375,Section Patterns: Efficiently Solving Narrow Passage Problems in Multilevel Motion Planning,2020,"Sampling-based planning methods often become inefficient due to narrow passages. Narrow passages induce a higher runtime, because the chance to sample them becomes vanishingly small. In recent work, we showed that narrow passages can be approached by relaxing the problem using admissible lower dimensional projections of the state space. Those relaxations often increase the volume of narrow passages under projection. Solving the relaxed problem is often efficient and produces an admissible heuristic we can exploit. However, given a base path, i.e., a solution to a relaxed problem, there are currently no tailored methods to efficiently exploit the base path. To efficiently exploit the base path and thereby its admissible heuristic, we develop section patterns, which are solution strategies to efficiently exploit base paths in particular around narrow passages. To coordinate section patterns, we develop the pattern dance algorithm, which efficiently coordinates section patterns to reactively traverse narrow passages. We combine the pattern dance algorithm with previously developed multilevel planning algorithms and benchmark them on challenging planning problems like the Bugtrap, the double L-shape, an egress problem, and on four pregrasp scenarios for a 37 degrees-of-freedom shadow hand mounted on a KUKA LWR robot. Our results confirm that section patterns are useful to efficiently solve high-dimensional narrow passage motion planning problems.",10.1109/tro.2021.3070975
029ad31df076828ad24a11039647ddbc33d02184,Utilizing Machine Learning Models to Predict the Car Crash Injury Severity among Elderly Drivers,2020,"Car crash can cause serious and severe injuries that impact people every day. Those injuries could be especially damaging for elderly drivers of age 60 or more. The goal of this research is to investigate the risk factors that contribute to crash injury severity among elderly drivers. This is accomplished by designing accurate machine learning based predictive models. Naïve Bayesian (NB), Decision Tree (DT), Logistic Regression (LR), Light-GBM, and Random Forest (RF) model are proposed. A set of influential factors are selected to build the five predictive models to classify the severity of injuries as severe injury or non-severe injury. Michigan traffic data of the elderly population is used in this paper. Data normalization and Synthetic Minority Oversampling Technique (SMOTE) as injury classes balancing technique are used in the pre-processing phase. Results show that the Light-GBM achieved the highest accuracy among the five tested models with 87%. According to the Light-GBM model, the three most important factors that impact the severity of injuries are the driver's age, traffic volume, and car's age.",10.1109/EIT48999.2020.9208259
029c31eee72ee0eb4d3058d48e276a2710f92325,Learning From Multiple Experts: Self-paced Knowledge Distillation for Long-tailed Classification,2020,"In real-world scenarios, data tends to exhibit a long-tailed distribution, which increases the difficulty of training deep networks. In this paper, we propose a novel self-paced knowledge distillation framework, termed Learning From Multiple Experts (LFME). Our method is inspired by the observation that networks trained on less imbalanced subsets of the distribution often yield better performances than their jointly-trained counterparts. We refer to these models as 'Experts', and the proposed LFME framework aggregates the knowledge from multiple 'Experts' to learn a unified student model. Specifically, the proposed framework involves two levels of adaptive learning schedules: Self-paced Expert Selection and Curriculum Instance Selection, so that the knowledge is adaptively transferred to the 'Student'. We conduct extensive experiments and demonstrate that our method is able to achieve superior performances compared to state-of-the-art methods. We also show that our method can be easily plugged into state-of-the-art long-tailed classification algorithms for further improvements.",10.1007/978-3-030-58558-7_15
28160b5586a428e2d7e3a8b9f98320cd07da42d6,A generalized framework for recognition of expiration dates on product packages using fully convolutional networks,2022,,10.1016/j.eswa.2022.117310
02ae998667e48ccd0b110dfe86f7ec7c3187356d,Physarum Powered Differentiable Linear Programming Layers and Applications,2020,"Consider a learning algorithm, which involves an internal call to an optimization routine such as a generalized eigenvalue problem, a cone programming problem or even sorting. Integrating such a method as a layer(s) within a trainable deep neural network (DNN) in an efficient and numerically stable way is not straightforward - for instance, only recently, strategies have emerged for eigendecomposition and differentiable sorting. We propose an efficient and differentiable solver for general linear programming problems which can be used in a plug and play manner within DNNs as a layer. Our development is inspired by a fascinating but not widely used link between dynamics of slime mold (physarum) and optimization schemes such as steepest descent. We describe our development and show the use of our solver in a video segmentation task and meta-learning for few-shot learning. We review the existing results and provide a technical analysis describing its applicability for our use cases. Our solver performs comparably with a customized projected gradient descent method on the first task and outperforms the differentiable CVXPY-SCS solver on the second task. Experiments show that our solver converges quickly without the need for a feasible initial point. Our proposal is easy to implement and can easily serve as layers whenever a learning procedure needs a fast approximate solution to a LP, within a larger network.",10.1609/aaai.v35i10.17081
02b11ef1f761be2f51360775c2aea4fec5946eda,Integration of Electric Vehicles in Smart Grid using Deep Reinforcement Learning,2020,"The vehicle-to-grid (V2G) technology provides an opportunity to generate revenue by selling electricity back to the grid at peak times when electricity is more expensive. Instead of sharing a contaminated pump handle at a gas station during the current covid-19 pandemic, plugging in the electric vehicle (EV) at home makes feel much safer. A V2G control algorithm is necessary to decide whether the electric vehicle (EV) should be charged or discharged in each hour. In this paper, we study the real-time V2G control problem under price uncertainty where the electricity price is determined dynamically every hour. Our model is inspired by the Deep Q-learning (DQN) algorithm which combines popular Q-learning with a deep neural network. The proposed Double-DQN model is an update of the DQN which maintains two distinct networks to select or evaluate an action. The Double-DQN algorithm is used to control charge/discharge operation in the hourly available electricity price in order to maximize the profit for the EV owner during the whole parking time. Experiment results show that our proposed method can work effectively in the real electricity market and it is able to increase the profit significantly compared with the other state-of-the-art EV charging schemes.",10.1109/IKT51791.2020.9345625
d24d6352269271aa906cb89c5c7c21cd421443d8,Hybrid Energy Storage System Optimization With Battery Charging and Swapping Coordination,2024,"Battery storage is a key technology for distributed renewable energy integration. Wider applications of battery storage systems call for smarter and more flexible deployment models to improve their economic viability. Here we propose a hybrid energy storage system (HESS) model that flexibly coordinates both portable energy storage systems (PESSs) and stationary energy storage systems (SESSs) in a grid. PESSs are batteries and power conversion systems loaded on vehicles that travel between grid nodes with price differences to alleviate grid congestion. PESSs can charge/discharge at grid nodes or swap (part of) batteries with SESSs for profit maximization. We introduce a spatiotemporal decision-making framework for HESS including the planning of SESS and the on- demand dispatch of PESS. We propose a two-phase decision-making algorithm (TPDM), where the first phase uses a spatiotemporal cost-effectiveness aggregation method to determine the optimal SESS location; the second phase shapes a low-complexity solution space by arc destroying and repairing. The results show that HESS achieves significant arbitrage benefit improvement in 86.3% of the operating periods through a year compared with SESS and PESS alone. Compared with commercial solver, the proposed TPDM, on average, can reduce the computational time by 95.5% with an optimality of 1.04%. Note to Practitioners—Battery storage and electric vehicles (EVs) play a crucial role in renewable energy integration and in shaping a low-carbon and sustainable energy and transportation systems. To achieve efficient and scalable management of battery storage across energy and transportation systems, we incorporate the portable energy storage (i.e., batteries transported by vehicles) and stationary energy storage (i.e., batteries placed at grids), into a hybrid energy storage system (HESS), and develop efficient planning framework and scheduling algorithms. Specifically, the proposed methods can provide decision supports for the owners of battery assets to determine the optimal SESS location and for the high-quality coordination of battery charging, swapping, and routing in a HESS. Our methods also have potentials in the on- demand applications of battery storage and EVs across energy and transportation systems, such as ancillary services, grid investment deferral, and battery trading and sharing.",10.1109/TASE.2023.3292189
02b14e5a132bd571bff66a234a16e6e26d052c82,Online Learning in Changing Environments,2020,"The usual goal of online learning is to minimize the regret, which measures the performance of online learner against a fixed comparator. However, it is not suitable for changing environments in which the best decision may change over time. To address this limitation, new performance measures, including dynamic regret and adaptive regret have been proposed to guide the design of online algorithms. In dynamic regret, the learner is compared with a sequence of comparators, and in adaptive regret, the learner is required to minimize the regret over every interval. In this paper, we will review the recent developments in this area, and highlight our contributions. Specifically, we have proposed novel algorithms to minimize the dynamic regret and adaptive regret, and investigated the relationship between them.",10.24963/ijcai.2020/731
bbbcb0528ee1fe814248fa25f32b62cf0e2b312a,Nonstationary online convex optimization with multiple predictions,2024,,10.1016/j.ins.2023.119862
dd2c0317dfc32db963f0a14f93a98c771ef784cd,Efficient Methods for Non-stationary Online Learning,2023,"Non-stationary online learning has drawn much attention in recent years. In particular, dynamic regret and adaptive regret are proposed as two principled performance measures for online convex optimization in non-stationary environments. To optimize them, a two-layer online ensemble is usually deployed due to the inherent uncertainty of the non-stationarity, in which a group of base-learners are maintained and a meta-algorithm is employed to track the best one on the fly. However, the two-layer structure raises the concern about the computational complexity -- those methods typically maintain $\mathcal{O}(\log T)$ base-learners simultaneously for a $T$-round online game and thus perform multiple projections onto the feasible domain per round, which becomes the computational bottleneck when the domain is complicated. In this paper, we first present efficient methods for optimizing dynamic regret and adaptive regret, which reduce the number of projections per round from $\mathcal{O}(\log T)$ to $1$. The obtained algorithms require only one gradient query and one function evaluation at each round. Our technique hinges on the reduction mechanism developed in parameter-free online learning and requires non-trivial twists on non-stationary online methods. Furthermore, we study an even strengthened measure, namely the ``interval dynamic regret'', and reduce the number of projections per round from $\mathcal{O}(\log^2 T)$ to $1$ to minimize it. Our reduction demonstrates great generalizability and can be applied to two important applications: online stochastic control and online principal component analysis, resulting in methods that are both efficient and optimal. Finally, empirical studies verify our theoretical findings.",10.48550/arXiv.2309.08911
02b5582844fe73d56dcb5ab805b28e7835ccd1b0,Searching the Web for Cross-lingual Parallel Data,2020,"While the World Wide Web provides a large amount of text in many languages, cross-lingual parallel data is more difficult to obtain. Despite its scarcity, this parallel cross-lingual data plays a crucial role in a variety of tasks in natural language processing with applications in machine translation, cross-lingual information retrieval, and document classification, as well as learning cross-lingual representations. Here, we describe the end-to-end process of searching the web for parallel cross-lingual texts. We motivate obtaining parallel text as a retrieval problem whereby the goal is to retrieve cross-lingual parallel text from a large, multilingual web-crawled corpus. We introduce techniques for searching for cross-lingual parallel data based on language, content, and other metadata. We motivate and introduce multilingual sentence embeddings as a core tool and demonstrate techniques and models that leverage them for identifying parallel documents and sentences as well as techniques for retrieving and filtering this data. We describe several large-scale datasets curated using these techniques and show how training on sentences extracted from parallel or comparable documents mined from the Web can improve machine translation models and facilitate cross-lingual NLP.",10.1145/3397271.3401417
129aad3e4d9845075acfe0105eac7a6dd344d9f8,Classification-based Quality Estimation: Small and Efficient Models for Real-world Applications,2021,"Sentence-level Quality estimation (QE) of machine translation is traditionally formulated as a regression task, and the performance of QE models is typically measured by Pearson correlation with human labels. Recent QE models have achieved previously-unseen levels of correlation with human judgments, but they rely on large multilingual contextualized language models that are computationally expensive and make them infeasible for real-world applications. In this work, we evaluate several model compression techniques for QE and find that, despite their popularity in other NLP tasks, they lead to poor performance in this regression setting. We observe that a full model parameterization is required to achieve SoTA results in a regression task. However, we argue that the level of expressiveness of a model in a continuous range is unnecessary given the downstream applications of QE, and show that reframing QE as a classification problem and evaluating QE models using classification metrics would better reflect their actual performance in real-world applications.",10.18653/v1/2021.emnlp-main.474
1d7e3bc217fe097ca39b366e98fbbe59fa6bff43,Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation,2021,"While it has been shown that Neural Machine Translation (NMT) is highly sensitive to noisy parallel training samples, prior work treats all types of mismatches between source and target as noise. As a result, it remains unclear how samples that are mostly equivalent but contain a small number of semantically divergent tokens impact NMT training. To close this gap, we analyze the impact of different types of fine-grained semantic divergences on Transformer models. We show that models trained on synthetic divergences output degenerated text more frequently and are less confident in their predictions. Based on these findings, we introduce a divergent-aware NMT framework that uses factors to help NMT recover from the degradation caused by naturally occurring divergences, improving both translation quality and model calibration on EN-FR tasks.",10.18653/v1/2021.acl-long.562
817e8a03162adbb268f08b206a5f04c5bfa97b26,Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data,2021,"The scarcity of parallel data is a major obstacle for training high-quality machine translation systems for low-resource languages. Fortunately, some low-resource languages are linguistically related or similar to high-resource languages; these related languages may share many lexical or syntactic structures. In this work, we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only monolingual data, in addition to any parallel data in the related high-resource language. Our method, NMT-Adapt, combines denoising autoencoding, back-translation and adversarial objectives to utilize monolingual data for low-resource adaptation. We experiment on 7 languages from three different language families and show that our technique significantly improves translation into low-resource language compared to other translation baselines.",10.18653/v1/2021.acl-long.66
50a22023ee96e364174a70452e2528686971f457,XLEnt: Mining a Large Cross-lingual Entity Dataset with Lexical-Semantic-Phonetic Word Alignment,2021,"Cross-lingual named-entity lexica are an important resource to multilingual NLP tasks such as machine translation and cross-lingual wikification. While knowledge bases contain a large number of entities in high-resource languages such as English and French, corresponding entities for lower-resource languages are often missing. To address this, we propose Lexical-Semantic-Phonetic Align (LSP-Align), a technique to automatically mine cross-lingual entity lexica from mined web data. We demonstrate LSP-Align outperforms baselines at extracting cross-lingual entity pairs and mine 164 million entity pairs from 120 different languages aligned with English. We release these cross-lingual entity pairs along with the massively multilingual tagged named entity corpus as a resource to the NLP community.",10.18653/v1/2021.emnlp-main.814
b4bfabb2b8dc1c41038fbf26d2d74196e6fd75bc,Quality Estimation without Human-labeled Data,2021,"Quality estimation aims to measure the quality of translated content without access to a reference translation. This is crucial for machine translation systems in real-world scenarios where high-quality translation is needed. While many approaches exist for quality estimation, they are based on supervised machine learning requiring costly human labelled data. As an alternative, we propose a technique that does not rely on examples from human-annotators and instead uses synthetic training data. We train off-the-shelf architectures for supervised quality estimation on our synthetic data and show that the resulting models achieve comparable performance to models trained on human-annotated data, both for sentence and word-level prediction.",10.18653/v1/2021.eacl-main.50
02b7b3a3250eefa3b57e06756c462a7e9d7f622f,Model for Semantic Base Structuring of Digital Data to Support Agricultural Management,2020,"This article presents a semantic model for structuring digital databases to function in a cloud environment and connect to data sources originating from Big Data. The work examines the process of receiving structured, semi-structured and unstructured data for use in agricultural risk management. It is conceived as an architecture that combines Data Mart, Data Warehouse (NoSQL), and Data Lake resources to support decision making, through knowledge discovery and applies algorithms for data mining by machine learning resources. The configuration presented addresses scenarios involving agricultural data, obtained from sensors operating in multiple modes.",10.1109/ICSC.2020.00067
9dd21d2fa34bb99d486b85508232427ea3dd8b59,Cloud and distributed architectures for data management in agriculture 4.0 : Review and future trends,2021,,10.1016/j.jksuci.2021.09.015
6015c598e4e35521bf10abe2c7574712bd5c7c8a,Ontology for Structuring a Digital Databases for Decision Making in Grain Production,2021,"This paper presents an ontology for the structuring of digital databases with the objective of acting in a cloud environment and meeting big data sources in the agricultural context of grain production. Its conception is structured in three stages: the first stage presents an ontological architecture aimed at public and private cloud environments, the second stage deals with a semantic model at process level, and a pseudocode for ontological application is elaborated in the third stage, considering the technologies applied to the cloud. This work combines advanced features to support decision making from Data Lake storage solutions, semantic treatment of big data, as well as the presentation of strategies based on machine learning and data quality analysis to obtain data and metadata organized for application in a decision model. The configuration of the ontology presented meets the diversity of big data projects in the grain production context, the characteristics of which are based on interoperability in the use of heterogeneous data and its integration, elasticity of computational resources, and high availability of cloud access.",10.1109/ICSC50631.2021.00071
fa7a5c6815f0c60994925fd2121f15050f5596f2,An Adaptive Evolutionary Neural Network Model for Load Management in Smart Grid Environment,2025,"To empower the management of smart meters’ demand load within a smart grid environment, this paper presents a Feed-forward Neural Network with ADaptive Evolutionary Learning Approach (ADELA). In this model, the load forecasting information is propagated via neurons of input and multiple hidden layers and the final estimated output is achieved with the help of the sigmoid activation function. An improved evolutionary algorithm is proposed for training and adjusting the interconnecting weights among the layers of the intended neural network. This model is capable of addressing the critical challenges of high volatility, uncertainty, missing smart meters data, and sudden upsurge and plunge in electricity demand. The proposed algorithm is able to learn the best suitable evolutionary operators from a given pool of operators and the probabilities associated with them. The proposed load forecasting approach is simulated over three real-world smart meter datasets, including the Australian Smart Grid Smart City project, the Irish Commission for Energy Regulation, and UMass Smart. The performance evaluation and comparison of the proposed approach with the existing state-of-the-art approaches revealed a relative improvement of up to 46.93%, 5.05%, and 2.20% in forecast accuracy over the Smart Grid Smart City, UMass Smart and the Irish Commission for Energy Regulation datasets, respectively.",10.1109/TNSM.2024.3470853
60d3a6237215dc40610e11694d35e0e6586fd933,Global energy use and carbon emissions from irrigated agriculture,2024,"Irrigation is a land management practice with major environmental impacts. However, global energy consumption and carbon emissions resulting from irrigation remain unknown. We assess the worldwide energy consumption and carbon emissions associated with irrigation, while also measuring the potential energy and carbon reductions achievable through the adoption of efficient and low-carbon irrigation practices. Currently, irrigation contributes 216 million metric tons of CO2 emissions and consumes 1896 petajoules of energy annually, representing 15% of greenhouse gas emissions and energy utilized in agricultural operations. Despite only 40% of irrigated agriculture relies on groundwater sources, groundwater pumping accounts for 89% of the total energy consumption in irrigation. Projections indicate that future expansion of irrigation could lead to a 28% increase in energy usage. Embracing highly efficient, low-carbon irrigation methods has the potential to cut energy consumption in half and reduce CO2 emissions by 90%. However, considering country-specific feasibility of mitigation options, global CO2 emissions may only see a 55% reduction. Our research offers comprehensive insights into the energy consumption and carbon emissions associated with irrigation, contributing valuable information that can guide assessments of the viability of irrigation in enhancing adaptive capacity within the agricultural sector.",10.1038/s41467-024-47383-5
02c541da65257ba4984e483f4afc8ceeb22b1ea4,Multi-level Bluetooth Intrusion Detection System,2020,"Large scale deployment of IoT devices has made Bluetooth Protocol (IEEE 802.15.1) the wireless protocol of choice for close-range communications. Devices such as keyboards, smartwatches, headphones, computer mouse, and various wearable connecting devices use Bluetooth network for communication. Moreover, Bluetooth networks are widely used in medical devices like heart monitors, blood glucose monitors, asthma inhalers, and pulse oximeters. Also, Bluetooth has replaced cables for wire-free equipment in a surgical environment. In hospitals, devices communicate with one another, sharing sensitive and critical information over Bluetooth scatter-networks. Thus, it is imperative to secure the Bluetooth networks against attacks like Man in the Middle attack (MITM), eavesdropping attacks, and Denial of Service (DoS) attacks. This paper presents a Multi-Level Bluetooth Intrusion Detection System (ML-BIDS) to detect malicious attacks against Bluetooth devices. In the ML-IDS framework, we perform continuous device identification and authorization in Bluetooth networks following the zero-trust principle [ref]. The ML-BIDS framework includes an anomaly-based intrusion detection system (ABIDS) to detect attacks on the Bluetooth protocol. The ABIDS tracks the normal behavior of the Bluetooth protocol by comparing it with the Bluetooth protocol state machine. Bluetooth frame flows consisting of Bluetooth frames received over 10 seconds are split into n-grams to track the current state of the protocol in the state machine. We evaluated the performance of several machine learning algorithms like C4.5, Adaboost, SVM, Naive Bayes, Jrip, and Bagging to classify normal Bluetooth protocol flows from abnormal Bluetooth protocol flows. The ABIDS detects attacks on Bluetooth protocols with a precision of up to 99.6% and recall up to 99.6%. The ML-BIDS framework also performs whitelisting of the devices on the Bluetooth network to prevent unauthorized devices from connecting to the network. ML-BIDS uses a combination of the Bluetooth Address, mac address, and IP address to uniquely identify a Bluetooth device connecting to the network, and hence ensuring only authorized devices can connect to the Bluetooth network.",10.1109/AICCSA50499.2020.9316514
650b5080c354c6f090a28b9e39c094b7d6e50dc5,OASIS: An Intrusion Detection System Embedded in Bluetooth Low Energy Controllers,2024,"Bluetooth Low Energy has established itself as one of the central protocols of the Internet of Things. Its many features (mobility, low energy consumption) make it an attractive protocol for smart devices. However, numerous critical vulnerabilities affecting BLE have been made public in recent years, some of which are linked to the protocol's design itself. The impossibility of correcting these vulnerabilities without affecting the specification requires the development of effective intrusion detection systems, enabling the detection and prevention of these threats. Unfortunately, the protocol relies on peer-to-peer communications and introduces many complex and dynamic mechanisms (e.g., channel hopping), making monitoring complex, costly and limited. Existing intrusion detection approaches lack flexibility, are limited in scope and introduce high deployment costs. In this paper, we explore a novel approach consisting in embedding an intrusion detection system directly within BLE controllers. This strategic position tackles these challenges by enabling a more advanced analysis and instrumentation of the protocol and opens the way to new defensive applications. We propose OASIS, a framework for injecting detection heuristics into controllers' firmwares in a generic way without affecting the normal operation of the protocol stack. It can be deployed in various contexts during the life cycle of a device, from the chip manufacturer to a software developer making use of proprietary components, or even in a full black box context by a security analyst to harden a commercial product. We describe its modular architecture and present its implementation within five of the most popular BLE chips from three different manufacturers, deployed in billions of devices and embedding heterogeneous protocol stacks. We present five modules for critical low-level protocol attack detection. We show that OASIS has a low impact on the controller performance (power, timing, memory) and evaluate its usage in a real-world setting.",10.1145/3634737.3645004
02c71d8fe168e2bdc9ff30074eba67eb29c48f16,Privacy-Preserving Machine Learning Using Federated Learning and Secure Aggregation,2020,"Over the past few years, machine learning has been responsible for the rapid advancements in fields such as computer vision, natural language processing and speech recognition. No small part of this success is due to data becoming more and more available, often being collected in privacy-invasive ways. The aim of this work is to study the use of two privacy-preserving solutions for training machine learning models: Federated Learning (FL) and Secure Multiparty Computation (MPC). Federated learning is a subfield of machine learning that allows training models on a large, decentralized corpus of data residing on edge devices like smartphones. Instead of sharing data, users collaboratively train a model by only sending weight updates to a server. By leveraging secure multiparty computation, it can be ensured that the server cannot inspect any individual user's update. To assess the feasibility of these approaches in different settings, a client-server architecture was implemented in Python and multiple experiments were run on datasets made available by LEAF in order to investigate ways of improving the overall performance of the models trained in a federated manner.",10.1109/ECAI50035.2020.9223127
b5ec6180d8f5ee42a6a7ce7a4f7b9a80b787d834,Information Theoretic Secure Aggregation With User Dropouts,2022,"In the robust secure aggregation problem, a server wishes to learn and only learn the sum of the inputs of a number of users while some users may drop out (i.e., may not respond). The identity of the dropped users is not known a priori and the server needs to securely recover the sum of the remaining surviving users. We consider the following minimal two-round model of secure aggregation. Over the first round, any set of no fewer than <inline-formula> <tex-math notation=""LaTeX"">$U$ </tex-math></inline-formula> users out of <inline-formula> <tex-math notation=""LaTeX"">$K$ </tex-math></inline-formula> users respond to the server and the server wants to learn the sum of the inputs of all responding users. The remaining users are viewed as dropped. Over the second round, any set of no fewer than <inline-formula> <tex-math notation=""LaTeX"">$U$ </tex-math></inline-formula> users of the surviving users respond (i.e., dropouts are still possible over the second round) and from the information obtained from the surviving users over the two rounds, the server can decode the desired sum. The security constraint is that even if the server colludes with any <inline-formula> <tex-math notation=""LaTeX"">$T$ </tex-math></inline-formula> users and the messages from the dropped users are received by the server (e.g., delayed packets), the server is not able to infer any additional information beyond the sum in the information theoretic sense. For this information theoretic secure aggregation problem, we characterize the optimal communication cost. When <inline-formula> <tex-math notation=""LaTeX"">$U \leq T$ </tex-math></inline-formula>, secure aggregation is not feasible, and when <inline-formula> <tex-math notation=""LaTeX"">$U > T$ </tex-math></inline-formula>, to securely compute one symbol of the sum, the minimum number of symbols sent from each user to the server is 1 over the first round, and <inline-formula> <tex-math notation=""LaTeX"">$1/(U-T)$ </tex-math></inline-formula> over the second round.",10.1109/TIT.2022.3192874
e644dc2e71d6c0ecbcce9ca911ab13d24033a600,Recursive Prediction Error Gradient-Based Algorithms and Framework to Identify PMSM Parameters Online,2022,"Real-time acquisition of accurate machine parameters is of significance to achieving high performance in electric drives, particularly targeted for mission-critical applications. Unlike the saturation effects, the temperature variations are difficult to predict, thus it is essential to track temperature-dependent parameters online. In this paper, a unified framework is developed for online parameter identification of rotating electric machines, premised on the Recursive Prediction Error Method (RPEM). Secondly, the prediction gradient (<inline-formula><tex-math notation=""LaTeX"">$\boldsymbol{\Psi }^{T}$</tex-math></inline-formula>)-based RPEM is adopted for identification of the temperature-sensitive parameters, i.e., the permanent magnet flux linkage (<inline-formula><tex-math notation=""LaTeX"">$\Psi _{m}$</tex-math></inline-formula>) and stator-winding resistance (<inline-formula><tex-math notation=""LaTeX"">$R_{s}$</tex-math></inline-formula>) of the Interior Permanent Magnet Synchronous Machine (IPMSM). Three algorithms, namely, Stochastic Gradient (SGA), Gauss-Newton (GNA), and physically interpretative method (PhyInt) are investigated for the estimation gains computation. A speed-dependent gain-scheduling scheme is used to decouple the inter-dependency of <inline-formula><tex-math notation=""LaTeX"">$\Psi _{m}$</tex-math></inline-formula> and <inline-formula><tex-math notation=""LaTeX"">$R_{s}$</tex-math></inline-formula>. With the aid of offline simulation methods, the main elements of RPEM such as <inline-formula><tex-math notation=""LaTeX"">$\boldsymbol{\Psi }^{T}$</tex-math></inline-formula> are analyzed. The concept validation and the choice of the optimal algorithm is made with the use of System-on-Chip (SoC) based Embedded Real-Time Simulator (ERTS). Subsequently, the selected algorithms are validated with the aid of a 3-kW, IPMSM drive where the control and estimation routines are implemented in the SoC-based industrial embedded control system. The experimental results reveal that <inline-formula><tex-math notation=""LaTeX"">$\boldsymbol{\Psi }^{T}$</tex-math></inline-formula>-based RPEM, in general, can be a versatile technique in temperature-sensitive parameter adaptation both online and offline.",10.1109/TIA.2022.3219041
02cce36fc58fd9a8682877435689eeef5c1a29fa,Knowledge Guided Metric Learning for Few-Shot Text Classification,2020,"Humans can distinguish new categories very efficiently with few examples, largely due to the fact that human beings can leverage knowledge obtained from relevant tasks. However, deep learning based text classification model tends to struggle to achieve satisfactory performance when labeled data are scarce. Inspired by human intelligence, we propose to introduce external knowledge into few-shot learning to imitate human knowledge. A novel parameter generator network is investigated to this end, which is able to use the external knowledge to generate different metrics for different tasks. Armed with this network, similar tasks can use similar metrics while different tasks use different metrics. Through experiments, we demonstrate that our method outperforms the SoTA few-shot text classification models.",10.18653/V1/2021.NAACL-MAIN.261
4fadcddf23b8b868b8d9cdc27f75ad65c8c26ce8,A fine-grained self-adapting prompt learning approach for few-shot learning with pre-trained language models,2024,,10.1016/j.knosys.2024.111968
9e0ba91fb52cacde9424e6b5b4d27cdce00af90a,Diagnosis of Multiple Faults in Rotating Machinery Using Ensemble Learning,2023,"Fault diagnosis of rotating machines is an important task to prevent machinery downtime, and provide verifiable support for condition-based maintenance (CBM) decision-making. Deep learning-enabled fault diagnosis operations have become increasingly popular because features are extracted and selected automatically. However, it is challenging for these models to give superior results with rotating machine components of different scales, single and multiple faults across different rotating components, diverse operating speeds, and diverse load conditions. To address these challenges, this paper proposes a comprehensive learning approach with optimized signal processing transforms for single as well as multiple faults diagnosis across dissimilar rotating machine components: gearbox, bearing, and shaft. The optimized bicoherence, spectral kurtosis and cyclic spectral coherence feature spaces, and deep blending ensemble learning are explored for multiple faults diagnosis of these components. The performance analysis of the proposed approach has been demonstrated through a single joint training of the entire framework on a compound dataset containing multiple faults derived from three public repositories. A comparison with the state-of-the-art approaches that used these datasets, shows that our method gives improved results with different components and faults with nominal retraining.",10.3390/s23021005
02dedd1b2b5e70b72dab3ee0a63976d4b839cb37,HGCN: A Heterogeneous Graph Convolutional Network-Based Deep Learning Model Toward Collective Classification,2020,"Collective classification, as an important technique to study networked data, aims to exploit the label autocorrelation for a group of inter-connected entities with complex dependencies. As the emergence of various heterogeneous information networks (HINs), collective classification at present is confronting several severe challenges stemming from the heterogeneity of HINs, such as complex relational hierarchy, potential incompatible semantics and node-context relational semantics. To address the challenges, in this paper, we propose a novel heterogeneous graph convolutional network-based deep learning model, called HGCN, to collectively categorize the entities in HINs. Our work involves three primary contributions: i) HGCN not only learns the latent relations from the relation-sophisticated HINs via multi-layer heterogeneous convolutions, but also captures the semantic incompatibility among relations with properly-learned edge-level filter parameters; ii) to preserve the fine-grained relational semantics of different-type nodes, we propose a heterogeneous graph convolution to directly tackle the original HINs without any in advance transforming the network from heterogeneity to homogeneity; iii) we perform extensive experiments using four real-world datasets to validate our proposed HGCN, the multi-facet results show that our proposed HGCN can significantly improve the performance of collective classification compared with the state-of-the-art baseline methods.",10.1145/3394486.3403169
f48c244d1635ac28a6ed26c93755121643a22e75,Harnessing Heterogeneous Information Networks: A systematic literature review,2024,,10.1016/j.cosrev.2024.100633
02df9479659d73bf83aea98b5e8cb0a93f3b6ac3,Wasserstein Exponential Kernels,2020,"In the context of kernel methods, the similarity between data points is encoded by the kernel function which is often defined thanks to the Euclidean distance; the squared exponential kernel is a common example. Recently, other distances relying on optimal transport theory – such as the Wasserstein distance between probability distributions – have shown their practical relevance for different machine learning techniques. In this paper, we study the use of exponential kernels defined thanks to the regularized Wasserstein distance and discuss their positive definiteness. More specifically, we define Wasserstein feature maps and illustrate their interest for supervised learning problems involving shapes and images. Empirically, Wasserstein squared exponential kernels are shown to yield smaller classification errors on small training sets of shapes, compared to analogous classifiers using Euclidean distances.",10.1109/IJCNN48605.2020.9207630
d8fd7395630504d5ad0e2171d0c615acbd24ff25,Unbalanced Optimal Transport: A Unified Framework for Object Detection,2023,"During training, supervised object detection tries to correctly match the predicted bounding boxes and associated classification scores to the ground truth. This is essential to determine which predictions are to be pushed towards which solutions, or to be discarded. Popular matching strategies include matching to the closest ground truth box (mostly used in combination with anchors), or matching via the Hungarian algorithm (mostly used in anchor free methods). Each of these strategies comes with its own properties, underlying losses, and heuristics. We show how Unbalanced Optimal Transport unifies these different approaches and opens a whole continuum of methods in between. This allows for a finer selection of the desired properties. Experimentally, we show that training an object detection model with Unbalanced Optimal Transport is able to reach the state-of-the-art both in terms of Average Precision and Average Recall as well as to provide a faster initial convergence. The approach is well suited for GPU implementation, which proves to be an advantage for large-scale models.",10.1109/CVPR52729.2023.00312
fa69f78a021f9f71363568de4a30e1603ea6fcf3,dtwParallel: A Python package to efficiently compute dynamic time warping between time series,2023,,10.1016/j.softx.2023.101364
cc905b2fd849c1b4ba2d7171b9ba2a393fdde9ee,Gaussian Process regression over discrete probability measures: on the non-stationarity relation between Euclidean and Wasserstein Squared Exponential Kernels,2022,"Gaussian Process regression is a kernel method successfully adopted in many real-life applications. Recently, there is a growing interest on extending this method to non-Euclidean input spaces, like the one considered in this paper, consisting of probability measures. Although a Positive Definite kernel can be defined by using a suitable distance—the Wasserstein distance— the common procedure for learning the Gaussian Process model can fail due to numerical issues, arising earlier and more frequently than in the case of an Euclidean input space and, as demonstrated, impossible to avoid by adding artificial noise (nugget effect) as usually done. This paper uncovers the main reason of these issues, that is a non-stationarity relation between the Wasserstein-based squared exponential kernel and its Euclidean counterpart. As a relevant result, we learn a Gaussian Process model by assuming the input space as Euclidean and then use an algebraic transformation, based on the uncovered relation, to transform it into a non-stationary and Wasserstein-based Gaussian Process model over probability measures. This algebraic transformation is simpler than log-exp maps used on data belonging to Riemannian manifolds and recently extended to consider the pseudo-Riemannian structure of an input space equipped with the Wasserstein distance.",10.48550/arXiv.2212.01310
ccf5ce2110f723e8d147392206e7d33677cb8320,Ollivier-Ricci Curvature for Hypergraphs: A Unified Framework,2022,"Bridging geometry and topology, curvature is a powerful and expressive invariant. While the utility of curvature has been theoretically and empirically confirmed in the context of manifolds and graphs, its generalization to the emerging domain of hypergraphs has remained largely unexplored. On graphs, the Ollivier-Ricci curvature measures differences between random walks via Wasserstein distances, thus grounding a geometric concept in ideas from probability theory and optimal transport. We develop ORCHID, a flexible framework generalizing Ollivier-Ricci curvature to hypergraphs, and prove that the resulting curvatures have favorable theoretical properties. Through extensive experiments on synthetic and real-world hypergraphs from different domains, we demonstrate that ORCHID curvatures are both scalable and useful to perform a variety of hypergraph tasks in practice.",10.48550/arXiv.2210.12048
02e906841452b9ed78a836ec15001e026688c993,Lifelong Language Knowledge Distillation,2020,"It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts. To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation. Specifically, when the LLL model is trained on a new task, we assign a teacher model to first learn the new task, and pass the knowledge to the LLL model via knowledge distillation. Therefore, the LLL model can better adapt to the new task while keeping the previously learned knowledge. Experiments show that the proposed L2KD consistently improves previous state-of-the-art models, and the degradation comparing to multi-task models in LLL tasks is well mitigated for both sequence generation and text classification tasks.",10.18653/v1/2020.emnlp-main.233
02ebd98c7e5e6fe53f64363c7bb7fa6ef15b47c0,The Pod People: Understanding Manipulation of Social Media Popularity via Reciprocity Abuse,2020,"Online Social Network (OSN) Users’ demand to increase their account popularity has driven the creation of an underground ecosystem that provides services or techniques to help users manipulate content curation algorithms. One method of subversion that has recently emerged occurs when users form groups, called pods, to facilitate reciprocity abuse, where each member reciprocally interacts with content posted by other members of the group. We collect 1.8 million Instagram posts that were posted in pods hosted on Telegram. We first summarize the properties of these pods and how they are used, uncovering that they are easily discoverable by Google search and have a low barrier to entry. We then create two machine learning models for detecting Instagram posts that have gained interaction through two different kinds of pods, achieving 0.91 and 0.94 AUC, respectively. Finally, we find that pods are effective tools for increasing users’ Instagram popularity, we estimate that pod utilization leads to a significantly increased level of likely organic comment interaction on users’ subsequent posts.",10.1145/3366423.3380256
35f18d7535abfb5ac2d3e1d1a65d9e08ec0706ab,#TeamFollowBack: Detection & Analysis of Follow Back Accounts on Social Media,2024,"Follow back accounts inflate their follower counts by engaging in reciprocal followings. Such accounts manipulate the public and the algorithms by appearing more popular than they really are. Despite their potential harm, no studies have analyzed such accounts at scale. In this study, we present the first large-scale analysis of follow back accounts. We formally define follow back accounts and employ a honeypot approach to collect a dataset of such accounts on X (formerly Twitter). We discover and describe 12 communities of follow back accounts from 12 different countries, some of which exhibit clear political agenda. We analyze the characteristics of follow back accounts and report that they are newer, more engaging, and have more followings and followers. Finally, we propose a classifier for such accounts and report that models employing profile metadata and the ego network have some success, although achieving high recall is challenging. Our study enhances understanding of the follow back accounts and discovering such accounts in the wild.",10.48550/arXiv.2403.15856
02ec037ce6e6e6009ccfaa7087e54835b3ddbfac,3D Object Detection and Pose Estimation of Unseen Objects in Color Images with Local Surface Embeddings,2020,"We present an approach for detecting and estimating the 3D poses of objects in images that requires only an untextured CAD model and no training phase for new objects. Our approach combines Deep Learning and 3D geometry: It relies on an embedding of local 3D geometry to match the CAD models to the input images. For points at the surface of objects, this embedding can be computed directly from the CAD model; for image locations, we learn to predict it from the image itself. This establishes correspondences between 3D points on the CAD model and 2D locations of the input images. However, many of these correspondences are ambiguous as many points may have similar local geometries. We show that we can use Mask-RCNN in a class-agnostic way to detect the new objects without retraining and thus drastically limit the number of possible correspondences. We can then robustly estimate a 3D pose from these discriminative correspondences using a RANSAC-like algorithm. We demonstrate the performance of this approach on the T-LESS dataset, by using a small number of objects to learn the embedding and testing it on the other objects. Our experiments show that our method is on par or better than previous methods.",10.1007/978-3-030-69525-5_3
16e34dcb80d4a2541774f16187a1218b3571aa2e,FoundPose: Unseen Object Pose Estimation with Foundation Features,2023,"We propose FoundPose, a model-based method for 6D pose estimation of unseen objects from a single RGB image. The method can quickly onboard new objects using their 3D models without requiring any object- or task-specific training. In contrast, existing methods typically pre-train on large-scale, task-specific datasets in order to generalize to new objects and to bridge the image-to-model domain gap. We demonstrate that such generalization capabilities can be observed in a recent vision foundation model trained in a self-supervised manner. Specifically, our method estimates the object pose from image-to-model 2D-3D correspondences, which are established by matching patch descriptors from the recent DINOv2 model between the image and pre-rendered object templates. We find that reliable correspondences can be established by kNN matching of patch descriptors from an intermediate DINOv2 layer. Such descriptors carry stronger positional information than descriptors from the last layer, and we show their importance when semantic information is ambiguous due to object symmetries or a lack of texture. To avoid establishing correspondences against all object templates, we develop an efficient template retrieval approach that integrates the patch descriptors into the bag-of-words representation and can promptly propose a handful of similarly looking templates. Additionally, we apply featuremetric alignment to compensate for discrepancies in the 2D-3D correspondences caused by coarse patch sampling. The resulting method noticeably outperforms existing RGB methods for refinement-free pose estimation on the standard BOP benchmark with seven diverse datasets and can be seamlessly combined with an existing render-and-compare refinement method to achieve RGB-only state-of-the-art results. Project page: evinpinar.github.io/foundpose.",10.48550/arXiv.2311.18809
eab9fc459bbd7ab4c2d4dff7137057ea78246c4e,GigaPose: Fast and Robust Novel Object Pose Estimation via One Correspondence,2023,"We present GigaPose, afast, robust, and accurate method for CAD-based novel object pose estimation in RGB images. GigaPose first leverages discriminative “templates ”, ren-dered images of the CAD models, to recover the out-of-plane rotation and then uses patch correspondences to estimate the four remaining parameters. Our approach samples tem-plates in only a two-degrees-of-freedom space instead of the usual three and matches the input image to the templates using fast nearest-neighbor search in feature space, results in a speedup factor of 35x compared to the state of the art. More-over, GigaPose is significantly more robust to segmentation errors. Our extensive evaluation on the seven core datasets of the BOP challenge demonstrates that it achieves state-of-the-art accuracy and can be seamlessly integrated with existing refinement methods. Additionally, we show the potential of GigaPose with 3D models predicted by recent work on 3D reconstruction from a single image, relaxing the need for CAD models and making 6D pose object estimation much more convenient. Our source code and trained models are publicly available at https://github.conllnv-nguyenlgigaPose.",10.1109/CVPR52733.2024.00945
02f0428e684ecbfd202f7497bd81ea60f116dd19,On the Importance of Word and Sentence Representation Learning in Implicit Discourse Relation Classification,2020,"Implicit discourse relation classification is one of the most difficult parts in shallow discourse parsing as the relation prediction without explicit connectives requires the language understanding at both the text span level and the sentence level. Previous studies mainly focus on the interactions between two arguments. We argue that a powerful contextualized representation module, a bilateral multi-perspective matching module, and a global information fusion module are all important to implicit discourse analysis. We propose a novel model to combine these modules together. Extensive experiments show that our proposed model outperforms BERT and other state-of-the-art systems on the PDTB dataset by around 8% and CoNLL 2016 datasets around 16%. We also analyze the effectiveness of different modules in the implicit discourse relation classification task and demonstrate how different levels of representation learning can affect the results.",10.24963/ijcai.2020/530
02f3ced09497c5db59985b2a5db9d3d0aebe5074,Sound2Sight: Generating Visual Dynamics from Sound and Context,2020,"Learning associations across modalities is critical for robust multimodal reasoning, especially when a modality may be missing during inference. In this paper, we study this problem in the context of audio-conditioned visual synthesis -- a task that is important, for example, in occlusion reasoning. Specifically, our goal is to generate future video frames and their motion dynamics conditioned on audio and a few past frames. To tackle this problem, we present Sound2Sight, a deep variational framework, that is trained to learn a per frame stochastic prior conditioned on a joint embedding of audio and past frames. This embedding is learned via a multi-head attention-based audio-visual transformer encoder. The learned prior is then sampled to further condition a video forecasting module to generate future frames. The stochastic prior allows the model to sample multiple plausible futures that are consistent with the provided audio and the past context. Moreover, to improve the quality and coherence of the generated frames, we propose a multimodal discriminator that differentiates between a synthesized and a real audio-visual clip. We empirically evaluate our approach, vis-a-vis closely-related prior methods, on two new datasets viz. (i) Multimodal Stochastic Moving MNIST with a Surprise Obstacle, (ii) Youtube Paintings; as well as on the existing Audio-Set Drums dataset. Our extensive experiments demonstrate that Sound2Sight significantly outperforms the state of the art in the generated video quality, while also producing diverse video content.",10.1007/978-3-030-58583-9_42
c0f453e22daef3d820a891c36afac71446b493fa,MM-LDM: Multi-Modal Latent Diffusion Model for Sounding Video Generation,2024,"Sounding Video Generation (SVG) is an audio-video joint generation task challenged by high-dimensional signal spaces, distinct data formats, and different patterns of content information. To address these issues, we introduce a novel multi-modal latent diffusion model (MM-LDM) for the SVG task. We first unify the representation of audio and video data by converting them into a single or a couple of images. Then, we introduce a hierarchical multi-modal autoencoder that constructs a low-level perceptual latent space for each modality and a shared high-level semantic feature space. The former space is perceptually equivalent to the raw signal space of each modality but drastically reduces signal dimensions. The latter space serves to bridge the information gap between modalities and provides more insightful cross-modal guidance. Our proposed method achieves new state-of-the-art results with significant quality and efficiency gains. Specifically, our method achieves a comprehensive improvement on all evaluation metrics and a faster training and sampling speed on Landscape and AIST++ datasets. Moreover, we explore its performance on open-domain sounding video generation, long sounding video generation, audio continuation, video continuation, and conditional single-modal generation tasks for a comprehensive evaluation, where our MM-LDM demonstrates exciting adaptability and generalization ability.",10.1145/3664647.3680889
02f9d4c2a1b29d0cb0a436bddffc6fd43e38973e,Learning to Ask Screening Questions for Job Postings,2020,"At LinkedIn, we want to create economic opportunity for everyone in the global workforce. A critical aspect of this goal is matching jobs with qualified applicants. To improve hiring efficiency and reduce the need to manually screening each applicant, we develop a new product where recruiters can ask screening questions online so that they can filter qualified candidates easily. To add screening questions to all 20M active jobs at Linked In, we propose a new task that aims to automatically generate screening questions for a given job posting. To solve the task of generating screening questions, we develop a two-stage deep learning model called Job2Questions, where we apply a deep learning model to detect intent from the text description, and then rank the detected intents by their importance based on other contextual features. Since this is a new product with no historical data, we employ deep transfer learning to train complex models with limited training data. We launched the screening question product and our AI models to LinkedIn users and observed significant impact in the job marketplace. During our online A/B test, we observed +53.10% screening question suggestion acceptance rate, +22.17% job coverage, +190% recruiter-applicant interaction, and +11 Net Promoter Score. In sum, the deployed Job2Questions model helps recruiters to find qualified applicants and job seekers to find jobs they are qualified for.",10.1145/3397271.3401118
f90b6ce61debef4151a48979952b32ca76cd4955,OpenResume: Advancing Career Trajectory Modeling with Anonymized and Synthetic Resume Datasets,2024,"Despite substantial advancements in various fields of AI, computational research in career and job domains has been significantly hindered by a critical lack of accessible datasets. This limitation is mainly due to the proprietary nature of job platforms, which restrict the sharing of job-domain datasets with the research community. The scarcity is particularly pronounced for career trajectory and resume datasets, severely constraining academic researchers in developing and evaluating new models. In this paper, we address the crucial issue of resume dataset unavailability in the job domain, identified through our comprehensive comparison of existing job-domain machine learning studies. To the best of our knowledge, we introduce OpenResume, the first publicly available, anonymized, and structured resume dataset, specifically designed for job-domain downstream tasks. This dataset aims to catalyze advancements in AI and foster new markets for machine learning and data science within career trajectory modeling. OpenResume is comprehensively processed from real-world resume data. We anonymize and substitute personal identifiers and company names, normalize job titles into ESCO-based ones (i.e., one of the most common occupation taxonomies), and employ differential privacy techniques on temporal features to ensure open accessibility and privacy protection. Additionally, we augment OpenResume with a synthetically generated resume dataset derived from the post-processed real-world data, extending its diversity and utility. To demonstrate that OpenResume retains challenges and properties similar to real-world job datasets, we benchmark OpenResume on state-of-the-art job-domain prediction models across four prevalent downstream tasks: (1) next job title prediction, (2) next company prediction, (3) turnover prediction, and (4) link prediction. Our experimental results show that these job-domain models perform comparably on OpenResume and the original data across all tasks, demonstrating OpenResume as a valuable career trajectory dataset for both academic research and practical applications. We also indicate the OpenResume applicability for the other eight downstream tasks. Our datasets are available at: https://tinyurl.com/OpenResumeData.",10.1109/BigData62323.2024.10825519
0300342dfa77b5ae48803a4c4096af08e60ce608,MP-Boost: Minipatch Boosting via Adaptive Feature and Observation Sampling,2020,"Boosting methods are among the best generalpurpose and off-the-shelf machine learning approaches, gaining widespread popularity. In this paper, we seek to develop a boosting method that yields comparable accuracy to popular AdaBoost and gradient boosting methods, yet is faster computationally and whose solution is more interpretable. We achieve this by developing MP-Boost, an algorithm loosely based on AdaBoost that learns by adaptively selecting small subsets of instances and features, or what we term minipatches (MP), at each iteration. By sequentially learning on tiny subsets of the data, our approach is computationally faster than classic boosting algorithms. MP-Boost upweights important features and challenging instances, hence adaptively selects the most relevant minipatches for learning. The learned probability distributions aid in interpretation of our method. We empirically demonstrate the interpretability and comparative accuracy of our algorithm on a variety of binary classification tasks.",10.1109/BigComp51126.2021.00023
03031d20494b9634b27fc5be6bce203a87383343,Generate Your Counterfactuals: Towards Controlled Counterfactual Generation for Text,2020,"Machine Learning has seen tremendous growth recently, which has led to a larger adaptation of ML systems for educational assessments, credit risk, healthcare, employment, criminal justice, to name a few. The trustworthiness of ML and NLP systems is a crucial aspect and requires a guarantee that the decisions they make are fair and robust. Aligned with this, we propose a novel framework GYC, to generate a set of exhaustive counterfactual text, which are crucial for testing these ML systems. Our main contributions include a) We introduce GYC, a framework to generate counterfactual samples such that the generation is plausible, diverse, goal-oriented, and effective, b) We generate counterfactual samples, that can direct the generation towards a corresponding \texttt{condition} such as named-entity tag, semantic role label, or sentiment. Our experimental results on various domains show that GYC generates counterfactual text samples exhibiting the above four properties. GYC generates counterfactuals that can act as test cases to evaluate a model and any text debiasing algorithm.",10.1609/aaai.v35i15.17594
051563de0a9d7fe4138b5ed08abdd9ec8f5a7719,Educational data mining to predict students' academic performance: A survey study,2022,,10.1007/s10639-022-11152-y
030a77e9fe7c9dcab005f60cd1041567a32aadfa,Learning Constellation Map with Deep CNN for Accurate Modulation Recognition,2020,"Modulation classification, recognized as the intermediate step between signal detection and demodulation, is widely deployed in several modern wireless communication systems. Although many approaches have been studied in the last decades for identifying the modulation format of an incoming signal, they often reveal the obstacle of learning radio characteristics for most traditional machine learning algorithms. To overcome this drawback, we propose an accurate modulation classification method by exploiting deep learning for being compatible with constellation diagram. Particularly, a convolutional neural network is developed for proficiently learning the most relevant radio characteristics of gray-scale constellation image. The deep network is specified by multiple processing blocks, where several grouped and asymmetric convolutional layers in each block are organized by a flow-in-flow structure for feature enrichment. These blocks are connected via skip-connection to prevent the vanishing gradient problem while effectively preserving the information identity throughout the network. Regarding several intensive simulations on the constellation image dataset of eight digital modulations, the proposed deep network achieves the remarkable classification accuracy of approximately 87% at 0 dB signal-to-noise ratio (SNR) under a multipath Rayleigh fading channel and further outperforms some state-of-the-art deep models of constellation-based modulation classification.",10.1109/GLOBECOM42002.2020.9348129
24bc665c8046ee7becfc0cfb7ab1fc8b270b1ea0,Achieving Efficient Feature Representation for Modulation Signal: A Cooperative Contrast Learning Approach,2024,"Seamless Internet of Things (IoT) connections expose many vulnerabilities in wireless networks, and IoT devices inevitably face many malicious active attacks. automatic modulation recognition (AMR) is an effective way to combat IoT physical layer threats. In the field of noncollaborative communication, feature representation learning for unlabeled signals is an important task of AMR. However, due to the unavailability of a priori knowledge and the influence of interference during signal transmission, the intercepted unlabeled signals are difficult to perform efficient feature representation. In this article, we propose cooperative contrast learning for unlabeled modulation signal Cooperative Contrast Learning for modulation Signals (CoCL-Sig). Specifically, the CoCL-Sig is trained using both sequence and constellation diagram modalities, and is divided into two parts: 1) modal-level feature representation and 2) instance-level auxiliary feature representation. In modal-level feature representation, two modal projections are matched in the same hyperplane space. To ensure the stability of the feature representation, a sequence auxiliary branch is added to form an instance-level feature representation of the sequence. In addition, the feature representations obtained by the CoCL-Sig can be applied to modulation signals for semi-supervised classification and clustering tasks. We have conducted extensive experiments on two widely used modulation signal data sets, RML2016.10A and RML2016.04C. The results demonstrate the effectiveness of our method in modulation signal feature representation and its superiority compared to other methods.",10.1109/JIOT.2024.3350927
969ca1a9b5003ed37f4634335a31e961df7c3a70,Early prediction of grape disease attack using a hybrid classifier in association with IoT sensors,2024,,10.1016/j.heliyon.2024.e38093
c82507ea8de02e07f9f1d2c156a97b6cb61ce82d,Robust diagnosis and meta visualizations of plant diseases through deep neural architecture with explainable AI,2024,"Deep learning has emerged as a highly effective and precise method for classifying images. The presence of plant diseases poses a significant threat to food security. However, accurately identifying these diseases in plants is challenging due to limited infrastructure and techniques. Fortunately, the recent advancements in deep learning within the field of computer vision have opened up new possibilities for diagnosing plant pathology. Detecting plant diseases at an early stage is crucial, and this research paper proposes a deep convolutional neural network model that can rapidly and accurately identify plant diseases. Given the minimal variation in image texture and color, deep learning techniques are essential for robust recognition. In this study, we introduce a deep, explainable neural architecture specifically designed for recognizing plant diseases. Fine-tuned deep convolutional neural network is designed by freezing the layers and adjusting the weights of learnable layers. By extracting deep features from a down sampled feature map of a fine-tuned neural network, we are able to classify these features using a customized K-Nearest Neighbors Algorithm. To train and validate our model, we utilize the largest standard plant village dataset, which consists of 38 classes. To evaluate the performance of our proposed system, we estimate specificity, sensitivity, accuracy, and AUC. The results demonstrate that our system achieves an impressive maximum validation accuracy of 99.95% and an AUC of 1, making it the most ideal and highest-performing approach compared to current state-of-the-art deep learning methods for automatically identifying plant diseases.",10.1038/s41598-024-64601-8
32244952ca2df170d6609d950d10a1576a2c73cd,A review on machine learning and deep learning image-based plant disease classification for industrial farming systems,2024,,10.1016/j.jii.2024.100572
030ce81c88d33b269f0a3a8a67f44c3e7ee647b2,Age-Based Coded Computation for Bias Reduction in Distributed Learning,2020,"Coded computation can speed up distributed learning in the presence of straggling workers. Partial recovery of the gradient vector can further reduce the computation time at each iteration; however, this can result in biased estimators, which may slow down convergence, or even cause divergence. Estimator bias is particularly prevalent when the straggling behavior is correlated over time, which results in the gradient estimators being dominated by a few fast servers. To mitigate biased estimators, we design a timely dynamic encoding framework for partial recovery that includes an ordering operator that changes the codewords and computation orders at workers over time. To regulate the recovery frequencies, we adopt an age metric in the design of the dynamic encoding scheme. The proposed age-based scheme prioritizes the recovery of computations with relatively large age. We show through numerical results that the proposed dynamic encoding strategy increases the timeliness of the recovered computations, which, as a result, reduces the bias in model updates, and accelerates the convergence compared to conventional static partial recovery schemes.",10.1109/GLOBECOM42002.2020.9322412
031456f7f93792054adbbde3e5eab254885860ed,Linear Distributed Clustering Algorithm for Modular Robots Based Programmable Matter,2020,"Modular robots are defined as autonomous kinematic machines with variable morphology. They are composed of several thousands or even millions of modules which are able to coordinate in order to behave intelligently. Clustering the modules in modular robots has many benefits, including scalability, energy-efficiency, reducing communication delay and improving the self-configuration processes that focuses on finding a sequence of reconfiguration actions to convert robots from an initial configuration to a goal one. The main idea is to divide the nodes in an initial shape into some clusters based on the final goal shape in order to reduce the time complexity and enhance the self-reconfiguration tasks. In this paper, we propose a robust clustering approach based on a distributed density-cut graph algorithm to divide the networks into a pre-defined number of clusters based on the final goal shape. The result is an algorithm with linear complexity that scales to large modular robot systems. We implement and demonstrate our algorithm on a real Blinky Blocks system and evaluate it in simulation on networks of up to 30,000 modules.",10.1109/IROS45743.2020.9341032
031bbb302d3ab6da299cb06a0c7c699b9c699610,Segmentation of medical images for the extraction of brain tumors: A comparative study between the Hidden Markov and Deep Learning approaches,2020,"Malignant brain tumors are one of the leading causes of death in adults and children. To identify a brain tumor, an MRI image is acquired and analyzed manually by an expert to find lesions. This procedure takes time and the intra and inter expert variations for the same case vary a lot. To overcome these problems, many automatic and semi-automatic methods have been proposed in recent years to help practitioners make decisions. The advent of Deep Learning methods and their success in many applications such as image classification has helped to promote Deep Learning in the analysis of medical images. In this paper, we will present two methods for the detection of brain tumors in medical images. The first is based on Deep Learning through the U-net architecture that has proven its robustness vis-vis the segmentation of images, especially medical images. The results obtained will be compared by a second method that we have published in another article [1], which uses LBP and k-means techniques. The classes found are improved using the Markov method, by calculating the class correlation. The comparison was made on the same BraTS2019 dataset [2], which will give us an idea of the performance of each.",10.1109/ISCV49265.2020.9204319
78e129b7685d723385de112fc5e721634e6ba0aa,Brain tumor segmentation in MRI with multi-modality spatial information enhancement and boundary shape correction,2024,,10.1016/j.patcog.2024.110553
03244f2b7f82407e476ccf080a8656bbad51f57f,Deep-Edge: An Efficient Framework for Deep Learning Model Update on Heterogeneous Edge,2020,"Deep Learning (DL) model-based AI services are increasingly offered in a variety of predictive analytics services such as computer vision, natural language processing, speech recognition. However, the quality of the DL models can degrade over time due to changes in the input data distribution, thereby requiring periodic model updates. Although cloud data-centers can meet the computational requirements of the resource-intensive and time-consuming model update task, transferring data from the edge devices to the cloud incurs a significant cost in terms of network bandwidth and are prone to data privacy issues. With the advent of GPU-enabled edge devices, the DL model update can be performed at the edge in a distributed manner using multiple connected edge devices. However, efficiently utilizing the edge resources for the model update is a hard problem due to the heterogeneity among the edge devices and the resource interference caused by the colocation of the DL model update task with latency-critical tasks running in the background. To overcome these challenges, we present Deep-Edge, a load- and interference-aware, fault-tolerant resource management framework for performing model update at the edge that uses distributed training. This paper makes the following contributions. First, it provides a unified framework for monitoring, profiling, and deploying the DL model update tasks on heterogeneous edge devices. Second, it presents a scheduler that reduces the total re-training time by appropriately selecting the edge devices and distributing data among them such that no latency-critical applications experience deadline violations. Finally, we present empirical results to validate the efficacy of the framework using a real-world DL model update case-study based on the Caltech dataset and an edge AI cluster testbed.",10.1109/ICFEC50348.2020.00016
bc8a557c950e2bbc35d5c58641bc975fbec93021,Edge AI for Internet of Medical Things: A literature review,2024,,10.1016/j.compeleceng.2024.109202
dc8f507fc2fda1dad396f5fb11876e93fa35637f,EdgeMesh: A hybrid distributed training mechanism for heterogeneous edge devices,2022,"The proliferation of large‐scale distributed Internet of Things (IoT) applications has resulted in a surge in demand for network models such as deep neural networks (DNNs) to be trained and inferred at the edge. Due to the central data transmission mechanism, heterogeneity of edge devices, and resource constraints, the existing single data‐parallel, and model‐parallel distributed training mechanisms frequently fail to fully utilize the computing power of edge devices, network topology and bandwidth resources. In light of the shortcomings mentioned earlier, this article proposes EdgeMesh, a hybrid parallel training mode based on the Mesh‐Tensorflow framework, consisting of an adaptive meshing strategy and a dynamic model convolutional partitioning strategy. The computing power of IoT edge devices significantly speeds up the DNN training process. In a resource‐constrained environment, each node only supports a subset of the model's parallel computing tasks, reducing communication and memory overhead while retaining high scalability. Experiments show that when compared to single‐machine training and data parallel mode, EdgeMesh distributed training mechanism can reduce the average delay by 3.2 times and average memory overhead by 43% while maintaining model accuracy. The computing power of IoT edge devices effectively accelerates the DNN training process.",10.1002/ett.4706
71aa819311ebd94807650b991507057b89c1dc21,Benchmarking Test-Time Unsupervised Deep Neural Network Adaptation on Edge Devices,2022,"The prediction accuracy of deep neural networks (DNNs) after deployment at the edge can suffer with time due to shifts in the distribution of the new data. To improve robustness of DNNs, they must be able to update themselves. However, DNN adaptation at the edge is challenging due to lack of resources. Recently, lightweight prediction-time unsupervised DNN adaptation techniques have been introduced that improve prediction accuracy of the models for noisy data by re-tuning the batch normalization parameters. This paper performs a comprehensive measurement study of such techniques to quantify their performance and energy on various edge devices as well as find bottlenecks and propose optimization opportunities.",10.48550/arXiv.2203.11295
032664444b92418998f51e37a87951873b0d81dc,"Algorithm Appreciation: Algorithmic Performance, Developmental Processes, and User Interactions",2020,"In this research, we conduct an online experiment to better understand perceptions of algorithmic features (fairness, accountability, transparency, and explainability) This study identifies key factors of algorithm and conceptualizes such issues in relation to trust by testing how they affect user emotion and satisfaction of personalized machine learning algorithms. The results indicate the heuristic roles of algorithmic characteristics in terms of their underlying links to trust and subsequent behaviors. Users experience a dual-process in assessing AI features and formulating trust through their heuristic-systematic evaluations. Heuristic and systematic processes are positively linked to trust and systematic processes are positively connected to trust and performance expectancy, which serve as antecedents of emotions. Heuristic and systematic processes offer a useful perspective on the conceptualization of AI experience and interaction. User cognitive processes identified provide solid foundations for algorithm design and development and a stronger basis for the design of sensemaking AI services.",10.1109/CCCI49893.2020.9256470
0b583f37bd2c82ded076c72179b82009fa0b0901,Fairness Perceptions of Artificial Intelligence: A Review and Path Forward,2023,"Abstract A key insight from research on organizational justice is that fairness is in the eye of the beholder. With increasing discussions – especially among computer scientists and policymakers – about the potential biases and unfairness of decisions made by Artificial Intelligence (AI) systems, there is a critical need to consider how decision-subjects perceive the fairness of AI-led decision-making. Drawing upon theoretical and empirical perspectives on perceived fairness in organizational justice scholarship, this review categorizes and analyzes perceptions of AI fairness as they impact the effective implementation of AI in workplaces and beyond. Specifically, we review existing empirical research on AI fairness according to distinct dimensions of perceived fairness – distributive, procedural, interpersonal, and informational – with a focus on its potential to inform organizational decision-making. In doing so, we provide new insights and offer directions for future interdisciplinary research in this burgeoning field.",10.1080/10447318.2023.2210890
ea1438e2891853048c6383f234414fe43de793c7,Knowing About Knowing: An Illusion of Human Competence Can Hinder Appropriate Reliance on AI Systems,2023,"The dazzling promises of AI systems to augment humans in various tasks hinge on whether humans can appropriately rely on them. Recent research has shown that appropriate reliance is the key to achieving complementary team performance in AI-assisted decision making. This paper addresses an under-explored problem of whether the Dunning-Kruger Effect (DKE) among people can hinder their appropriate reliance on AI systems. DKE is a metacognitive bias due to which less-competent individuals overestimate their own skill and performance. Through an empirical study (N = 249), we explored the impact of DKE on human reliance on an AI system, and whether such effects can be mitigated using a tutorial intervention that reveals the fallibility of AI advice, and exploiting logic units-based explanations to improve user understanding of AI advice. We found that participants who overestimate their performance tend to exhibit under-reliance on AI systems, which hinders optimal team performance. Logic units-based explanations did not help users in either improving the calibration of their competence or facilitating appropriate reliance. While the tutorial intervention was highly effective in helping users calibrate their self-assessment and facilitating appropriate reliance among participants with overestimated self-assessment, we found that it can potentially hurt the appropriate reliance of participants with underestimated self-assessment. Our work has broad implications on the design of methods to tackle user cognitive biases while facilitating appropriate reliance on AI systems. Our findings advance the current understanding of the role of self-assessment in shaping trust and reliance in human-AI decision making. This lays out promising future directions for relevant HCI research in this community.",10.1145/3544548.3581025
1f07cfdea0151260643c5b506ac60844eefb77b4,The Effect of Recommendation Source and Justification on Professional Development Recommendations for High School Teachers,2022,"This paper describes a study conducted in the process of building a recommender system that provides personalized professional development pathways for high school teachers seeking to increase their disciplinary knowledge and/or their teaching skills. A controlled experiment (N = 190) was conducted to study the effects of the presented justification for the recommendations (teachers’ needs vs. their interests) and the presented source of the recommendations (a human expert vs. an AI algorithm) on users’ perceptions of and experience with the system. Our results show an interaction effect between these two system aspects: users who are told that the recommendations are based on their interests have a better experience when the recommendations are presented as originating from an AI algorithm, while users who are told that the recommendations are based on their needs have a better experience when the recommendations are presented as originating from a human expert.",10.1145/3511095.3531280
35546dbfeae1327d58c6604798d4c935e3379ebf,For What It’s Worth: Humans Overwrite Their Economic Self-interest to Avoid Bargaining With AI Systems,2022,"As algorithms are increasingly augmenting and substituting human decision-making, understanding how the introduction of computational agents changes the fundamentals of human behavior becomes vital. This pertains to not only users, but also those parties who face the consequences of an algorithmic decision. In a controlled experiment with 480 participants, we exploit an extended version of two-player ultimatum bargaining where responders choose to bargain with either another human, another human with an AI decision aid or an autonomous AI-system acting on behalf of a passive human proposer. Our results show strong responder preferences against the algorithm, as most responders opt for a human opponent and demand higher compensation to reach a contract with autonomous agents. To map these preferences to economic expectations, we elicit incentivized subject beliefs about their opponent’s behavior. The majority of responders maximize their expected value when this is line with approaching the human proposer. In contrast, responders predicting income maximization for the autonomous AI-system overwhelmingly override economic self-interest to avoid the algorithm.",10.1145/3491102.3517734
032856bb493e3a26c04f17e59c84d1372e0b0411,A Novel Model for Imbalanced Data Classification,2020,"Recently, imbalanced data classification has received much attention due to its wide applications. In the literature, existing researches have attempted to improve the classification performance by considering various factors such as the imbalanced distribution, cost-sensitive learning, data space improvement, and ensemble learning. Nevertheless, most of the existing methods focus on only part of these main aspects/factors. In this work, we propose a novel imbalanced data classification model that considers all these main aspects. To evaluate the performance of our proposed model, we have conducted experiments based on 14 public datasets. The results show that our model outperforms the state-of-the-art methods in terms of recall, G-mean, F-measure and AUC.",10.1609/AAAI.V34I04.6145
93b906be68b64932e7695491b715f32513e0246b,Trustworthy AI-based Performance Diagnosis Systems for Cloud Applications: A Review,2025,"Performance diagnosis systems are defined as detecting abnormal performance phenomena and play a crucial role in cloud applications. An effective performance diagnosis system is often developed based on artificial intelligence (AI) approaches, which can be summarized into a general framework from data to models. However, the AI-based framework has potential hazards that could degrade the user experience and trust. For example, a lack of data privacy may compromise the security of AI models, and low robustness can be hard to apply in complex cloud environments. Therefore, defining the requirements for building a trustworthy AI-based performance diagnosis system has become essential. This article systematically reviews trustworthiness requirements in AI-based performance diagnosis systems. We first introduce trustworthiness requirements and extract six key requirements from a technical perspective, including data privacy, fairness, robustness, explainability, efficiency, and human intervention. We then unify these requirements into a general performance diagnosis framework, ranging from data collection to model development. Next, we comprehensively provide related works for each component and concrete actions to improve trustworthiness in the framework. Finally, we identify possible research directions and challenges for the future development of trustworthy AI-based performance diagnosis systems.",10.1145/3701740
03380b937bbe24ac0d9f08cf89e907e6e6cc8714,A CEEMD Method for Diesel Engine Misfire Fault Diagnosis based on Vibration Signals,2020,"Aiming at the characteristics of diesel engine fault vibration signals which are generally nonlinear and non-stationary, and the difficulty in extracting fault frequencies, a diesel engine fault diagnosis method based on Complementary Ensemble Empirical Mode Decomposition (CEEMD) and Least Square Support Vector Machine (LSSVM) was proposed. CEEMD was used to decompose the original signals, and a number of inherent mode functions (IMF) were obtained. The IMF components were screened by the correlation coefficient method. In order to extract features from vibration signals, we made normalized energy as the features which were inputted into LSSVM for training and testing. Finally we realize the identification and diagnosis of diesel engine misfire fault.",10.23919/CCC50068.2020.9189312
848331da4639e7284c797833e1b4c62d3a8755a9,Improving Misfire Fault Diagnosis with Cascading Architectures via Acoustic Vehicle Characterization,2022,"In a world dependent on road-based transportation, it is essential to understand automobiles. We propose an acoustic road vehicle characterization system as an integrated approach for using sound captured by mobile devices to enhance transparency and understanding of vehicles and their condition for non-expert users. We develop and implement novel deep learning cascading architectures, which we define as conditional, multi-level networks that process raw audio to extract highly granular insights for vehicle understanding. To showcase the viability of cascading architectures, we build a multi-task convolutional neural network that predicts and cascades vehicle attributes to enhance misfire fault detection. We train and test these models on a synthesized dataset reflecting more than 40 hours of augmented audio. Through cascading fuel type, engine configuration, cylinder count and aspiration type attributes, our cascading CNN achieves 87.0% test set accuracy on misfire fault detection which demonstrates margins of 8.0% and 1.7% over naïve and parallel CNN baselines. We explore experimental studies focused on acoustic features, data augmentation, and data reliability. Finally, we conclude with a discussion of broader implications, future directions, and application areas for this work.",10.3390/s22207736
02e8eed0f3bd60875c802adcf05af282ea8063da,Research on Fuel Offset Control of High-Pressure Common-Rail Diesel Engine Based on Crankshaft Segment Signals,2022,"This paper studies the fuel supply offset of diesel engines based on the crankshaft segment signal. Engine nonuniformity refers to the crankshaft torque fluctuation caused by cylinder-to-cylinder differences caused by misfiring or differences in the fuel supply or air supply. Fuel injection offset can reduce the nonuniformity between cylinders to realize high-efficiency and low pollution combustion. Based on crankshaft segment signal characteristics, an individual cylinder fuel offset control (FOC) strategy was built. The high-pressure common-rail diesel engine operating nonuniform control strategy was put forward. Based on crankshaft segment signal characteristics at different operating conditions, the reasonable operating condition of detecting the engine individual cylinder nonuniform degree was put forward. The open-loop and closed-loop control mode based on the condition was set up. The proportional-integral (PI) control algorithm is proposed to quantify engine individual cylinder nonuniform degree, and the fuel amount offset value was obtained. According to the principle of FOC, based on the automotive electronics development ASCET software platform, the FOC strategy module of the electronic control unit (ECU) was designed, and the simulation experiment was carried out. Research shows that for Z cylinder engine, just the first Z/2 harmonic components below fire frequency can fully reflect the state of the engine’s nonuniform operation. The control target to individual cylinder FOC is zero for the synthetic waveform amplitude of the first Z/2 harmonic components. Compared with the traditional quantization method, the fuel offset information extracted from the crankshaft segment signal has stronger anti-interference and more accurate parameters. FOC algorithm can accurately reflect the engine’s operating nonuniformity. The control of the nonuniformity is reasonable. The offset fuel amount calculated by FOC is very consistent with the fuel supply state of each cylinder set by the experiment, which meets the requirement of accurate fuel injection control of the diesel engine.",10.3390/s22093355
2be53bcf0a734788677ddda90aa155625128330e,Research on Feature Extraction Method of Engine Misfire Fault Based on Signal Sparse Decomposition,2021,"Engine vibration signals are easy to be interfered by other noise, causing feature signals that represent its operating status get submerged and further leading to difficulty in engine fault diagnosis. In addition, most of the signals utilized to verify the extraction method are derived from numerical simulation, which are far away from the real engine signals. To address these problems, this paper combines the priority of signal sparse decomposition and engine finite element model to research a novel feature extraction method for engine misfire diagnosis. Firstly, in order to highlight resonance regions related with impact features, the vibration signal is performed with a high-pass filter process. Secondly, the dictionary with clear physical meaning is constructed by the unit impulse function, whose parameters are associated with engine system modal characteristics. Afterwards, the signals that indicate the engine operating status are accurately reconstructed by segmental matching pursuit. Finally, a series of precise simulation signals originated from the engine dynamic finite element model, and experimental signals on the automotive engine are used to verify the proposed method’s effectiveness and antinoise performance. Additionally, comparisons with wavelet decomposition further show the proposed method to be more reliable in engine misfire diagnosis.",10.1155/2021/6650932
0341673e7ed78a096b9b9b51fbdbeff08beed660,Meta Soft Label Generation for Noisy Labels,2020,"The existence of noisy labels in the dataset causes significant performance degradation for deep neural networks (DNNs). To address this problem, we propose a Meta Soft Label Generation algorithm called MSLG, which can jointly generate soft labels using meta-learning techniques and learn DNN parameters in an end-to-end fashion. Our approach adapts the meta-learning paradigm to estimate optimal label distribution by checking gradient directions on both noisy training data and noise-free meta-data. In order to iteratively update soft labels, meta-gradient descent step is performed on estimated labels, which would minimize the loss of noise-free meta samples. In each iteration, the base classifier is trained on estimated meta labels. MSLG is model-agnostic and can be added on top of any existing model at hand with ease. We performed extensive experiments on CIFAR10, Clothing1M and Food101N datasets. Results show that our approach outperforms other state-of-the-art methods by a large margin. Our code is available at https://github.com/gorkemalgan/MSLG_noisy_label.",10.1109/ICPR48806.2021.9412490
7d98e03231653906f10e3ae86d902e39f7e87384,On the Maximal Local Disparity of Fairness-Aware Classifiers,2024,"Fairness has become a crucial aspect in the development of trustworthy machine learning algorithms. Current fairness metrics to measure the violation of demographic parity have the following drawbacks: (i) the average difference of model predictions on two groups cannot reflect their distribution disparity, and (ii) the overall calculation along all possible predictions conceals the extreme local disparity at or around certain predictions. In this work, we propose a novel fairness metric called Maximal Cumulative ratio Disparity along varying Predictions' neighborhood (MCDP), for measuring the maximal local disparity of the fairness-aware classifiers. To accurately and efficiently calculate the MCDP, we develop a provably exact and an approximate calculation algorithm that greatly reduces the computational complexity with low estimation error. We further propose a bi-level optimization algorithm using a differentiable approximation of the MCDP for improving the algorithmic fairness. Extensive experiments on both tabular and image datasets validate that our fair training algorithm can achieve superior fairness-accuracy trade-offs.",10.48550/arXiv.2406.03255
034522a2b8db1dfc0b4153a80683c0f0390fe64e,Optimizing loss functions through multi-variate taylor polynomial parameterization,2020,"Metalearning of deep neural network (DNN) architectures and hyperparameters has become an increasingly important area of research. Loss functions are a type of metaknowledge that is crucial to effective training of DNNs, however, their potential role in metalearning has not yet been fully explored. Whereas early work focused on genetic programming (GP) on tree representations, this paper proposes continuous CMA-ES optimization of multivariate Taylor polynomial parameterizations. This approach, TaylorGLO, makes it possible to represent and search useful loss functions more effectively. In MNIST, CIFAR-10, and SVHN benchmark tasks, TaylorGLO finds new loss functions that outperform the standard cross-entropy loss as well as novel loss functions previously discovered through GP, in fewer generations. These functions serve to regularize the learning task by discouraging overfitting to the labels, which is particularly useful in tasks where limited training data is available. The results thus demonstrate that loss function optimization is a productive new avenue for metalearning.",10.1145/3449639.3459277
10a6e667f43bdad697565d54f3bc3f2c96ebafa9,A Fast and Accurate GaN Power Transistor Model and Its Application for Electric Vehicle,2024,"In order to overcome the challenge of balancing accuracy with simulation speed of power electronics converters for system-level simulation, the paper proposes a GaN power transistor model that can accurately and rapidly predict power losses, which is suitable for system-level application such as an electric vehicle. The model is based on equivalent circuit and formed by behavioural equations to carefully model both conduction and switching losses. As a novelty, transistor power losses due to dynamic ON-state resistance is also included in the model. By comparison with experimental measurements and other available models of the similar type from the literature, it is shown that our model gives accurate results of the power losses and it helps to reduce the error by more than 70%. To accelerate simulation speed, power loss calculation and simulation time-step is decoupled. The power losses are represented in different levels and in the form of mathematical equations and look-up tables in MATLAB/Simulink. It is shown that our approach is able to reduce the simulation time by almost 18 times and maintain the same accuracy. The proposed GaN transistor loss model is finally implemented into a racing vehicle powertrain, where designers can obtain the power losses and temperature of the used power transistors in an easy and rapid way to optimise power electronics design.",10.1109/TVT.2023.3340297
034697983626566addc2c2832d9a176e66fe1bec,Meta-CoTGAN: A Meta Cooperative Training Paradigm for Improving Adversarial Text Generation,2020,"Training generative models that can generate high-quality text with sufficient diversity is an important open problem for Natural Language Generation (NLG) community. Recently, generative adversarial models have been applied extensively on text generation tasks, where the adversarially trained generators alleviate the exposure bias experienced by conventional maximum likelihood approaches and result in promising generation quality. However, due to the notorious defect of mode collapse for adversarial training, the adversarially trained generators face a quality-diversity trade-off, i.e., the generator models tend to sacrifice generation diversity severely for increasing generation quality. In this paper, we propose a novel approach which aims to improve the performance of adversarial text generation via efficiently decelerating mode collapse of the adversarial training. To this end, we introduce a cooperative training paradigm, where a language model is cooperatively trained with the generator and we utilize the language model to efficiently shape the data distribution of the generator against mode collapse. Moreover, instead of engaging the cooperative update for the generator in a principled way, we formulate a meta learning mechanism, where the cooperative update to the generator serves as a high level meta task, with an intuition of ensuring the parameters of the generator after the adversarial update would stay resistant against mode collapse. In the experiment, we demonstrate our proposed approach can efficiently slow down the pace of mode collapse for the adversarial text generators. Overall, our proposed method is able to outperform the baseline approaches with significant margins in terms of both generation quality and diversity in the testified domains.",10.1609/AAAI.V34I05.6490
034c9f16d7d16cf540c890a9f4ac6b9a4b562e21,Developing Computational Thinking through Project-Based Airplane Design Activities,2020,"This study investigated the outcome of project-based, airplane design activities on promoting computational thinking (CT) in sixth grade students in the context of an integrated STEM learning environment. A curriculum unit of airplane design activities was implemented in a sixth grade classroom over 10 days. The students' CT skills measured by the Bebras Challenges were significantly improved after their completion of the airplane design curriculum unit.",10.1109/FIE44824.2020.9274021
5cda080753c1729533d1d5d27387844dd8547279,A review of teaching and learning approach in implementing Project-Based Learning (PBL) with Computational Thinking (CT),2024,,10.1080/10494820.2024.2328280
034cfd74bf49688decde034c81fa3f8b71b5b424,Class-Discriminative Feature Embedding For Meta-Learning based Few-Shot Classification,2020,"Although deep learning-based approaches have been very effective in solving problems with plenty of labeled data, they suffer in tackling problems for which labeled data are scarce. In few-shot classification, the objective is to train a classifier from only a handful of labeled examples in a support set. In this paper, we propose a few-shot learning framework based on structured margin loss which takes into account the global structure of the support set in order to generate a highly discriminative feature space where the features from distinct classes are well separated in clusters. Moreover, in our meta-learning-based framework, we propose a context-aware query embedding encoder for incorporating support set context into query embedding and generating more discriminative and task-dependent query embeddings. The task-dependent features help the metalearner to learn a distribution over tasks more effectively. Extensive experiments based on few-shot, zero-shot and semi-supervised learning on three benchmarks show the advantages of the proposed model compared to state-of-the- art.",10.1109/WACV45572.2020.9093623
990be11df3e31eff2875661cee62482921274ded,Meta-learning meets the Internet of Things: Graph prototypical models for sensor-based human activity recognition,2021,,10.1016/j.inffus.2021.10.009
3b6c09150cad1f9bc4c1817aef27e8125119a2f7,Robust Fast Adaptation from Adversarially Explicit Task Distribution Generation,2024,"Meta-learning is a practical learning paradigm to transfer skills across tasks from a few examples. Nevertheless, the existence of task distribution shifts tends to weaken meta-learners' generalization capability, particularly when the training task distribution is naively hand-crafted or based on simple priors that fail to cover critical scenarios sufficiently. Here, we consider explicitly generative modeling task distributions placed over task identifiers and propose robustifying fast adaptation from adversarial training. Our approach, which can be interpreted as a model of a Stackelberg game, not only uncovers the task structure during problem-solving from an explicit generative model but also theoretically increases the adaptation robustness in worst cases. This work has practical implications, particularly in dealing with task distribution shifts in meta-learning, and contributes to theoretical insights in the field. Our method demonstrates its robustness in the presence of task subpopulation shifts and improved performance over SOTA baselines in extensive experiments. The code is available at the project site https://sites.google.com/view/ar-metalearn.",10.48550/arXiv.2407.19523
5d55a245edd6bfcd44c14d881d601bd400b71a25,Efficient matrix profile computation with Euclidean distance using Eigen transformation: Performance evaluation based on beat‐to‐beat interval (BBI) data,2024,"The matrix profile serves as a fundamental tool to provide insights into similar patterns within time series. Existing matrix profile algorithms have been primarily developed for the normalized Euclidean distance, which may not be a proper distance measure in many settings. The methodology work of this paper was motivated by statistical analysis of beat‐to‐beat interval (BBI) data collected from smartwatches to monitor e‐cigarette users' heart rate change patterns for which the original Euclidean distance ( L2$$ {L}_2 $$ ‐norm) would be a more suitable choice. Yet, incorporating the Euclidean distance into existing matrix profile algorithms turned out to be computationally challenging, especially when the time series is long with extended query sequences. We propose a novel methodology to efficiently compute matrix profile for long time series data based on the Euclidean distance. This methodology involves four key steps including (1) projection of the time series onto eigenspace; (2) enhancing singular value decomposition (SVD) computation; (3) early abandon strategy; and (4) determining lower bounds based on the first left singular vector. Simulation studies based on BBI data from the motivating example have demonstrated remarkable reductions in computational time, ranging from one‐fourth to one‐twentieth of the time required by the conventional method. Unlike the conventional method of which the performance deteriorates sharply as the time series length or the query sequence length increases, the proposed method consistently performs well across a wide range of the time series length or the query sequence length.",10.1002/sim.10123
154cd0ef271e6689fff492e738e1ce29389f9be6,Detection of Anomalies in Power Profiles using Data Analytics,2022,"Deployment of high reporting rate smart metering infrastructure together with a multitude of sensors for automation and control are an increasing trend among energy communities and prosumers. These systems provide useful information for data-driven prediction and classification models for micro-loads and local power generation. Matrix Profile is a promising general purpose data mining technique for time series data, such as electrical measurements from advanced smart meters. In this work, we first describe the measurement context that provides rich data availability for current advanced energy analytics applications. We target power profiles for both generation and load to highlight salient and complementary characteristics thereof, which can be leveraged in applications involving data-driven analytics for enhancing observability in distribution grids. A sensitivity analysis investigating the chosen method under various input noise assumptions is presented using Monte Carlo simulation. The comparative results indicate the relative robustness of the Matrix Profile approach for anomaly detection tasks in energy measurements traces.",10.1109/AMPS55790.2022.9978833
f7c53ed02ac540b44dcc87bb993126004b53be20,ALDI++: Automatic and parameter-less discord and outlier detection for building energy load profiles,2022,,10.1016/j.enbuild.2022.112096
94da5069d74e0831f6e1b2fb6d666d6c70c8d286,Evaluation of Compressed Residential Energy Forecasting Models,2021,"Embedded energy gateways are increasingly being implemented for monitoring and control tasks in smart energy applications in buildings, cities, and localized electrical grids. These leverage state-of-the-art computational intelligence models for forecasting and anomaly detection. We present a study comparing various neural network model implementations for residential energy forecasting, both full-size models and com-pressed models using weight quantization and network pruning techniques. The evaluation is carried out on publicly available data consisting of 23 homes from the reference Pecan Street database. We analyse the optimal trade-off between accuracy, in terms of MSE quantitative metrics, and model complexity as reflected by network memory footprint for resource constrained embedded platforms. Using a pre-determined MSE threshold for residential energy forecasting, we achieve higher than 50% memory footprint reduction compared to the baseline scenario, using a combination of weight pruning and quantization, for both individual homes and global models that are trained across time series.",10.1109/SMC52423.2021.9658990
0356ef80d5ad57b732b79c4888143012e262b41b,Preserving Semantic Neighborhoods for Robust Cross-modal Retrieval,2020,"The abundance of multimodal data (e.g. social media posts) has inspired interest in cross-modal retrieval methods. Popular approaches rely on a variety of metric learning losses, which prescribe what the proximity of image and text should be, in the learned space. However, most prior methods have focused on the case where image and text convey redundant information; in contrast, real-world image-text pairs convey complementary information with little overlap. Further, images in news articles and media portray topics in a visually diverse fashion; thus, we need to take special care to ensure a meaningful image representation. We propose novel within-modality losses which encourage semantic coherency in both the text and image subspaces, which does not necessarily align with visual coherency. Our method ensures that not only are paired images and texts close, but the expected image-image and text-text relationships are also observed. Our approach improves the results of cross-modal retrieval on four datasets compared to five baselines.",10.1007/978-3-030-58523-5_19
0fb7059cc6ae87944ef24c5884dded556f106927,Object-Aware Query Perturbation for Cross-Modal Image-Text Retrieval,2024,"The pre-trained vision and language (V\&L) models have substantially improved the performance of cross-modal image-text retrieval. In general, however, V\&L models have limited retrieval performance for small objects because of the rough alignment between words and the small objects in the image. In contrast, it is known that human cognition is object-centric, and we pay more attention to important objects, even if they are small. To bridge this gap between the human cognition and the V\&L model's capability, we propose a cross-modal image-text retrieval framework based on ``object-aware query perturbation.'' The proposed method generates a key feature subspace of the detected objects and perturbs the corresponding queries using this subspace to improve the object awareness in the image. In our proposed method, object-aware cross-modal image-text retrieval is possible while keeping the rich expressive power and retrieval performance of existing V\&L models without additional fine-tuning. Comprehensive experiments on four public datasets show that our method outperforms conventional algorithms. Our code is publicly available at \url{https://github.com/NEC-N-SOGI/query-perturbation}.",10.48550/arXiv.2407.12346
035d0151e91d21e4be5b45a16fc1a986fd6dc4f1,Prediction of Fan-out Panel Level Warpage using Neural Network Model with Edge Detection Enhancement,2020,"In recent years’ fan-out panel level package (FOPLP) is playing a most important role in electronics packaging industry due to lower cost, thinner profile, better electrical and thermal performance. However, thermally induced warpage is the critical issue for each and every researcher’s problem. Using larger wafer and panel size, and mismatch of coefficient of thermal expansion (CTE) among the constituent material, the thermally induced warpage is occurred which needs to be controlled effectively for fan-out package industry. In this work, finite element method (FEM) is used to build the FOPLP by considering equivalent CTE of molding material and then thermally induced warpage issue in the molding process is investigated. To reduce the experimental time and cost, FEM is the best method to simulate and analysis of warpage in FOPLP. Simulation can build a set of warpage result database of different geometric structures of panel level package (PLP). In addition to that, this work has been introduced Artificial Intelligence (AI) approach and the main purpose of this AI technique is to build a best model for warpage analysis by considering the design parameters like chip size, chip thickness, gap between the chips, thin film thick and so on. The best part of AI model is that, if numbers of different geometric parameters are used in this model then AI can predict the better warpage value without the help of FEM simulation. The implementation of machine learning concept which is an AI technique is to train the model from the different geometric model related to warpage datasets which are generated by FEM simulation. Convolution Neural Network (CNN) algorithm is applied to learn the relationship between geometry and amount of warpage occurred from the generated database. To enhance the training process, it has also considered Edge Detection technique. Eventually, this AI approach is successfully used to remove data points with so many similar warpage values and it could reduce training data as well as training time. Moreover, this paper successfully digitized warpage model obtained by the FEM and also successfully used the Artificial Neural Network (ANN) to train the database to learn and estimate the warpage for FOPLP",10.1109/ectc32862.2020.00255
03641310a3e7522caaae01914ee438e6f4175164,DroidLight: Lightweight Anomaly-based Intrusion Detection System for Smartphone Devices,2020,"Smartphone malware attacks are increasing alongside the growth of smartphone applications in the market. Researchers have proposed techniques to detect malware attacks using various approaches, which broadly include signature and anomaly-based intrusion detection systems (IDSs). Anomaly-based IDSs usually require training machine learning models with datasets collected from running both benign and malware applications. This may result in low detection accuracy when detecting zero-day malwares, i.e. those not previously seen or recorded. In this paper, we propose DroidLight, a lightweight IDS which can detect zero-day malware efficiently and effectively. We designed an algorithm for DroidLight that is based on one class classification and probability distribution analysis. For each smartphone application, the classification model learns its normal CPU utilisation and network traffic pattern. The model flags an intrusion alert if there is any significant deviation from the normal pattern. By deploying three self-developed malwares we performed realistic evaluation of DroidLight, i.e. the evaluation was performed on a real device while a real user was interacting with it. Evaluation results demonstrate that DroidLight can detect smartphone malwares with accuracy ranging from 93.3% to 100% while imposing only 1.5% total overhead on device resources.",10.1145/3369740.3369796
0b148c0593c4e32a457f8e005fd390407c9ce3fa,"A Systematic Review on Intrusion Detection System in Wireless Networks: Variants, Attacks, and Applications",2023,,10.1007/s11277-023-10773-x
ce28373ec1ad630be90ba22a5f3f9e0a1928058b,DroidPerf: Profiling Memory Objects on Android Devices,2023,"Optimizing performance inefficiencies in memory hierarchies is well-known for native languages, such as C and C++. There are few studies, however, on exploring memory inefficiencies in Android Runtime (ART). Running in ART, managed languages, such as Java and Kotlin, employ various abstractions, such as runtime support, ahead-of-time (AOT) compilation, and garbage collection (GC), which hide important execution details from the plain source code. In this paper, we develop DroidPerf, a lightweight, object-centric memory profiler for ART, which associates memory inefficiencies with objects created and used in Android apps. With such object-level information, DroidPerf is able to guide locality optimization on memory layouts, access patterns, and allocation patterns. Guided by DroidPerf, we optimize a number of popular Android apps and obtain significant performance gains. Many inefficiencies are confirmed by the code authors and optimization patches are under evaluation for upstreaming. As a practical tool, DroidPerf incurs ~32% runtime overhead and ~14% memory overhead on average. Furthermore, DroidPerf works in the production environment with off-the-shelf hardware, OS, Dalvik virtual machine, ART, and unmodified Android app source code.",10.1145/3570361.3592503
036e107534a995eaae1ccc88b9f303e551c4d51d,Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit Planes,2020,"As humans, we inherently perceive images based on their predominant features, and ignore noise embedded within lower bit planes. On the contrary, Deep Neural Networks are known to confidently misclassify images corrupted with meticulously crafted perturbations that are nearly imperceptible to the human eye. In this work, we attempt to address this problem by training networks to form coarse impressions based on the information in higher bit planes, and use the lower bit planes only to refine their prediction. We demonstrate that, by imposing consistency on the representations learned across differently quantized images, the adversarial robustness of networks improves significantly when compared to a normally trained model. Present state-of-the-art defenses against adversarial attacks require the networks to be explicitly trained using adversarial samples that are computationally expensive to generate. While such methods that use adversarial training continue to achieve the best results, this work paves the way towards achieving robustness without having to explicitly train on adversarial samples. The proposed approach is therefore faster, and also closer to the natural learning process in humans.",10.1109/cvpr42600.2020.00110
d135e14810052e40d59857fb393f33e367b44149,How Deep Learning Sees the World: A Survey on Adversarial Attacks & Defenses,2023,"Deep Learning is currently used to perform multiple tasks, such as object recognition, face recognition, and natural language processing. However, Deep Neural Networks (DNNs) are vulnerable to perturbations that alter the network prediction, named adversarial examples, which raise concerns regarding the usage of DNNs in critical areas, such as Self-driving Vehicles, Malware Detection, and Healthcare. This paper compiles the most recent adversarial attacks in Object Recognition, grouped by the attacker capacity and knowledge, and modern defenses clustered by protection strategies, providing background details to understand the topic of adversarial attacks and defenses. The new advances regarding Vision Transformers are also presented, which have not been previously done in the literature, showing the resemblance and dissimilarity between this architecture and Convolutional Neural Networks. Furthermore, the most used datasets and metrics in adversarial settings are summarized, along with datasets requiring further evaluation, which is another contribution. This survey compares the state-of-the-art results under different attacks for multiple architectures and compiles all the adversarial attacks and defenses with available code, comprising significant contributions to the literature. Finally, practical applications are discussed, and open issues are identified, being a reference for future works.",10.1109/ACCESS.2024.3395118
6163a9b5aaa5d367c1c3801ccf7612e49cf47239,Exploring transformer models in the sentiment analysis task for the under-resource Bengali language,2024,,10.1016/j.nlp.2024.100091
2e581de473030592d6f02ed764438730f086ef4b,Model Selection for Latency-Critical Inference Serving,2024,"In an inference service system, model selection and scheduling (MS&S) schemes map inference queries to trained machine learning (ML) models, hosted on a finite set of workers, to solicit accurate predictions within strict latency targets. MS&S is challenged by both varying query load and stochastic query inter-arrival patterns; however, state-of-the-art MS&S approaches conservatively account for load exclusively. In this paper, we first show that explicitly considering inter-arrival patterns creates opportunities to map queries to higher-accuracy (higher-latency) models during intermittent arrival lulls. We then propose RAMSIS, a framework for generating MS&S policies that exploits this finding. RAMSIS leverages a statistical problem model of query load and inter-arrival pattern to produce policies that maximize accuracy given some latency target. We evaluate RAMSIS-generated MS&S policies alongside state-of-the-art approaches. Notably, RAMSIS requires as low as 50.00% (on average 18.77%) fewer resources to achieve the same accuracy for an ImageNet image classification task given 26 trained models.",10.1145/3627703.3629565
f44bd2121b833e2f2ddda35cc37f9841eaf0695b,Dystri: A Dynamic Inference based Distributed DNN Service Framework on Edge,2023,"Deep neural network (DNN) inference poses unique challenges in serving computational requests due to high request intensity, concurrent multi-user scenarios, and diverse heterogeneous service types. Simultaneously, mobile and edge devices provide users with enhanced computational capabilities, enabling them to utilize local resources for deep inference processing. Moreover, dynamic inference techniques allow content-based computational cost selection per request. This paper presents Dystri, an innovative framework devised to facilitate dynamic inference on distributed edge infrastructure, thereby accommodating multiple heterogeneous users. Dystri offers a broad applicability in practical environments, encompassing heterogeneous device types, DNN-based applications, and dynamic inference techniques, surpassing the state-of-the-art (SOTA) approaches. With distributed controllers and a global coordinator, Dystri allows per-request, per-user adjustments of quality-of-service, ensuring instantaneous, flexible, and discrete control. The decoupled workflows in Dystri naturally support user heterogeneity and scalability, addressing crucial aspects overlooked by existing SOTA works. Our evaluation involves three multi-user, heterogeneous DNN inference service platforms deployed on distributed edge infrastructure, encompassing seven DNN applications. Results show Dystri achieves near-zero deadline misses and excels in adapting to varying user numbers and request intensities. Dystri outperforms baselines with accuracy improvement up to 95 ×.",10.1145/3605573.3605598
2ab5be2e45255cc3a268d5d6fb45f2ef95401f9c,Offloading Algorithms for Maximizing Inference Accuracy on Edge Device in an Edge Intelligence System,2023,"With the emergence of edge computing, the problem of offloading jobs between an Edge Device (ED) and an Edge Server (ES) received significant attention in the past. Motivated by the fact that an increasing number of applications are using Machine Learning (ML) inference from the data samples collected at the EDs, we study the problem of offloading <italic>inference jobs</italic> by considering the following novel aspects: 1) in contrast to a typical computational job, the processing time of an inference job depends on the size of the ML model, and 2) recently proposed Deep Neural Networks (DNNs) for resource-constrained devices provide the choice of scaling down the model size by trading off the inference accuracy. Considering that multiple ML models are available at the ED, and a powerful ML model is available at the ES, we formulate an Integer Linear Programming (ILP) problem with the objective of maximizing the total inference accuracy of <inline-formula><tex-math notation=""LaTeX"">$n$</tex-math><alternatives><mml:math><mml:mi>n</mml:mi></mml:math><inline-graphic xlink:href=""champati-ieq1-3267458.gif""/></alternatives></inline-formula> data samples at the ED subject to a time constraint <inline-formula><tex-math notation=""LaTeX"">$T$</tex-math><alternatives><mml:math><mml:mi>T</mml:mi></mml:math><inline-graphic xlink:href=""champati-ieq2-3267458.gif""/></alternatives></inline-formula> on the makespan. Noting that the problem is NP-hard, we propose an approximation algorithm Accuracy Maximization using LP-Relaxation and Rounding (AMR<inline-formula><tex-math notation=""LaTeX"">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=""champati-ieq3-3267458.gif""/></alternatives></inline-formula>) and prove that it results in a makespan at most <inline-formula><tex-math notation=""LaTeX"">$\text{2}T$</tex-math><alternatives><mml:math><mml:mrow><mml:mtext>2</mml:mtext><mml:mi>T</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""champati-ieq4-3267458.gif""/></alternatives></inline-formula> and achieves a total accuracy that is lower by a small constant from the optimal total accuracy implying that AMR<inline-formula><tex-math notation=""LaTeX"">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=""champati-ieq5-3267458.gif""/></alternatives></inline-formula> is asymptotically optimal. Further, if the data samples are identical we propose Accuracy Maximization using Dynamic Programming (AMDP), an optimal pseudo-polynomial time algorithm. Furthermore, we extend AMR<inline-formula><tex-math notation=""LaTeX"">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=""champati-ieq6-3267458.gif""/></alternatives></inline-formula> for the case of multiple ESs, where each ES is equipped with a powerful ML model. As proof of concept, we implemented AMR<inline-formula><tex-math notation=""LaTeX"">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=""champati-ieq7-3267458.gif""/></alternatives></inline-formula> on a Raspberry Pi, equipped with MobileNets, that is connected to a server equipped with ResNet, and studied the total accuracy and makespan performance of AMR<inline-formula><tex-math notation=""LaTeX"">$^{2}$</tex-math><alternatives><mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=""champati-ieq8-3267458.gif""/></alternatives></inline-formula> for image classification.",10.1109/TPDS.2023.3267458
c24918943fb52d9390b7e50fdf413c116de89f97,Layercake: Efficient Inference Serving with Cloud and Mobile Resources,2023,"Many mobile applications are now integrating deep learning models into their core functionality. These functionalities have diverse latency requirements while demanding high-accuracy results. Currently, mobile applications statically decide to use either in-cloud inference, relying on a fast and consistent network, or on-device execution, relying on sufficient local resources. However, neither mobile networks nor computation resources deliver consistent performance in practice. Consequently, mobile inference often experiences variable performance or struggles to meet performance goals, when inference execution decisions are not made dynamically. In this paper, we introduce Layer Cake, a deep-learning inference framework that dynamically selects the best model and location for executing inferences. Layercake accomplishes this by tracking model state and availability, both locally and remotely, as well as the network bandwidth, allowing for accurate estimations of model response time. By doing so, Layercake achieves latency targets in up to 96.4% of cases, which is an improvement of 16.7% over similar systems, while decreasing the cost of cloud-based resources by over 68.33% than in-cloud inference.",10.1109/CCGrid57682.2023.00027
0370d2ca08e1e16d9d388d27084798b43cf43749,Exploring Middle School Students' Reflections on the Infusion of CS into Science Classrooms,2020,"In recent years, there has been a dramatic increase in teaching CS in the context of other disciplines such as science. However, learning CS in an interdisciplinary context may be particularly challenging for students. An important goal for CS education researchers is to develop a deep understanding of the student experience when integrating CS into science classrooms in K-12. This paper presents the results of a mixed-methods study in which 75 middle school students engaged in a series of computationally rich science activities by creating simulations and models in a block-based programming language. After two semesters, students reported their experiences on in-class computer science activities through reflection essays. The quantitative results show that both experienced and novice students increased their CS knowledge significantly after several weeks, and a majority of students (72%) had positive sentiment toward the integration of CS into their science class. Deeper qualitative analysis of students' reflections revealed positive themes centered around the visualization and gamification of science concepts, the hands-on nature of the coding activities, and showing science from a different angle. On the other hand, students expressed negative sentiments on weaknesses in the activity design, lack of CS/science background/interest, and failing to make connections between CS and science concepts. These findings inform efforts to infuse CS education into different disciplines and reveal patterns that may foster success of K-12 classroom implementations.",10.1145/3328778.3366871
9c88d1a6981c150b92e87b72d08bccbe441c2488,Integrating Natural Language Processing in Middle School Science Classrooms: An Experience Report,2024,"With the increasing prevalence of large language models (LLMs) such as ChatGPT, there is a growing need to integrate natural language processing (NLP) into K-12 education to better prepare young learners for the future AI landscape. NLP, a sub-field of AI that serves as the foundation of LLMs and many advanced AI applications, holds the potential to enrich learning in core subjects in K-12 classrooms. In this experience report, we present our efforts to integrate NLP into science classrooms with 98 middle school students across two US states, aiming to increase students' experience and engagement with NLP models through textual data analyses and visualizations. We designed learning activities, developed an NLP-based interactive visualization platform, and facilitated classroom learning in close collaboration with middle school science teachers. This experience report aims to contribute to the growing body of work on integrating NLP into K-12 education by providing insights and practical guidelines for practitioners, researchers, and curriculum designers.",10.1145/3626252.3630881
0377b3da0cde3fa4abdf546c07d0bd6d899026e8,The complexity of constrained min-max optimization,2020,"Despite its important applications in Machine Learning, min-max optimization of objective functions that are nonconvex-nonconcave remains elusive. Not only are there no known first-order methods converging to even approximate local min-max equilibria (a.k.a. approximate saddle points), but the computational complexity of identifying them is also poorly understood. In this paper, we provide a characterization of the computational complexity as well as of the limitations of first-order methods in this problem. Specifically, we show that in linearly constrained min-max optimization problems with nonconvex-nonconcave objectives an approximate local min-max equilibrium of large enough approximation is guaranteed to exist, but computing such a point is PPAD-complete. The same is true of computing an approximate fixed point of the (Projected) Gradient Descent/Ascent update dynamics, which is computationally equivalent to computing approximate local min-max equilibria. An important byproduct of our proof is to establish an unconditional hardness result in the Nemirovsky-Yudin 1983 oracle optimization model, where we are given oracle access to the values of some function f : P → [−1, 1] and its gradient ∇ f, where P ⊆ [0, 1]d is a known convex polytope. We show that any algorithm that uses such first-order oracle access to f and finds an ε-approximate local min-max equilibrium needs to make a number of oracle queries that is exponential in at least one of 1/ε, L, G, or d, where L and G are respectively the smoothness and Lipschitzness of f. This comes in sharp contrast to minimization problems, where finding approximate local minima in the same setting can be done with Projected Gradient Descent using O(L/ε) many queries. Our result is the first to show an exponential separation between these two fundamental optimization problems in the oracle model.",10.1145/3406325.3451125
038087519b736ae17f2c091c876309a38be47e19,A Deep Learning Model for Early Detection of Fake News on Social Media*,2020,"Fake news detection has recently become an important topic of research. This is due to the impact of fake news on the internet especially on social media. Numerous of the models proposed in the previous studies are based on supervised learning. Therefore, these models are unable to deal with the huge amount of unlabeled data about fake news. Few studies focused on early detection. In this study, we built a semi-supervised learning model to detect fake news on social media at an early stage. By using a semi-supervised learning, we make our model able to deal with the huge amount of unlabeled data on social media. We first built a model to extract users' opinion expressed in comments, then we used CredRank Algorithm to evaluate users' credibility and built a small network of users involved in the spread of a given news. The outputs of these three steps serve as inputs of our news classifier SSLNews. SSLNews is composed of three networks: a shared CNN, an unsupervised CNN and a supervised CNN. We used real world datasets to evaluate our model, Politifact and Gossipcop. When using 25% of labeled data, SSLNews reaches an accuracy of 72.25% on Politifact and 70.35% on Gossipcop. When using data produced in the first 10 minutes of the beginning of the spread of the news, SSLNews reaches an accuracy of 71.10% on Politifact and 68.07% on Gossipcop.",10.1109/BESC51023.2020.9348311
cf6e0ac752beac9cf6187552c303089255b527e3,Integrating Social Explanations Into Explainable Artificial Intelligence (XAI) for Combating Misinformation: Vision and Challenges,2024,"This article overviews the state of the art, research challenges, and future directions in our vision: integrating social explanation into explainable artificial intelligence (XAI) to combat misinformation. In our context, “social explanation” is an explanatory approach that reveals the social aspect of misinformation by analyzing sociocontextual cues, such as user attributes, user engagement metrics, diffusion patterns, and user comments. Our vision is motivated by the research gap in the existing XAI that tends to overlook the broader social context in which misinformation spreads. In this article, we first define social explanation, demonstrating it through examples, enabling technologies, and real-world applications. We then outline the unique benefits social explanation brings to the fight against misinformation and discuss the challenges that make our vision complex. The significance of this article lies in introducing the “social explanation” concept in XAI, which has been underexplored in the previous literature. Also, we demonstrate how social explanations can be effectively employed to tackle misinformation and promote collaboration across diverse fields by drawing upon interdisciplinary techniques spanning from computer science, social computing, human–computer interaction, to psychology. We hope that this article will advance progress in the field of XAI and contribute to the ongoing efforts to counter misinformation.",10.1109/TCSS.2024.3404236
