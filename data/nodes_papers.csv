PaperID,Title,Year,Abstract,DOI
00000f4552fc7742585d81666afb9c1163393e17,Real-Time Monitoring of ECG and GSR Signals during Computer-Based Training,2012,,10.1007/978-3-642-30950-2_10
8509485721393d06595834a98c527a3f54052009,Continuously analysing fine-grained student behaviours in an online collaborative learning environment,2022,"ABSTRACT This study examines the use of data analytics to evaluate students’ behaviours during their participation in an online collaborative learning environment called SkyApp. To visualise the learning traits of engagement, emotion and motivation, students’ inputs and activity data were captured and quantified for analysis. Experiments were first carried out in a primary school with 66 fifth-grade students. Each participating student collaborated with other group members using SkyApp through a personal computer in a classroom setting. While the students collaboratively solved mathematics questions in two quizzes, data about their learning traits were captured by SkyApp; various patterns of these data were then analysed. The findings based on the data analysis were triangulated with the results of questionnaires, revealing the relationship between student behaviours and collaborative learning as a kind of pedagogical intervention. The correlation between the data from the two approaches – data analytics and questionnaires – shows the potential of understanding students’ behaviours through continuous data analysis of learner-produced data without the need to collect self-reported data from the students involved. This study demonstrates that teachers can monitor and identify the effects of specific pedagogical interventions on students’ behaviours by looking at different snapshots taken over the course of e-learning activities.",10.1080/10494820.2022.2039944
a7446c94ef2515f744c1bb04e8f51f0f141a1993,Using Wearable Physiological Sensors for Affect-Aware Intelligent Tutoring Systems,2021,"Intelligent Tutoring Systems (ITS) have shown great potential in enhancing the learning process by being able to adapt to the learner’s knowledge level, abilities, and difficulties. An aspect that can affect the learning process but is not taken into consideration by traditional ITS is the affective state of the learner. In this work, we propose the use of physiological signals and machine learning for the task of detecting a learner’s affective state during test taking. To this end, wearable physiological sensors were used to record electroencephalography (EEG), electrocardiography (ECG), and electromyography (EMG) signals from 27 individuals while participating in a computerised English language test. Features extracted from the acquired signals were used in order to train machine learning models for the prediction of the self-reported difficulty level of the test’s questions, as well as for the prediction of whether the questions would be answered correctly. Supervised classification experiments showed that there is a relation between the acquired signals and the examined tasks, reaching a classification F1-score of 74.21% for the prediction of the self-reported question difficulty level, and a classification F1-score of 59.14% for predicting whether a question was answered correctly. The acquired results demonstrate the potential of the examined approach for enhancing ITS with information relating to the affective state of the learners.",10.1109/JSEN.2020.3023886
e8038192e1de82211b9c45d7a0f2fac2aabdd016,Comparison and Efficacy of Synergistic Intelligent Tutoring Systems with Human Physiological Response,2019,"The analysis of physiological signals is ubiquitous in health and medical diagnosis as a primary tool for investigation and inquiry. Physiological signals are now being widely used for psychological and social fields. They have found promising application in the field of computer-based learning and tutoring. Intelligent Tutoring Systems (ITS) is a fast-paced growing field which deals with the design and implementation of customized computer-based instruction and feedback methods without human intervention. This paper introduces the key concepts and motivations behind the use of physiological signals. It presents a detailed discussion and experimental comparison of ITS. The synergism of ITS and physiological signals in automated tutoring systems adapted to the learner’s emotions and mental states are presented and compared. The insights are developed, and details are presented. The accuracy and classification methods of existing systems are highlighted as key areas of improvement. High-precision measurement systems and neural networks for machine-learning classification are deemed prospective directions for future improvements to existing systems.",10.3390/s19030460
dc106710715e3b724e393e3562cbb8f2e5865161,Personalized Affective Feedback to Address Students’ Frustration in ITS,2019,"The importance of affective states in learning has led many Intelligent Tutoring Systems (ITS) to include students’ affective states in their learner models. The adaptation and hence the benefits of an ITS can be improved by detecting and responding to students’ affective states. In prior work, we have created and validated a theory-driven model for detecting students’ frustration, as well as identifying its causes as students interact with the ITS. In this paper, we present a strategy to respond to students’ frustration by offering motivational messages that address different causes of frustration. Based on attribution theory, these messages are created to praise the student's effort, attribute the results to the identified cause, show sympathy for failure or obtain feedback from the students. We implemented our approach in three schools where students interacted with the ITS. Data from 188 students from the three schools collected across two weeks was used for our analysis. The results suggest that the frustration instances reduced significantly statistically ($p < 0.05$), due to the motivational messages. This study suggests that motivational messages that use attribution theory and address the reason for frustration reduce the number of frustration instances per session.",10.1109/TLT.2018.2807447
20004cb54319bbfd29c0828913c3bf5a8d6c8a1c,Spotting prejudice with nonverbal behaviours,2016,"Despite prejudice cannot be directly observed, nonverbal behaviours provide profound hints on people inclinations. In this paper, we use recent sensing technologies and machine learning techniques to automatically infer the results of psychological questionnaires frequently used to assess implicit prejudice. In particular, we recorded 32 students discussing with both white and black collaborators. Then, we identified a set of features allowing automatic extraction and measured their degree of correlation with psychological scores. Results confirmed that automated analysis of nonverbal behaviour is actually possible thus paving the way for innovative clinical tools and eventually more secure societies.",10.1145/2971648.2971703
f5658866a7e1a75d0815deb1c269d0b44b4b20b0,Physiological Measurement on Students' Engagement in a Distributed Learning Environment,2015,"Measuring studentsâ?? engagement in a distributed learning environment is a challenge. In particular, a 
 
teacher gives a lecture at one location, while at the same time the remote students watch the lecture through 
 
a display screen. In such situation, it is difficult for the teacher to know the reaction at the remote location. 
 
In this paper, we conducted a field study to measure studentsâ?? engagement by using galvanic skin response 
 
(GSR) sensors, where students simultaneously watched the lecture at the two locations. Our results showed 
 
the studentsâ?? GSR response was aligned with the surveys, which means that during a distributed learning 
 
environment, GSR sensors can be used as an indicator on studentsâ?? engagement. Furthermore, our user 
 
studies resulted in non-engaging student learning experiences that would be difficult obtained at a lab 
 
condition. Based on the findings, we found that the patterns of GSR readings were rather different when 
 
compared to the previous relevant studies, where users were engaged. In addition, we noticed that the 
 
density of GSR response at the remote location was higher when compared to the one at the lecture room. 
 
We believe that our studies are beneficial on physiological computing, as we first presented the patterns of 
 
GSR sensors on non-engaging user experiences. Moreover, as an alternative method, GSR sensors can be 
 
easily implemented in a distributed learning environment to provide feedback to teachers.",10.5220/0005229101490156
d11678f5e923165d55c6434a445ab39dfbe65f5e,A Theory-Driven Approach to Predict Frustration in an ITS,2013,"The importance of affect in learning has led many intelligent tutoring systems (ITS) to include learners' affective states in their student models. The approaches used to identify affective states include human observation, self-reporting, data from physical sensors, modeling affective states, and mining students' data in log files. Among these, data mining and modeling affective states offer the most feasible approach in real-world settings, which may involve a huge number of students. Systems using data mining approaches to predict frustration have reported high accuracy, while systems that predict frustration by modeling affective states, not only predict a student's affective state but also the reason for that state. In our approach, we combine these approaches. We begin with the theoretical definition of frustration, and operationalize it as a linear regression model by selecting and appropriately combining features from log file data. We illustrate our approach by modeling the learners' frustration in Mindspark, a mathematics ITS with large-scale deployment. We validate our model by independent human observation. Our approach shows comparable results to existing data mining approaches and also the clear interpretation of the reasons for the learners' frustration.",10.1109/TLT.2013.31
8153eeca504bfe4395b0b5620e706b21740af954,Embodied Affect in Tutorial Dialogue: Student Gesture and Posture,2013,,10.1007/978-3-642-39112-5_1
276a31ccce7e80084334355db5e2445c6201595b,Algorithms Based on CWT and Classifiers to Control Cardiac Alterations and Stress Using an ECG and a SCR,2013,"This paper presents the results of using a commercial pulsimeter as an electrocardiogram (ECG) for wireless detection of cardiac alterations and stress levels for home control. For these purposes, signal processing techniques (Continuous Wavelet Transform (CWT) and J48) have been used, respectively. The designed algorithm analyses the ECG signal and is able to detect the heart rate (99.42%), arrhythmia (93.48%) and extrasystoles (99.29%). The detection of stress level is complemented with Skin Conductance Response (SCR), whose success is 94.02%. The heart rate variability does not show added value to the stress detection in this case. With this pulsimeter, it is possible to prevent and detect anomalies for a non-intrusive way associated to a telemedicine system. It is also possible to use it during physical activity due to the fact the CWT minimizes the motion artifacts.",10.3390/s130506141
0000c55f1170657127217eb9b3c28fc6383cc647,Performance Analysis of RACH Procedure with Beta Traffic-Activated Machine-Type-Communication,2014,"Machine-Type-Communication (MTC) is a key enabler for a variety of novel smart systems, such as smart grid, eHealth, Intelligent Transport System (ITS), and smart city, opening the area of the cyber physical systems. These systems may require the use of a huge number of MTC devices, which will put a great pressure on the whole network, i.e. Radio Access Network (RAN) and Core Network (CN) parts, resulting in the shape of congestion and system overload. Aiming at better evaluating the network performance under the existence of MTC traffic and also the effectiveness of the congestion control methods, the 3rd Generation Partnership Project (3GPP) group has proposed two traffic models: Uniform Distribution (over 60 s) and Beta Distribution (over 10 s). In this paper, a recursive operation-based analytical model, namely General Recursive Estimation (GRE), for modeling the performance of RACH procedure in the existence of MTC with Beta traffic is proposed. In order to show the effectiveness of our analytical model GRE, many metrics have been considered, such as the total number of MTC devices in each Random Access (RA) slot, the number of success MTC devices in each RA slot, and the Cumulative Distribution Function (CDF) of preamble transmission. Numerical results demonstrate the accuracy of GRE. Moreover, our model GRE could be used to model the performance of RACH procedure with any type of traffic.",10.1109/GLOCOM.2014.7417095
2e1c953f3f76907e98a3f3df23ec6858e0def92c,Machine-to-Machine Communications With Massive Access: Congestion Control,2019,"With the deployment of machine-to-machine (M2M) communications, it is expected that the number of devices will enormously increase. When these devices attempt to concurrently access the network, a radio access network overload problem arises. In this case, the conventional random access procedure used in Long Term Evolution-Advanced (LTE-A) networks is rendered inefficient due to the frequent collisions that lead to excessive delay and resource wastage. In this paper, we propose an efficient scalable overload control algorithm for M2M with massive access. The proposed algorithm can allocate the uplink resources within bounded contention time in a distributed manner. Hence, it can achieve full resource utilization that leads to reduced: access delay, energy consumption, and blocking probability. Additionally, we provide a method for estimating the number of backlogged devices in the network. The performance of the proposed algorithm is evaluated analytically and using simulations. To prove its effectiveness, the performance of the proposed algorithm is compared to the dynamic-access class-barring scheme, where the results depict the superiority of the proposed scheme. Finally, a binary integer programming problem is formulated, where we show that the achieved access delay using the proposed algorithm approaches the optimal value.",10.1109/JIOT.2018.2888502
93574827efa9dddf11d290cc55dbde6aea714453,A Simple Model of MTC in Smart Factories,2018,"In this paper we develop a simple, yet accurate, performance model to understand if and how evolutions of traditional cellular network protocols can be exploited to allow large numbers of devices to gain control of transmission resources in smart factory radio access networks. The model results shed light on the applicability of evolved access procedures and help understand how many devices can be served per base station. In addition, considering the simultaneous presence of different traffic classes, we investigate the effectiveness of prioritised access, exploiting access class barring techniques. Our model shows that, even with the sub-millisecond time slots foreseen in LTE Advanced Pro and 5G, a base station can accommodate at most few thousand devices to guarantee access latencies below 100 ms with high transmission success probabilities. This calls for a rethinking of wireless access strategies to avoid ultra-dense cell deployments within smart factory infrastructures.",10.1109/INFOCOM.2018.8485942
0001732a8f4f8443731a16387dda81c61a7bf108,Convolutional Neural Network Techniques on X-ray Images for Covid-19 Classification,2021,"At the end of 2019, the World Health Organization (WHO) referred that the Public Health Commission of Hubei Province, China, reported cases of severe and unknown pneumonia. A new coronavirus, SARS-CoV-2, was identified as responsible for the lung infection, called COVID-19 (coronavirus disease 2019). An early diagnosis of those carrying the virus becomes crucial to contain the spread, morbidity and mortality of the pandemic. The definitive diagnosis is made through specific tests, among which imaging tests play a very important role. Achieving this goal cannot be separated from radiological examination, and chest X-ray is the most easily available and least expensive alternative. The use of X-ray chest radiographs, as an element that assists the diagnosis and that allows the follow up of the disease, is the subject of many publications that adopt machine learning approaches. This work focuses on the most adopted Convolutional Neural Network Techniques applied on chest X-ray images.",10.1109/BIBM52615.2021.9669784
000178cd12c8a6e5da8215b6365fae03c20fd18d,End-to-End Representation Learning for Correlation Filter Based Tracking,2017,"The Correlation Filter is an algorithm that trains a linear template to discriminate between images and their translations. It is well suited to object tracking because its formulation in the Fourier domain provides a fast solution, enabling the detector to be re-trained once per frame. Previous works that use the Correlation Filter, however, have adopted features that were either manually designed or trained for a different task. This work is the first to overcome this limitation by interpreting the Correlation Filter learner, which has a closed-form solution, as a differentiable layer in a deep neural network. This enables learning deep features that are tightly coupled to the Correlation Filter. Experiments illustrate that our method has the important practical benefit of allowing lightweight architectures to achieve state-of-the-art performance at high framerates.",10.1109/CVPR.2017.531
d534fc35fb46d0d1df38838db2107ebb0ec71ff4,EANTrack: An Efficient Attention Network for Visual Tracking,2024,"Recently, Siamese trackers have gained widespread attention in visual tracking due to their exceptional performance. However, many trackers still suffer from limitations in challenging scenarios, such as fast motion and scale variation, which hinder the full exploitation of target features. Consequently, the accuracy and efficiency of the trackers are limited. Therefore, this paper proposes an efficient attention network, called EAN, to improve tracking performance. The EAN comprises three primary components, namely a Transformer-s subnetwork, a Transformer-t subnetwork, and a Feature-Fused Attention Module (FFAM). The designed Transformer-s and Transformer-t subnetworks adopt complementary structures and functions to fully integrate and emphasize the relevant feature information, including channel and spatial features. The FFAM is responsible for fusing the multi-level features from both subnetworks, which establishes the global dependencies between the templates and search regions and enhances the discriminative power of the model. To further improve the tracking accuracy, a novel Feature-Aware Attention Module (FAAM) is introduced into the tracking prediction head to enhance the feature representation capability of the model. Finally, we propose an efficient EANTrack tracker based on EAN for robust tracking in complex scenarios, which exhibits significant advantages in challenging attributes. Experimental results on multiple benchmarks indicate that our approach achieves remarkable tracking performance with a real-time running speed of 55.6fps. Note to Practitioners—Siamese trackers have garnered considerable attention in the field of visual tracking due to their impressive performance. However, these trackers often face limitations in challenging scenarios, which impede the complete exploitation of target features. As a result, the accuracy and efficiency of many trackers are compromised. To address these issues, we propose an efficient tracker called EANTrack to enable robust tracking in complex scenarios. Our EANTrack exhibits significant advantages in handling challenging attributes. Please refer to our complete paper for detailed information on the EANTrack tracker and experimental results. Practitioners in the field can benefit from our research by leveraging our findings and methodologies in their work. We encourage further exploration and experimentation to enhance the performance and applicability of visual tracking systems.",10.1109/TASE.2023.3319676
82f4f334e87737921035e7d172a895ed41a03fdf,The use of video clickstream data to predict university students’ test performance: A comprehensive educational data mining approach,2022,"Video clickstream behaviors such as pause, forward, and backward offer great potential for educational data mining and learning analytics since students exhibit a significant amount of these behaviors in online courses. The purpose of this study is to investigate the predictive relationship between video clickstream behaviors and students’ test performance with two consecutive experiments. The first experiment was performed as an exploratory study with 22 university students using a single test performance measure and basic statistical techniques. The second experiment was performed as a conclusive study with 16 students using repeated measures and comprehensive data mining techniques. The findings show that a positive correlation exists between the total number of clicks and students’ test performance. Those students who performed a high number of clicks, slow backward speed or doing backwards or pauses achieved better test performance than those who performed a lower number of clicks, or who used fast-backward or fast-forward. In addition, students’ test performance could be predicted using video clickstream data with a good level of accuracy (Root Mean Squared Error Percentage (%RMSE) ranged between 15 and 20). Furthermore, the mean of backward speed, number of pauses, and number/percentage of backwards were found to be the most important indicators in predicting students’ test performance. These findings may help educators or researchers identify students who are at risk of failure. Finally, the study provides design suggestions based on the findings for the preparation of video-based lectures.",10.1007/s10639-022-11403-y
860fc334ad902d847f28cbbe0234d158590da5b4,Immersion Measurement in Watching Videos Using Eye-tracking Data,2022,"Immersion plays a crucial role in video watching, leading viewers to a positive experience, such as increased engagement and decreased fatigue. However, few studies measure immersion while watching videos, and questionnaires are typically used in the measurement of immersion for other applications. These methods may rely on the viewer's memory and cause biased results. Therefore, we propose an objective immersion detection model by leveraging people's gaze behavior while watching videos. In a lab study with 30 participants, an in-depth analysis is carried out on a number of gaze features and machine learning (ML) models to identify the immersion state. Several gaze features are highly indicative of immersion and ML models with these features are able to detect an immersion state of video watchers. Post-hoc interviews demonstrate that our approach is applicable to measure immersion in the middle of watching a video, where some practical issues are discussed as well.",10.1109/TAFFC.2022.3209311
48748deac5da9d0ab0d09bffe254ada4b151bbb0,Investigating student engagement with intentional content: An exploratory study of instructional videos,2021,,10.1016/J.IJME.2021.100505
77cb4b76139c0e798b0e3fecc4f5db08f97aaf00,Behavior analysis of students in video classes,2020,"This Research-to-Practice full paper presents the result of a study whose core is characterized by the application of data analysis techniques from the use of video lessons. Distance education contributes to the generation of educational data to conduct investigations of educational processes. Methodologies for the automatic extraction of useful information from large volumes of data, especially data mining, have significantly contributed to improvements in the field of education. However, most traditional methods focus entirely on the data or how it is structured, with no significant concern with the educational process as a whole. Also, little attention has been given to information related to the students’ behavior during the use of educational resources and media. Therefore, we consider that examining the students’ behavior during the execution of educational videos can contribute to a more accurate analysis of the quality of the resources used, the way they learn, and what factors influence their learning. Thus, this research to practice full paper presents conducts a study to investigate how students behave during the use of video classes and seek to highlight the benefits of this type of analysis for education. Results demonstrate that evaluating the educational resource through the analysis of the actions involved can contribute substantially to the activities performed by teachers and content makers.",10.1109/FIE44824.2020.9274274
6e33e92e106c0a1044a89e8dd33330a4d755e7eb,Continuous Evaluation of Video Lectures from Real-Time Difficulty Self-Report,2019,"With the increased reach and impact of video lectures, it is crucial to understand how they are experienced. Whereas previous studies typically present questionnaires at the end of the lecture, they fail to capture students' experience in enough granularity. In this paper we propose recording the lecture difficulty in real-time with a physical slider, enabling continuous and fine-grained analysis of the learning experience. We evaluated our approach in a study with 100 participants viewing two variants of two short lectures. We demonstrate that our approach helps us paint a more complete picture of the learning experience. Our analysis has design implications for instructors, providing them with a method that helps them compare their expectations with students' beliefs about the lectures and to better understand the specific effects of different instructional design decisions.",10.1145/3290605.3300816
000270f4b973ba6db82a3cb6a6f1e139ae2974c6,A primary-permanent-magnet vernier linear machine with improved fault-tolerant capability,2013,"This paper proposes and analyzes a new three-phase primary permanent-magnet fault-tolerant vernier (PPMFTV) linear machine. The key of this new machine topology is to adopt the concept of fault-tolerant teeth (FTT) to provide the desired decoupling among phases. The electromagnetic performances of the newly designed machine are analyzed including the flux, configuration, back-EMF, inductance, fault tolerance, thrust ripple as well as thrust density. The finite-element analysis (FEA) results show that the proposed PPMFTV machine not only retains the merits of primary-PM vernier machines, but also offers lower thrust ripple, higher thrust density and higher fault-tolerant capability.",10.1109/ICEMS.2013.6713219
49a9a7363858a821ce824f57ae8b3efa675372a3,Permanent magnet vernier machine: a review,2019,"Permanent magnet vernier machines (PMVMs) gained a lot of interest over the past couple of decades. This is mainly due to their high torque density enabled by the magnetic gearing effect. This study will provide a thorough review of recent advances in PMVMs. This review will cover the principle of operation and nature of magnetic gearing in PMVMs, and a better understanding of novel PMVM topologies using different winding configuration as well as different modulation poles and rotor structures. Detailed discussions on the choice of gear ratio, slot-pole combinations, design optimisation and role of advanced materials in PMVMs will be presented. This will provide an update on the current state-of-the art as well as future areas of research. Furthermore, the power factor issue, fault tolerance as well as cost reduction will be discussed highlighting the gap between the current state-of-the art and what is needed in practical applications.",10.1049/IET-EPA.2018.5474
927d7b49cafc6a856e9d41ae0e66424a21c81a31,Permanent Magnet Vernier Machines: A Review,2018,"Permanent-magnet Vernier machines (PMVMs) gained a lot of interest over the past couple of decades. This is mainly due to their high torque density enabled by magnetic gearing effect. This paper will provide a thorough review of recent advances in PMVMs. This review will cover principle of operation and nature of magnetic gearing in PMVMs, and better understanding of novel PMVMs topologies using different winding configuration as well as different modulation poles and rotor structures. Detailed discussions on choice of gear ratio/slot-pole combinations, design optimization and role of advanced materials in PMVMs will be presented. This will provide an update on the current state of the art as well as future areas of research. Furthermore, design issues, fault tolerance as well as cost reduction will be discussed highlighting the gap between the current state of the art and what is needed in practical applications.",10.1109/ICELMACH.2018.8507194
a766e4ae21d21e3ae16c40c9c6c7c9b396c8f769,Location of generating units most affecting the angular stability of the power system based on the analysis of instantaneous power waveforms,2023,"the paper, the results of investigations on",10.24425/ACS.2020.133500
ad321149076b26ae9b23521cd04f0c9436f100ce,Design static VAR compensator controller using artificial neural network optimized by modify Grey Wolf Optimization,2015,This paper introduce a novel design of the static VAR compensator (SVC) controller for damping power system oscillations. A multi layer neural network model tuned by Grey Wolf Optimization algorithm (GWO) is investigated and presented. GWO search algorithm is used to optimized all the connection of weights and biases for the artificial neural network. The proposed approach depends up on the expected wide range of the effective operating conditions of the SVC. Modification is introduced in the proposed optimizer exploration-exploitation balance to enhance its rate of convergence over the original algorithm. The robustness of the proposed controller successfully testing for damping oscillations of two-axis nonlinear single machine infinite bus system. A comparative study for the controller based the classical PI controller have been presented.,10.1109/IJCNN.2015.7280704
bc291d55606ed79f1bab48b35d1ce83b406c17db,A review on speech processing using machine learning paradigm,2021,,10.1007/s10772-021-09808-0
8daca718703f71dc1772d911b1cabdbee3a86f7d,Fuzzy-based algorithm for Fongbe continuous speech segmentation,2017,,10.1007/s10044-016-0591-6
eec878d2f940d79408e2a3d6e20bdc7d20c2f0c5,Adaptive decision-level fusion for Fongbe phoneme classification using fuzzy logic and Deep Belief Networks,2015,"In this paper, we compare three approaches for decision fusion in a phoneme classification problem. We especially deal with decision-level fusion from Naive Bayes and Learning Vector Quantization (LVQ) classifiers that were trained and tested by three speech analysis techniques: Mel-frequency Cepstral Coefficients (MFCC), Relative Spectral Transform - Perceptual Linear Prediction (Rasta-PLP) and Perceptual Linear Prediction (PLP). Optimal decision making is performed with the non-parametric and parametric methods. We investigated the performance of both decision methods with a third proposed approach using fuzzy logic. The work discusses the classification of an African language phoneme namely Fongbe language and all experiments were performed on its dataset. After classification and the decision fusion, the overall decision fusion performance is obtained on test data with the proposed approach using fuzzy logic whose classification accuracies are 95,54% for consonants and 83,97% for vowels despite the lower execution time of Deep Belief Networks.",10.5220/0005536100150024
8acccb75195a265961aa11ff2de5843103be764c,Speech Phoneme Classification by Intelligent Decision-Level Fusion,2015,,10.1007/978-3-319-31898-1_4
0003975c67dc6e9f0c405cd4dd072b04b684b742,Support Vector Machines for Visualization and Dimensionality Reduction,2008,,10.1007/978-3-540-87536-9_36
03874e05c356223ba0362bc5483e2dbaba8f3202,Understanding Support Vector Machines with Polynomial Kernels,2019,"Interpreting models learned by a support vector machine (SVM) is often difficult, if not impossible, due to working in high-dimensional spaces. In this paper, we present an investigation into polynomial kernels for the SVM. We show that the models learned by these machines are constructed from terms related to the statistical moments of the support vectors. This allows us to deepen our understanding of the internal workings of these models and, for example, gauge the importance of combinations of features. We also discuss how the SVM with a quadratic kernel is related to the likelihood-ratio test for normally distributed populations.",10.23919/EUSIPCO.2019.8903042
56e803a517d8cd7fd9f1474ad54dc926533ec38f,Multidimensional Data Visualization Based on the Minimum Distance Between Convex Hulls of Classes,2018,,10.1134/S1054661818040247
8962505574f752c9b82ee243bceabf4753bfe717,A novel representation in three-dimensions for high dimensional data sets,2018,,10.1016/J.DATAK.2018.07.001
87c198f86bc6eaa9439d309904081ac294382abb,"Constructing Interactive Visual Classification, Clustering and Dimension Reduction Models for n-D Data",2017,"Abstract: The exploration of multidimensional datasets of all possible sizes and dimensions is a long-standing challenge in knowledge discovery, machine learning, and visualization. While multiple efficient visualization methods for n-D data analysis exist, the loss of information, occlusion, and clutter continue to be a challenge. This paper proposes and explores a new interactive method for visual discovery of n-D relations for supervised learning. The method includes automatic, interactive, and combined algorithms for discovering linear relations, dimension reduction, and generalization for non-linear relations. This method is a special category of reversible General Line Coordinates (GLC). It produces graphs in 2-D that represent n-D points losslessly, i.e., allowing the restoration of n-D data from the graphs. The projections of graphs are used for classification. The method is illustrated by solving machine-learning classification and dimension-reduction tasks from the domains of image processing, computer-aided medical diagnostics, and finance. Experiments conducted on several datasets show that this visual interactive method can compete in accuracy with analytical machine learning algorithms.",10.3390/informatics4030023
e1b372337eb0606ab883cf4a290004c7d7471781,Prediction-oriented dimensionality reduction of industrial data sets,2011,,10.1007/978-3-642-21822-4_24
4683f3b5aa7c23b3ac9fbe312ef29a4f44c8bb1b,Data Mining of Agricultural Yield Data: A Comparison of Regression Models,2009,,10.1007/978-3-642-03067-3_3
181e892fa1207461e93a953c05e0395223345b10,Compressed Bayesian Federated Learning for Reliable Passive Radio Sensing in Industrial IoT,2024,"Bayesian Federated Learning (FL) has been recently introduced to provide well-calibrated Machine Learning (ML) models quantifying the uncertainty of their predictions. Despite their advantages compared to frequentist FL setups, Bayesian FL tools implemented over decentralized networks are subject to high communication costs due to the iterated exchange of local posterior distributions among cooperating devices. Therefore, this paper proposes a communication-efficient decentralized Bayesian FL policy to reduce the communication overhead without sacrificing final learning accuracy and calibration. The proposed method integrates compression policies and allows devices to perform multiple optimization steps before sending the local posterior distributions. We integrate the developed tool in an Industrial Internet of Things (IIoT) use case where collaborating nodes equipped with autonomous radar sensors are tasked to reliably localize human operators in a workplace shared with robots. Numerical results show that the developed approach obtains highly accurate yet well-calibrated ML models compatible with the ones provided by conventional (uncompressed) Bayesian FL tools while substantially decreasing the communication overhead (i.e., up to 99%). Furthermore, the proposed approach is advantageous when compared with state-of-the-art compressed frequentist FL setups in terms of calibration, especially when the statistical distribution of the testing dataset changes.",10.1109/CAI59869.2024.00071
914968d3df158691e8eae81da1e4b8086ddac91d,Client Selection for Federated Bayesian Learning,2022,"Distributed Stein Variational Gradient Descent (DSVGD) is a non-parametric distributed learning framework for federated Bayesian learning, where multiple clients jointly train a machine learning model by communicating a number of non-random and interacting particles with the server. Since communication resources are limited, selecting the clients with most informative local learning updates can improve the model convergence and communication efficiency. In this paper, we propose two selection schemes for DSVGD based on Kernelized Stein Discrepancy (KSD) and Hilbert Inner Product (HIP). We derive the upper bound on the decrease of the global free energy per iteration for both schemes, which is then minimized to speed up the model convergence. We evaluate and compare our schemes with conventional schemes in terms of model accuracy, convergence speed, and stability using various learning tasks and datasets.",10.1109/JSAC.2023.3242720
87cbb87d3d4f623280c87a7677e984be7b98f2f2,Federated Averaging Langevin Dynamics: Toward a unified theory and new algorithms,2022,"This paper focuses on Bayesian inference in a federated learning context (FL). While several distributed MCMC algorithms have been proposed, few consider the specific limitations of FL such as communication bottlenecks and statistical heterogeneity. Recently, Federated Averaging Langevin Dynamics (FALD) was introduced, which extends the Federated Averaging algorithm to Bayesian inference. We obtain a novel tight non-asymptotic upper bound on the Wasserstein distance to the global posterior for FALD. This bound highlights the effects of statistical heterogeneity, which causes a drift in the local updates that negatively impacts convergence. We propose a new algorithm VR-FALD* that uses control variates to correct the client drift. We establish non-asymptotic bounds showing that VR-FALD* is not affected by statistical heterogeneity. Finally, we illustrate our results on several FL benchmarks for Bayesian inference.",10.48550/arXiv.2211.00100
cc40bf6fdb7047912277051b9ec9d55f0c6d110d,Channel-Driven Decentralized Bayesian Federated Learning for Trustworthy Decision Making in D2D Networks,2022,"Bayesian Federated Learning (FL) offers a principled framework to account for the uncertainty caused by limitations in the data available at the nodes implementing collaborative training. In Bayesian FL, nodes exchange information about local posterior distributions over the model parameters space. This paper focuses on Bayesian FL implemented in a Device-to-Device (D2D) network via Decentralized Stochastic Gradient Langevin Dynamics (DSGLD), a recently introduced gradient-based Markov Chain Monte Carlo (MCMC) method. Based on the observation that DSGLD applies random Gaussian perturbations to the model parameters, we propose to leverage channel noise on the D2D links as a mechanism for MCMC sampling. The proposed approach is compared against a conventional implementation of frequentist FL based on compression and digital transmission, highlighting advantages and limitations.",10.1109/ICASSP49357.2023.10095158
a47326d7996def2cd015c4d2558b63073bb6846d,Low-Precision Stochastic Gradient Langevin Dynamics,2022,"While low-precision optimization has been widely used to accelerate deep learning, low-precision sampling remains largely unexplored. As a consequence, sampling is simply infeasible in many large-scale scenarios, despite providing remarkable benefits to generalization and uncertainty estimation for neural networks. In this paper, we provide the first study of low-precision Stochastic Gradient Langevin Dynamics (SGLD), showing that its costs can be significantly reduced without sacrificing performance, due to its intrinsic ability to handle system noise. We prove that the convergence of low-precision SGLD with full-precision gradient accumulators is less affected by the quantization error than its SGD counterpart in the strongly convex setting. To further enable low-precision gradient accumulators, we develop a new quantization function for SGLD that preserves the variance in each update step. We demonstrate that low-precision SGLD achieves comparable performance to full-precision SGLD with only 8 bits on a variety of deep learning tasks.",10.48550/arXiv.2206.09909
caf27f4542e8dccdaaa22a27a7b95b5ef0c6d2b8,County-level prioritization for managing the Covid-19 pandemic: a systematic unsupervised learning approach,2024,"
Purpose
The COVID-19 pandemic has posed many challenges in almost all sectors around the globe. Because of the pandemic, government entities responsible for managing health-care resources face challenges in managing and distributing their limited and valuable health resources. In addition, severe outbreaks may occur in a small or large geographical area. Therefore, county-level preparation is crucial for officials and organizations who manage such disease outbreaks. However, most COVID-19-related research projects have focused on either state- or country-level. Only a few studies have considered county-level preparations, such as identifying high-risk counties of a particular state to fight against the COVID-19 pandemic. Therefore, the purpose of this research is to prioritize counties in a state based on their COVID-19-related risks to manage the COVID outbreak effectively.


Design/methodology/approach
In this research, the authors use a systematic hybrid approach that uses a clustering technique to group counties that share similar COVID conditions and use a multi-criteria decision-making approach – the analytic hierarchy process – to rank clusters with respect to the severity of the pandemic. The clustering was performed using two methods, k-means and fuzzy c-means, but only one of them was used at a time during the experiment.


Findings
The results of this study indicate that the proposed approach can effectively identify and rank the most vulnerable counties in a particular state. Hence, state health resources managing entities can identify counties in desperate need of more attention before they allocate their resources and better prepare those counties before another surge.


Originality/value
To the best of the authors’ knowledge, this study is the first to use both an unsupervised learning approach and the analytic hierarchy process to identify and rank state counties in accordance with the severity of COVID-19.
",10.1108/jsit-02-2023-0027
94702376b1afab661189d8a68416178a4da8102b,Review on the Evaluation and Development of Artificial Intelligence for COVID-19 Containment,2023,"Artificial intelligence has significantly enhanced the research paradigm and spectrum with a substantiated promise of continuous applicability in the real world domain. Artificial intelligence, the driving force of the current technological revolution, has been used in many frontiers, including education, security, gaming, finance, robotics, autonomous systems, entertainment, and most importantly the healthcare sector. With the rise of the COVID-19 pandemic, several prediction and detection methods using artificial intelligence have been employed to understand, forecast, handle, and curtail the ensuing threats. In this study, the most recent related publications, methodologies and medical reports were investigated with the purpose of studying artificial intelligence’s role in the pandemic. This study presents a comprehensive review of artificial intelligence with specific attention to machine learning, deep learning, image processing, object detection, image segmentation, and few-shot learning studies that were utilized in several tasks related to COVID-19. In particular, genetic analysis, medical image analysis, clinical data analysis, sound analysis, biomedical data classification, socio-demographic data analysis, anomaly detection, health monitoring, personal protective equipment (PPE) observation, social control, and COVID-19 patients’ mortality risk approaches were used in this study to forecast the threatening factors of COVID-19. This study demonstrates that artificial-intelligence-based algorithms integrated into Internet of Things wearable devices were quite effective and efficient in COVID-19 detection and forecasting insights which were actionable through wide usage. The results produced by the study prove that artificial intelligence is a promising arena of research that can be applied for disease prognosis, disease forecasting, drug discovery, and to the development of the healthcare sector on a global scale. We prove that artificial intelligence indeed played a significantly important role in helping to fight against COVID-19, and the insightful knowledge provided here could be extremely beneficial for practitioners and research experts in the healthcare domain to implement the artificial-intelligence-based systems in curbing the next pandemic or healthcare disaster.",10.3390/s23010527
000429c966f527613c700947a0912b10ce3f359d,Grasp Anything: Combining Teacher-Augmented Policy Gradient Learning with Instance Segmentation to Grasp Arbitrary Objects,2024,"Interactive grasping from clutter, akin to human dexterity, is one of the longest-standing problems in robot learning. Challenges stem from the intricacies of visual perception, the demand for precise motor skills, and the complex interplay between the two. In this work, we present Teacher-Augmented Policy Gradient (TAPG), a novel two-stage learning framework that synergizes reinforcement learning and policy distillation. After training a teacher policy to master the motor control based on object pose information, TAPG facilitates guided, yet adaptive, learning of a sensorimotor policy, based on object segmentation. We zero-shot transfer from simulation to a real robot by using Segment Anything Model for promptable object segmentation. Our trained policies adeptly grasp a wide variety of objects from cluttered scenarios in simulation and the real world based on human-understandable prompts. Furthermore, we show robust zero-shot transfer to novel objects. Videos of our experiments are available at https://maltemosbach.github.io/grasp_anything.",10.1109/ICRA57147.2024.10610700
ea0cb965e2a5190d2497caddbbb79e7036d207cf,Dexterous Pre-grasp Manipulation for Human-like Functional Categorical Grasping: Deep Reinforcement Learning and Grasp Representations,2023,"Many objects, such as tools and household items, can be used only if grasped in a very specific way - grasped functionally. Often, a direct functional grasp is not possible, though. We propose a method for learning a dexterous pre-grasp manipulation policy to achieve human-like functional grasps using deep reinforcement learning. We introduce a dense multi-component reward function that enables learning a single policy, capable of dexterous pre-grasp manipulation of novel instances of several known object categories with an anthropomorphic hand. The policy is learned purely by means of reinforcement learning from scratch, without any expert demonstrations. It implicitly learns to reposition and reorient objects of complex shapes to achieve given functional grasps. In addition, we explore two different ways to represent a desired grasp: explicit and more abstract, constraint-based. We show that our method consistently learns to successfully manipulate and achieve desired grasps on previously unseen object instances of known categories using both grasp representations. Training is completed on a single GPU in under three hours.",10.1109/TASE.2025.3541768
000458c2651ab147d503429f6460aa80155e8e35,Segment as Points for Efficient Online Multi-Object Tracking and Segmentation,2020,"Current multi-object tracking and segmentation (MOTS) methods follow the tracking-by-detection paradigm and adopt convolutions for feature extraction. However, as affected by the inherent receptive field, convolution based feature extraction inevitably mixes up the foreground features and the background features, resulting in ambiguities in the subsequent instance association. In this paper, we propose a highly effective method for learning instance embeddings based on segments by converting the compact image representation to un-ordered 2D point cloud representation. Our method generates a new tracking-by-points paradigm where discriminative instance embeddings are learned from randomly selected points rather than images. Furthermore, multiple informative data modalities are converted into point-wise representations to enrich point-wise features. The resulting online MOTS framework, named PointTrack, surpasses all the state-of-the-art methods including 3D tracking methods by large margins (5.4% higher MOTSA and 18 times faster over MOTSFusion) with the near real-time speed (22 FPS). Evaluations across three datasets demonstrate both the effectiveness and efficiency of our method. Moreover, based on the observation that current MOTS datasets lack crowded scenes, we build a more challenging MOTS dataset named APOLLO MOTS with higher instance density. Both APOLLO MOTS and our codes are publicly available at this https URL.",10.1007/978-3-030-58452-8_16
0004fe12ae50ed77aab5d8bfbe74bb58d9b333c1,Design and experimental evaluation of an intelligent PID controller using CMACs,2009,"Skilled workers decrease in these years. Then it is a problem that skilled techniques and know-how of specific work are not taken over to next generations. Therefore, a controller which is based on human-skill is needed to design. On the other hand, in recent year, various neural networks (NNs) have been proposed. These technologies of the NNs enable us to deal with the nonlinear systems, and they play an important role in the field of control engineering. Furtheremore, a cerebellar model articulation controller (CMAC) has been proposed as one of artificial NNs. The CMAC is highly structured mathematically, considering the cerebellar as a controller for the body motion system. The CMAC has teh advantage for the shor learning time compared with the conventional NNs. In this paper, a skill based intelligent PID controller using CMACs is proposed. According to the proposed method, PID gains are tuned by trained CMACs. Those CMACs are learned using a human skill data in an off-line manner. And also they are trained on-line by a reference model. Furthermore, the experiment in order to illustrate the proposed scheme is performed by using one of pilot-scaled helicopter control models.",10.1109/ICNSC.2009.4919370
761163778df822d53efe6c3015eb7afeba768b78,Data-driven human skill evaluation for excavator operation,2016,"The construction field is increasingly adopting automation and manpower-saving technologies. However, automating all types of construction machinery is difficult because some operations require human skill and judgment-that is, professional skills-for realizing optimal work efficiency. However, the lack of competent professionals because of the decreasing and aging population in Japan is an emerging societal problem and is leading to concerns such as deterioration in workplace safety and difficulty in transferring professional skills. This study proposes human skill evaluation and assesses human skills through the application of control engineering. The human operator is analyzed as a nonlinear controller comprised of a proportional-integral-derivative (PID) controller and a PID parameter tuner. The parameters of this database-driven PID controller are tuned using data (i.e., reference signals, input signals, and output signals) obtained from operation. The proposed approach is experimentally demonstrated by applying it to the case of an excavator.",10.1109/AIM.2016.7576814
00059087c954c1af6ece33115315e3e0ecc2f2c2,Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem,2020,"Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men. In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al, 2019) Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. Rather than attempt to create a ‘balanced’ dataset, we use transfer learning on a small set of trusted, gender-balanced examples. This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. A known pitfall of transfer learning on new domains is ‘catastrophic forgetting’, which we address at adaptation and inference time. During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction. At inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al, 2019 on WinoMT with no degradation of general test set BLEU. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability.",10.18653/v1/2020.acl-main.690
0005b67f248df75bf0f4eba37328142951559498,End-to-End Rate-Distortion Optimized 3D Gaussian Representation,2024,"3D Gaussian Splatting (3DGS) has become an emerging technique with remarkable potential in 3D representation and image rendering. However, the substantial storage overhead of 3DGS significantly impedes its practical applications. In this work, we formulate the compact 3D Gaussian learning as an end-to-end Rate-Distortion Optimization (RDO) problem and propose RDO-Gaussian that can achieve flexible and continuous rate control. RDO-Gaussian addresses two main issues that exist in current schemes: 1) Different from prior endeavors that minimize the rate under the fixed distortion, we introduce dynamic pruning and entropy-constrained vector quantization (ECVQ) that optimize the rate and distortion at the same time. 2) Previous works treat the colors of each Gaussian equally, while we model the colors of different regions and materials with learnable numbers of parameters. We verify our method on both real and synthetic scenes, showcasing that RDO-Gaussian greatly reduces the size of 3D Gaussian over 40x, and surpasses existing methods in rate-distortion performance.",10.48550/arXiv.2406.01597
00064ea12f64bd7770aa923e67e09597f5a9925b,Learning Pairwise Similarity for Data Clustering,2006,"Each clustering algorithm induces a similarity between given data points, according to the underlying clustering criteria. Given the large number of available clustering techniques, one is faced with the following questions: (a) Which measure of similarity should be used in a given clustering problem? (b) Should the same similarity measure be used throughout the d-dimensional feature space? In other words, are the underlying clusters in given data of similar shape? Our goal is to learn the pairwise similarity between points in order to facilitate a proper partitioning of the data without the a priori knowledge of k, the number of clusters, and of the shape of these clusters. We explore a clustering ensemble approach combined with cluster stability criteria to selectively learn the similarity from a collection of different clustering algorithms with various parameter configurations",10.1109/ICPR.2006.754
422337fa8bfebf37fad9730b1711acf8ea5f4e92,An Efficient Retrieval System Framework for Fabrics Based on Fine-Grained Similarity,2022,"In the context of “double carbon”, as a traditional high energy consumption industry, the textile industry is facing the severe challenges of energy saving and emission reduction. To improve production efficiency in the textile industry, we propose the use of content-based image retrieval technology to shorten the fabric production cycle. However, fabric retrieval has high requirements for results, which makes it difficult for common retrieval methods to be directly applied to fabric retrieval. This paper presents a novel method for fabric image retrieval. Firstly, we define a fine-grained similarity to measure the similarity between two fabric images. Then, a convolutional neural network with a compact structure and cross-domain connections is designed to narrow the gap between fabric images and similarities. To overcome the problems of probabilistic missing and difficult training in classical hashing, we introduce a variational network module and structural module into the hashing model, which is called DVSH. We employ list-wise learning to perform similarity embedding. The experimental results demonstrate the superiority and efficiency of the proposed hashing model, DVSH.",10.3390/e24091319
88b10094e3ba27033dff0f96a412329220cb1f9a,An Ensemble Clustering Framework Based on Hierarchical Clustering Ensemble Selection and Clusters Clustering,2022,"Abstract Ensemble clustering combines the results of multiple individual clustering methods for better results. Basically, all available clustering methods can be combined to produce final clusters. However, selecting a subset of optimal methods can reduce the complexity and increase the efficiency of ensemble clustering methods. This article examines the problem of selecting individual clustering methods to produce an ensemble hierarchical clustering method. Hierarchical clustering is a technique for grouping data at different scales by creating dendrograms. The aim is to select a subset of individual hierarchical clustering methods considering diversity and quality that can create an ensemble clustering method with minimal complexity. The proposed method consists of three main phases. The selection of a subset of individual hierarchical clustering methods is done in the first phase. In the second phase, the results of the selected clusters are re-clustered to create super-clusters. Super-clusters can combine clustering knowledge of different methods into one clustering form. Finally, the final clusters are formed by assigning each sample to a super-cluster with the shortest distance in the third phase. Experimental results on several datasets from the University of California Irvine (UCI) repository show that the proposed method performs better than the state-of-the-art algorithms.",10.1080/01969722.2022.2073704
1df91e92ac8e388ed0ccf305f0c0eb769373aeff,A comprehensive study of clustering ensemble weighting based on cluster quality and diversity,2019,,10.1007/s10044-017-0676-x
22301fafc0d082fe05d0c6d85693abf34527320c,Accelerating Infinite Ensemble of Clustering by Pivot Features,2018,,10.1007/s12559-018-9583-8
c995f1f102e886192bdd6d29ac567f1c68cb03ad,Clustering ensemble selection considering quality and diversity,2018,,10.1007/s10462-018-9642-2
f8413d402f4aa45c2f517ec9f03c7264a49df2d0,Community detection in networks via nonlinear modularity eigenvectors,2017,"Revealing a community structure in a network or dataset is a central problem arising in many scientific areas. The modularity function $Q$ is an established measure quantifying the quality of a community, being identified as a set of nodes having high modularity. In our terminology, a set of nodes with positive modularity is called a \textit{module} and a set that maximizes $Q$ is thus called \textit{leading module}. Finding a leading module in a network is an important task, however the dimension of real-world problems makes the maximization of $Q$ unfeasible. This poses the need of approximation techniques which are typically based on a linear relaxation of $Q$, induced by the spectrum of the modularity matrix $M$. In this work we propose a nonlinear relaxation which is instead based on the spectrum of a nonlinear modularity operator $\mathcal M$. We show that extremal eigenvalues of $\mathcal M$ provide an exact relaxation of the modularity measure $Q$, however at the price of being more challenging to be computed than those of $M$. Thus we extend the work made on nonlinear Laplacians, by proposing a computational scheme, named \textit{generalized RatioDCA}, to address such extremal eigenvalues. We show monotonic ascent and convergence of the method. We finally apply the new method to several synthetic and real-world data sets, showing both effectiveness of the model and performance of the method.",10.1137/17M1144143
30c7e10fec9464c9745e1756acd1ca485509524b,Clustering Ensemble Selection Considering Quality and Diversity,2015,". Information clustering means classifying information or partitioning some samples in clusters such that samples inside each cluster have maximum similarity to each other and maximum distance from other clusters. As clustering is unsupervised, selecting a specific algorithm for clustering of an unknown set may fail. As a consequence of problem complexity and deficiencies in basic clustering methods, most of studies have focused on ensemble clustering methods in recent years. Diversity in initial results is one of the most important factors which may affect final quality of the results. Moreover, the quality of primary results affects the quality of final results. Both factors have been investigated in recent studies on clustering. Here, a new framework is proposed which is used for improving clustering efficiency and it is based on use of a subset of initial clusters. Selection of this subset plays a significant role in performance of the scheme. The subset is selected using two intelligent methods. The main idea in these methods is utilizing stable clusters through intelligent search algorithms. Two stability factors are utilized for cluster evaluation. One of these two stability factors is based on mutual information and the other one is based on Fisher measure. Finally, the selected clusters are added using several final combining methods. Practical results of several standard data sets demonstrate that the proposed method may improve combination clustering method significantly.",10.13053/RCS-102-1-8
761eebdd0c9fda978e316c220c54549978c3f987,New similarity index based on the aggregation of membership functions through OWA operator,2015,"In the field of data analysis, the use of metrics is a classical way to assess pairwise similarity. Unfortunately the popular distances are often inoperative because of the noise, the multidimensionality and the heterogeneous nature of data. These drawbacks lead us to propose a similarity index based on fuzzy set theory. Each object of the dataset is described with the vector of its fuzzy attributes. Thanks to aggregation operators, the object is fuzzified by using the fuzzy attributes. Thus each object becomes a fuzzy subset within the dataset. The similarity of a reference object compared to another one is assessed through the membership function of the fuzzified reference object and an aggregation method using OWA operator.",10.15439/2015F174
000651c8f5971dff64d5cbeafe7302a53895823e,Picture-to-Amount (PITA): Predicting Relative Ingredient Amounts from Food Images,2020,"Increased awareness of the impact of food consumption on health and lifestyle today has given rise to novel data-driven food analysis systems. Although these systems may recognize the ingredients, a detailed analysis of their amounts in the meal, which is paramount for estimating the correct nutrition, is usually ignored. In this paper, we study the novel and challenging problem of predicting the relative amount of each ingredient from a food image. We propose PITA, the Picture-to-Amount deep learning architecture to solve the problem. More specifically, we predict the ingredient amounts using a domain-driven Wasserstein loss from image-to-recipe cross-modal embeddings learned to align the two views of food data. Experiments on a dataset of recipes collected from the Internet show the model generates promising results and improves the baselines on this challenging task. A demo of our system and our data is available at: foodai.cs.rutgers.edu.",10.1109/ICPR48806.2021.9412828
0dc21823b956251f746415eef82aa6a77d42045e,Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning,2021,"Cross-modal recipe retrieval has recently gained substantial attention due to the importance of food in people’s lives, as well as the availability of vast amounts of digital cooking recipes and food images to train machine learning models. In this work, we revisit existing approaches for cross-modal recipe retrieval and propose a simplified end-to-end model based on well established and high performing encoders for text and images. We introduce a hierarchical recipe Transformer which attentively encodes individual recipe components (titles, ingredients and instructions). Further, we propose a self-supervised loss function computed on top of pairs of individual recipe components, which is able to leverage semantic relationships within recipes, and enables training using both image-recipe and recipe-only samples. We conduct a thorough analysis and ablation studies to validate our design choices. As a result, our proposed method achieves state-of-the-art performance in the cross-modal recipe retrieval task on the Recipe1M dataset. We make code and models publicly available 1.",10.1109/CVPR46437.2021.01522
0007dba348869daebdaa5f02dd120dde7ce1a305,Microprocessor Verification Using Efficient Decision Procedures for a Logic of Equality with Uninterpreted Functions,1999,,10.1007/3-540-48754-9_1
894f88289351fc5ade3ac0c1d5cc249c7c107391,Combination of Isabelle/HOL with Automatic Tools,2005,,10.1007/11559306_18
6b79f8f0566bada168403c18159ef4ef0206d799,A Framework for Microprocessor Correctness Statements,2001,,10.1007/3-540-44798-9_33
00085367adf8049b2091ac5e87b36a7eee7155ae,Modelling of switched reluctance motor drive by MICRO-CAP III,1993,"Researchers have been interested in the development and application of switched reluctance motors (SRM). In spite of their simple construction, the mathematical analysis of the steady-state and transient behaviour of machines is very complicated. Scientific discussions are carried out in many respects, e.g. related to the useful or detrimental character of magnetic saturation in this case. The accuracy of results achieved by the applied mathematical procedure is not satisfactory in every respect. The model of SRM drives on the basis of MICRO-CAP III CAD program is shown.<<ETX>>",10.1109/ISIE.1993.268794
b6b7fbf9b28b99f4e4ffd4e853260ffc3a926b16,Optimal control of current commutation of high speed SRM drive,2008,"The problem of optimal current commutation control, which bases on optimal off-line selection of switching angles, was investigated in the paper. Two different criteria of optimal control were taken into account: the maximum electromagnetic torque for given current and the minimum electromagnetic torque ripples. Optimization process was provided on base of computer model of SRM drive. Obtained simulation results were validated by experimental investigations.",10.1109/EPEPEMC.2008.4635432
91137c72a9a9ff30b8dd8fad6b712b087981f07e,Speed and position estimation of SRM,2008,The paper deals with the problem of speed and position estimation in SRM drive equipped with hysteresis band current controller. Instead of measured current the observer utilize reference current. The voltage is calculated from switching on-time. A speed observer structure which uses estimated back EMF and current of each phase is presented. The shaft position is integrated from estimated speed signal.,10.1109/EPEPEMC.2008.4635472
049034282f17a084b7e0a7839d8ddc3652468baa,Lagrange’s energy method based approach for switched reluctance drive systems modelling,2007,"Switched reluctance motors are often used nowadays in various industrial applications. Because of their principle of operation switched reluctance motors cannot work without a power electronic converter and a control system. Therefore not only design of individual components but also complex design of a complete switched reluctance drive system is very important. Complex approach is also necessary as far as switched reluctance drive system modelling is concerned. In the paper a method of modelling and mathematical model of a switched reluctance drive system are presented. The model derivation is based on circuit-oriented modelling methods and Lagrange's energy method. Coefficients of the Lagrange's equations have been calculated using the finite element method. The model, because of its method of derivation, is linear but allows for modelling asymmetries. Discussion on determination of model parameters for two cases - for fully symmetrical switched reluctance drive and for drive with mechanical and/or electrical asymmetries - is presented. Selected simulation results, verified against the experiment, are also presented.",10.1109/EPE.2007.4417277
7df8f1310faa6b63c17f19ef30a0871e726c78d6,Application of digital phase locked loop for control of SRM drive,2007,"High performance SRM drive requires precise information concerning instantaneous rotor position. Sensors mounted internally by manufacturers of SRM, are often inaccurate and of low resolution. The substantial improvement of both: resolution and accuracy has been achieved by application of unconventional all digital phase locked loop (ADPLL), which has been implemented in complex programmable logic device (CPLD). The ADPLL is fully synchronized with supervising DSP-based control system for high-performance SRM drive. Such solution is very flexible; the PLL can be easily tuned with DSP or re-programmed in purpose to perform various control strategies. Some proposals for sensorless control with ADPLL are also presented in the paper. Usually, equations describing position estimator or observer are calculated in software. Our approach enables time-critical equations resolved in hardware and there are no limits for high-speed operation.",10.1109/EPE.2007.4417738
0008afdfa123c7d32b644f5cebafae98a58e1bfa,Hyper-parameter Tuning of Federated Learning Based on Particle Swarm Optimization,2021,"The learning task of federated learning (FL) is solved by a federation of a center server and individual clients. Contrary to traditional deep learning models, federated learning consists of two parts, the global model and individual models. However, since the process is carried out by two parts of equal importance, it is ideal to deal with both parts simultaneously. To achieve superior performance, federated learning requires carefully selected hyper-parameters, which has more hyper-parameters than those of traditional deep learning models. To solve this tuning problem, we propose a method using particle swarm optimization (PSO) algorithm to tune the hyperparameters of federated learning. PSO algorithm is a gradient-free, stochastic optimization method which is better than grid search method when it comes to large search space. It helps locate the optimal combination of multiple parameters. In this article, we focus on applying PSO method on tuning the hyper-parameters of FL models, and prove that it is an efficient way to acquire satisfactory results. Experiments on MNIST dataset with convolution neural networks have proved the superiority of the proposed method.",10.1109/CCIS53392.2021.9754676
380fdeb36995ede563642f1b77a8859f95d9fc4d,FedGR: Genetic Algorithm and Relay Strategy Based Federated Learning,2024,"Federated learning (FL) is a privacy-preserving distributed machine learning approach that enables multiple parties to collaboratively train machine learning models without sharing local data. However, compared with the models trained on independent and identically distributed (IID) data, existing methods still face significant degradation in model performance when running on non-IID data. To solve this problem, we propose a federated learning framework based on genetic algorithm and relay strategy in this paper. The framework groups clients according to their local data distribution using genetic algorithm. After obtaining the optimal grouping result, a relay strategy is used to train and aggregate models within each group. Extensive experiments on three benchmark datasets show that FedGR significantly outperforms other state-of-the-art federated learning algorithms on various image classification tasks. The source code is available at https://github.com/zyfhylyh/FedGR.",10.1109/CSCWD61410.2024.10580045
8bc2f055c6e0a54f7a1915b96b4e8bf2bf89a10f,A multi-perspective revisit to the optimization methods of Neural Architecture Search and Hyper-parameter optimization for non-federated and federated learning environments,2023,,10.1016/j.compeleceng.2023.108867
430a13ce74e2cce8a5316fdc14d4aba7f651890b,Federated Learning Hyperparameter Tuning From a System Perspective,2022,"Federated learning (FL) is a distributed model training paradigm that preserves clients’ data privacy. It has gained tremendous attention from both academia and industry. FL hyperparameters (e.g., the number of selected clients and the number of training passes) significantly affect the training overhead in terms of computation time, transmission time, computation load, and transmission load. However, the current practice of manually selecting FL hyperparameters imposes a heavy burden on FL practitioners because applications have different training preferences. In this article, we propose FedTune, an automatic FL hyperparameter tuning algorithm tailored to applications’ diverse system requirements in FL training. FedTune iteratively adjusts FL hyperparameters during FL training and can be easily integrated into existing FL systems. Through extensive evaluations of FedTune for diverse applications and FL aggregation algorithms, we show that FedTune is lightweight and effective, achieving 8.48%–26.75% system overhead reduction compared to using fixed FL hyperparameters. This article assists FL practitioners in designing high-performance FL training solutions. The source code of FedTune is available at https://github.com/DataSysTech/FedTune.",10.1109/JIOT.2023.3253813
0008cb017b9659277632d26566e512a0bdfde553,Adaptive Variance Based Label Distribution Learning for Facial Age Estimation,2020,,10.1007/978-3-030-58592-1_23
229530a78e6c5c1f135f2c85ecc1b5e25e6bb463,Teach CLIP to Develop a Number Sense for Ordinal Regression,2024,"Ordinal regression is a fundamental problem within the field of computer vision, with customised well-trained models on specific tasks. While pre-trained vision-language models (VLMs) have exhibited impressive performance on various vision tasks, their potential for ordinal regression has received less exploration. In this study, we first investigate CLIP's potential for ordinal regression, from which we expect the model could generalise to different ordinal regression tasks and scenarios. Unfortunately, vanilla CLIP fails on this task, since current VLMs have a well-documented limitation of encapsulating compositional concepts such as number sense. We propose a simple yet effective method called NumCLIP to improve the quantitative understanding of VLMs. We disassemble the exact image to number-specific text matching problem into coarse classification and fine prediction stages. We discretize and phrase each numerical bin with common language concept to better leverage the available pre-trained alignment in CLIP. To consider the inherent continuous property of ordinal regression, we propose a novel fine-grained cross-modal ranking-based regularisation loss specifically designed to keep both semantic and ordinal alignment in CLIP's feature space. Experimental results on three general ordinal regression tasks demonstrate the effectiveness of NumCLIP, with 10% and 3.83% accuracy improvement on historical image dating and image aesthetics assessment task, respectively. Code is publicly available at https://github.com/xmed-lab/NumCLIP.",10.48550/arXiv.2408.03574
a5ff51640fcf48cbd8f73b8e3b236adae66ac2e2,Multi-threshold Deep Metric Learning for Facial Expression Recognition,2024,,10.1016/j.patcog.2024.110711
4c8da4fa2c8102a7b7dad024f85c54b49ba0636b,DADL: Double Asymmetric Distribution Learning for head pose estimation in wisdom museum,2023,,10.1016/j.jksuci.2023.101869
10dcde63c6ce45a850018a52d32bcec621038b21,Ranking-preserved generative label enhancement,2023,,10.1007/s10994-023-06388-9
7b494e3e4fbc1d2fb3eeabe2ceb4b288d027470f,Label Enhancement via Joint Implicit Representation Clustering,2023,"Label distribution is an effective label form to portray label polysemy (i.e., the cases that an instance can be described by multiple labels simultaneously). However, the expensive annotating cost of label distributions limits its application to a wider range of practical tasks. Therefore, LE (label enhancement) techniques are extensively studied to solve this problem. Existing LE algorithms mostly estimate label distributions by the instance relation or the label relation. However, they suffer from biased instance relations, limited model capabilities, or suboptimal local label correlations. Therefore, in this paper, we propose a deep generative model called JRC to simultaneously learn and cluster the joint implicit representations of both features and labels, which can be used to improve any existing LE algorithm involving the instance relation or local label correlations. Besides, we develop a novel label distribution recovery module, and then integrate it with JRC model, thus constituting a novel generative label enhancement model that utilizes the learned joint implicit representations and instance clusters in a principled way. Finally, extensive experiments validate our proposal.",10.24963/ijcai.2023/447
b57611c4ad704d2a9c44fad3bd1e062bc10e3d87,General vs. Long-Tailed Age Estimation: An Approach to Kill Two Birds With One Stone,2023,"Facial age estimation has received a lot of attention for its diverse application scenarios. Most existing studies treat each sample equally and aim to reduce the average estimation error for the entire dataset, which can be summarized as General Age Estimation. However, due to the long-tailed distribution prevalent in the dataset, treating all samples equally will inevitably bias the model toward the head classes (usually the adult with a majority of samples). Driven by this, some works suggest that each class should be treated equally to improve performance in tail classes (with a minority of samples), which can be summarized as Long-tailed Age Estimation. However, Long-tailed Age Estimation usually faces a performance trade-off, i.e., achieving improvement in tail classes by sacrificing the head classes. In this paper, our goal is to design a unified framework to perform well on both tasks, killing two birds with one stone. To this end, we propose a simple, effective, and flexible training paradigm named GLAE, which is two-fold. First, we propose Feature Rearrangement (FR) and Pixel-level Auxiliary learning (PA) for better feature utilization to improve the overall age estimation performance. Second, we propose Adaptive Routing (AR) for selecting the appropriate classifier to improve performance in the tail classes while maintaining the head classes. Moreover, we introduce a new metric, named Class-wise Mean Absolute Error (CMAE), to equally evaluate the performance of all classes. Our GLAE provides a surprising improvement on Morph II, reaching the lowest MAE and CMAE of 1.14 and 1.27 years, respectively. Compared to the previous best method, MAE dropped by up to 34%, which is an unprecedented improvement, and for the first time, MAE is close to 1 year old. Extensive experiments on other age benchmark datasets, including CACD, MIVIA, and Chalearn LAP 2015, also indicate that GLAE outperforms the state-of-the-art approaches significantly.",10.1109/TIP.2023.3327540
ae29578c017e6ee5a6f3d829e8929168b1844126,Ord2Seq: Regarding Ordinal Regression as Label Sequence Prediction,2023,"Ordinal regression refers to classifying object instances into ordinal categories. It has been widely studied in many scenarios, such as medical disease grading and movie rating. Known methods focused only on learning inter-class ordinal relationships, but still incur limitations in distinguishing adjacent categories thus far. In this paper, we propose a simple sequence prediction framework for ordinal regression called Ord2Seq, which, for the first time, transforms each ordinal category label into a special label sequence and thus regards an ordinal regression task as a sequence prediction process. In this way, we decompose an ordinal regression task into a series of recursive binary classification steps, so as to subtly distinguish adjacent categories. Comprehensive experiments show the effectiveness of distinguishing adjacent categories for performance improvement and our new approach exceeds state-of-the-art performances in four different scenarios. Codes are available at https://github.com/wjh892521292/Ord2Seq.",10.1109/ICCV51070.2023.00539
a4f7df958aa56eac6cdcd7aa6c25c66726e44758,Parameter Estimation Via Expectation Maximization - Expectation Consistent Algorithm,2024,"In the context of the expectation-maximization (EM) algorithm, which often faces challenges due to intractable posterior distributions, this study explores an innovative approach by integrating the EM algorithm with expectation consistent (EC) approximate inference. Our method involves the incorporation of the EC algorithm into the M-step of the EM algorithm, resulting in the EM-EC algorithm. We demonstrate that the fixed points of the proposed EM-EC algorithm correspond to stationary points of a specific constrained auxiliary function, thereby providing a variational interpretation of the algorithm. Through simulations, we showcase the effectiveness and robustness of this novel approach, highlighting its potential for advancing the field of Bayesian network estimation.",10.1109/ICASSP48485.2024.10447082
7635dd17557475edb06d91599d9a9bfc9c961cd5,Fast and flexible joint fine-mapping of multiple traits via the Sum of Single Effects model,2023,"We introduce mvSuSiE, a multi-trait fine-mapping method for identifying putative causal variants from genetic association data (individual-level or summary data). mvSuSiE learns patterns of shared genetic effects from data, and exploits these patterns to improve power to identify causal SNPs. Comparisons on simulated data show that mvSuSiE is competitive in speed, power and precision with existing multi-trait methods, and uniformly improves on single-trait fine-mapping (SuSiE) in each trait separately. We applied mvSuSiE to jointly fine-map 16 blood cell traits using data from the UK Biobank. By jointly analyzing the traits and modeling heterogeneous effect sharing patterns, we discovered a much larger number of causal SNPs (>3,000) compared with single-trait fine-mapping, and with narrower credible sets. mvSuSiE also more comprehensively characterized the ways in which the genetic variants affect one or more blood cell traits; 68% of causal SNPs showed significant effects in more than one blood cell type.",10.1101/2023.04.14.536893
d8cfdb075bf76e1ba6279337ac975a1b0d3d55bb,"A simple new approach to variable selection in regression, with application to genetic fine-mapping",2018,"We introduce a simple new approach to variable selection in linear regression, and to quantifying uncertainty in selected variables. The approach is based on a new model – the “Sum of Single Effects” (SuSiE) model – which comes from writing the sparse vector of regression coefficients as a sum of “single-effect” vectors, each with one non-zero element. We also introduce a corresponding new fitting procedure – Iterative Bayesian Step-wise Selection (IBSS) – which is a Bayesian analogue of traditional stepwise selection methods. IBSS shares the computational simplicity and speed of traditional stepwise methods, but instead of selecting a single variable at each step, IBSS computes a distribution on variables that captures uncertainty in which variable to select. We show that the IBSS algorithm computes a variational approximation to the posterior distribution under the SuSiE model. Further, this approximate posterior distribution naturally leads to a convenient, novel, way to summarize uncertainty in variable selection, and provides a Credible Set for each selected variable. Our methods are particularly well suited to settings where variables are highly correlated and true effects are very sparse, both of which are characteristics of genetic fine-mapping applications. We demonstrate through numerical experiments that our methods outperform existing methods for this task, and illustrate the methods by fine-mapping genetic variants that influence alternative splicing in human cell-lines. We also discuss both the potential and the challenges for applying these methods to generic variable selection problems.",10.1111/rssb.12388
26baef4b4409858dd43f89db2394ea47784f845a,The design and analysis of real-time systems using the ASTRAL software development environment,1999,,10.1023/A:1018934104631
0009d673b46c3f79835ac13e5a987b3c5e153628,ROIAL: Region of Interest Active Learning for Characterizing Exoskeleton Gait Preference Landscapes,2020,"Characterizing what types of exoskeleton gaits are comfortable for users, and understanding the science of walking more generally, require recovering a user’s utility landscape. Learning these landscapes is challenging, as walking trajectories are defined by numerous gait parameters, data collection from human trials is expensive, and user safety and comfort must be ensured. This work proposes the Region of Interest Active Learning (ROIAL) framework, which actively learns each user’s underlying utility function over a region of interest that ensures safety and comfort. ROIAL learns from ordinal and preference feedback, which are more reliable feedback mechanisms than absolute numerical scores. The algorithm’s performance is evaluated both in simulation and experimentally for three non-disabled subjects walking inside of a lower-body exoskeleton. ROIAL learns Bayesian posteriors that predict each exoskeleton user’s utility landscape across four exoskeleton gait parameters. The algorithm discovers both commonalities and discrepancies across users’ gait preferences and identifies the gait parameters that most influenced user feedback. These results demonstrate the feasibility of recovering gait utility landscapes from limited human trials.",10.1109/ICRA48506.2021.9560840
0338dabad410f2cd01696078ba1d13dec10b0800,A Generalized Acquisition Function for Preference-based Reward Learning,2024,"Preference-based reward learning is a popular technique for teaching robots and autonomous systems how a human user wants them to perform a task. Previous works have shown that actively synthesizing preference queries to maximize information gain about the reward function parameters improves data efficiency. The information gain criterion focuses on precisely identifying all parameters of the reward function. This can potentially be wasteful as many parameters may result in the same reward, and many rewards may result in the same behavior in the downstream tasks. Instead, we show that it is possible to optimize for learning the reward function up to a behavioral equivalence class, such as inducing the same ranking over behaviors, distribution over choices, or other related definitions of what makes two rewards similar. We introduce a tractable framework that can capture such definitions of similarity. Our experiments in a synthetic environment, an assistive robotics environment with domain transfer, and a natural language processing problem with real datasets demonstrate the superior performance of our querying method over the state-of-the-art information gain method.",10.1109/ICRA57147.2024.10611472
5fbbacd98613c917920a10d0626391b513673e80,Principled Preferential Bayesian Optimization,2024,"We study the problem of preferential Bayesian optimization (BO), where we aim to optimize a black-box function with only preference feedback over a pair of candidate solutions. Inspired by the likelihood ratio idea, we construct a confidence set of the black-box function using only the preference feedback. An optimistic algorithm with an efficient computational method is then developed to solve the problem, which enjoys an information-theoretic bound on the total cumulative regret, a first-of-its-kind for preferential BO. This bound further allows us to design a scheme to report an estimated best solution, with a guaranteed convergence rate. Experimental results on sampled instances from Gaussian processes, standard test functions, and a thermal comfort optimization problem all show that our method stably achieves better or competitive performance as compared to the existing state-of-the-art heuristics, which, however, do not have theoretical guarantees on regret bounds or convergence.",10.48550/arXiv.2402.05367
ccd0a61325d74e037ef7ae2674a5eebdb20877e1,Active Reward Learning from Online Preferences,2023,"Robot policies need to adapt to human preferences and/or new environments. Human experts may have the domain knowledge required to help robots achieve this adaptation. However, existing works often require costly offline re-training on human feedback, and those feedback usually need to be frequent and too complex for the humans to reliably provide. To avoid placing undue burden on human experts and allow quick adaptation in critical real-world situations, we propose designing and sparingly presenting easy-to-answer pairwise action preference queries in an online fashion. Our approach designs queries and determines when to present them to maximize the expected value derived from the queries' information. We demonstrate our approach with experiments in simulation, human user studies, and real robot experiments. In these settings, our approach outperforms baseline techniques while presenting fewer queries to human experts. Experiment videos, code and appendices are found on our website: http://tinyurl.com/online-active",10.1109/ICRA48891.2023.10160439
000a00d829abda9262013a88a5356953f449fc73,Opinion mining and sentiment classification: A review,2017,"With the evolution of web technology, there is a huge amount of data present in the web for the internet users. These users not only use the available resources in the web, but also give their feedback, thus generating additional useful information. Due to overwhelming amount of user's opinions, views, feedback and suggestions available through the web resources, it's very much essential to explore, analyze and organize their views for better decision making. Opinion Mining or Sentiment Analysis is a Natural Language Processing and Information Extraction task that identifies the user's views or opinions explained in the form of positive, negative or neutral comments and quotes underlying the text. Various supervised or data-driven techniques to Sentiment analysis like Naïve Byes, Maximum Entropy and SVM. For classification use support vector machine (SVM), it performs the sentiment classification task also consider sentiment classification accuracy.",10.1109/ICISC.2017.8068637
f6464f77281201e5fb811eaacc61d1497524fc8b,Sentiment Analysis towards Actionable Intelligence via Deep Learning,2020,"The exponential growth of unstructured data and the ability of businesses to utilize such data in decision-making have led to competitive advantages. The knowledge provided by analyzing unstructured data is crucial for product developers or service providers because it might affect the sustainability of the business. Sentiment analysis is used to gain an understanding of the attitudes, opinions, and emotions expressed within an online review. Naïve Bayes (NB), logistic regression (LR), decision trees (DT), deep learning (DL), and support vector machines (SVM) were used to build a classification model. In the data mining settings, the classification accuracy is the best metric to highlight the best classifier. The DL classifier outperformed other models in terms of accuracy rate. Classifying customers' feelings toward a product or service is critical for providing actionable insights. Utilizing such models will help to analyze huge volumes of reviews, saving both time and costs.",10.18421/tem94-44
16fd3947cefbf8cd51d8a82832c184f1351447e6,Reconstructing Strokes and Writing Sequences from Chinese Character Images,2007,"The Chinese characters evolved from pictograms and they are composed of strokes. A standard stroke sequence for each character is available in the dictionary. People introduced heuristic rules to specify the stroke order for easy memorization but it is very ambiguous to reconstruct the dictionary sequence according to the heuristic rules. In this paper, we combine the stroke extraction and stroke sequence reconstruction algorithms to reconstruct the strokes and their sequence from a Chinese character image. A well-known public Chinese character database (the HITPU database) is used as our input data. Performance evaluation shows the robustness of our proposed method and user evaluation shows that our proposed system helps users to create online Chinese character templates quickly and conveniently.",10.1109/ICMLC.2007.4370133
2bad667e45c76b6edf8e7fe54f6cdd267189c743,Offline Chinese handwriting recognition: an assessment of current technology,2007,,10.1007/s11704-007-0015-2
1b0cda4e00db8567142379549dc0a53190bc77b6,Reconstructing the Correct Writing Sequence from a Set of Chinese Character Strokes,2006,,10.1007/11940098_34
f2d7849571599130272e79f20197b4b6ccb89da9,Web based Chinese Calligraphy Learning with 3-D Visualization Method,2006,Chinese calligraphy is pictographic and each calligraphist has his own writing style. People often feel difficult in writing a demanded beautiful calligraphy style. In order to help people enjoy the art of calligraphy and learn how it is written step-by-step we present a new approach to animate its writing process by 3-D visualization method. In this paper some novel algorithms used in the approach are presented to solve the following problems: 1) estimate varied stroke's thickness 2) extract strokes order from an offline Chinese calligraphic writing. Through this approach we implement a system. Experimental result is given to demonstrate the application finally,10.1109/ICME.2006.262642
a4dacead25704c5dad8828e2c9cd5bf5d85deccb,'Online recognition of Chinese characters: the state-of-the-art,2004,"Online handwriting recognition is gaining renewed interest owing to the increase of pen computing applications and new pen input devices. The recognition of Chinese characters is different from western handwriting recognition and poses a special challenge. To provide an overview of the technical status and inspire future research, this paper reviews the advances in online Chinese character recognition (OLCCR), with emphasis on the research works from the 1990s. Compared to the research in the 1980s, the research efforts in the 1990s aimed to further relax the constraints of handwriting, namely, the adherence to standard stroke orders and stroke numbers and the restriction of recognition to isolated characters only. The target of recognition has shifted from regular script to fluent script in order to better meet the requirements of practical applications. The research works are reviewed in terms of pattern representation, character classification, learning/adaptation, and contextual processing. We compare important results and discuss possible directions of future research.",10.1109/TPAMI.2004.1262182
000ab88c22b8501af71f91b4b965a19d0da1a1c8,Investigation of electromagnetic performance of salient-pole synchronous reluctance machines having different concentrated winding connections,2013,"This paper presents a comparative study of the electromagnetic performance of 6-stator/4-rotor pole switched reluctance (SR) machines having AC sinusoidal bipolar excitation which is equivalent to salient-pole synchronous reluctance (SynR) machines having different non-overlapping concentrated winding connections, i.e. asymmetric, symmetrical and hybrid. The output torque of such machines is mainly generated by the self and mutual inductances, which are principally highlighted in this paper. In addition, the average value and quality of output torque and phase and line voltage waveforms are also predicted and compared. During the investigations the influence of the magnetic saturation on the machine performances is particularly examined. In order to confirm and verify the predicted results, a prototype is constructed and tested under the three different winding connections.",10.1109/IEMDC.2013.6556276
8920b3deb590c3082a8372cabe68e6ac67922708,Comparative researches on double‐sided switched reluctance linear machines with different winding connections,2020,": This study presents comparative researches on performances of a double-sided switched reluctance linear machine (DSRLM) under different winding connection modes. The static self/mutual inductances of a 6/4 yokeless DSRLM under two winding connection modes are calculated through the finite element method (FEM). This DSRLM can work either as a motor or as a generator. Now the performances of the DSRLM under different winding connection modes, including the force ripple in the electric model and the output voltage ripple in generation model, are compared through an established electric model and an established generation model. In addition, the iron losses under different winding connections are also investigated. Dynamic magnetic flux density waveforms of the DSRLM are calculated via FEM, and an iron loss computation module is established. Simulation results indicate that the advantages of both winding connections reflect in different aspects. This study summarises how to select a proper winding connection mode of DSRLMs for different performance demands responding to different applications. The experimental validations are conducted on a prototype DSRLM finally.",10.1049/iet-epa.2020.0072
f9ac8a623f436b52e744fe79f45f457ced878618,An improved fractional slot concentrated winding for low-poles induction machines,2016,"Recent investigations on IMs with FSCWs show that, due to the higher MMF harmonic content of concentrated windings, this machine type is characterized by low torque capability and quality, as well as, low efficiency and thermal problems. This paper presents a new fundamental wave FSCW with sinusoidal MMF waveform and a non-overlapping winding arrangement. The proposed winding solution consists of two different FSCW types and a dual slot-layer stator structure. The first winding is located on the outer slot-layer region, while the second one is located on the inner slot-layer. Additionally, both windings are shifted in space to each other for a specific angle. This winding configuration has several advantages like high quality MMF wave with low harmonic contents, shorter end-winding length, and applicability to low poles IMs. Obtained results from the analysis of different IMs show that the proposed FSCW provides high performances in different application areas.",10.1109/ICELMACH.2016.7732514
ce150f1082cefcdb82845fefef07fb01321d33c7,Comparative Study of Torque Production in Conventional and Mutually Coupled SRMs Using Frozen Permeability,2016,"This paper investigates the influence of mutual fluxes (inductances) on the resultant torque in three-phase conventional switched-reluctance machine (CSRM) and mutually coupled SRM (MCSRM) using the frozen permeability (FP) method. Under saturation conditions, the FP method allows accurately separating the torques due to self-flux and mutual flux, hence quantifying their contributions to torque generation. Then, appropriate current waveforms (unipolar or bipolar, square wave or sinewave) can be established to maximize the output torques. It is well known that the mutual torque of CSRM can be negligible. However, this paper has shown that when sinewave current is employed and under full or overload conditions, the torque will be significantly reduced due to non-negligible negative mutual torques. Different from CSRM, the self-torque and the mutual torque of MCSRM can be added if current waveform is properly chosen, e.g., sinewave currents. This can significantly boost the resultant torque. The predictions have been validated by experiments.",10.1109/TMAG.2016.2516967
d073ba490280df2ade4e70e969896a62e6d063ce,Design of a switched reluctance machine assisted by DC field windings for a turbo blower,2015,"A switched reluctance machine (SRM) is applicable to high speed applications due to its simple magnetic structure and mechanical robustness. During the operation of a converter, however, it causes high peak phase current which leads to a rise in the VA rating of an SRM system. Therefore, this paper investigates the reduction of peak phase current and describes the basic design procedure of an SRM along with a converter topology of the machine assisted by DC field windings. Physical dimensions are first determined, and then the field windings are employed in the SRM without changing copper area and copper weight. Also, a commutation method is investigated by activating the field windings matching with switching angles in phase windings in order to achieve both low peak current and high efficiency at the same time. By doing so, the dynamic performance of an SRM assisted by DC field windings is estimated and compared to conventional symmetric and asymmetric winding configurations. It is concluded that the criterion of peak current reduction and efficiency improvement is met by matching the turn-off angle of field windings based on that of phase windings.",10.1109/ECCE.2015.7310444
2e4561251785cf38d691d441a7e429e55a9dd9ac,Hybrid-Excited Doubly Salient Synchronous Machine With Permanent Magnets Between Adjacent Salient Stator Poles,2015,"A novel stator hybrid-excited, parallel flux path, synchronous machine of doubly salient topology is proposed. It has the novel features of: 1) hybrid dc field and permanent magnet (PM) excitation in the stator; 2) magnets placed in the slots between adjacent salient stator poles; and 3) the magnetic poles of PMs arranged, such that the flux premagnetizes the stator, but it is in a direction to oppose the dc excitation flux. The electromagnetic characteristics of the machine are analyzed on open-circuit and load. Since the new machine topology is developed from the variable-flux machine (VFM), a comparison of their electromagnetic torque and machine losses is conducted. The average electromagnetic torque of the hybrid-excited machine can be increased by 18% for fixed copper loss in comparison with the VFM due to the reduction of magnetic saturation in the stator. It is also shown that at high temperatures some risk of PM demagnetization exists when excited with dc and armature currents due to fringing flux that exists between the stator and the rotor poles. However, this affects only a very small area of the PMs. The performance of the hybrid-excited machine is predicted by a 2-D finite-element analysis and experimentally validated.",10.1109/TMAG.2015.2446951
3e696e53734615c4fd43ebeccc66ab1ee924a66c,Analysis of different PM machines with concentrated windings and flux barriers in stator core,2014,"The new stator structure with magnetic flux-barriers in the stator yoke or tooth region represents an efficient method for reducing the sub-harmonics of electric machines with fractional slots, tooth-concentrated windings. In this paper the both flux-barriers techniques are considered during the analysis of different PM machines. The 12-teeth single-layer and double-layer concentrated winding in combination with a 10-poles and 14-poles PM rotor are investigated. For the all machine topologies the new stator design is used to improve their performances and characteristics. The flux-barrier effects on the main machine parameters, such as in the air-gap flux density harmonics, dq-machine parameters, characteristic currents, electromagnetic torque, and so on, are studied carefully. Comparisons performed with the analogous conventional machines (with conventional stator) show that, the new stator design offers significant advantages.",10.1109/ICELMACH.2014.6960208
d5d307e4e6653b6d43464c3c517032340b0e0763,Vector control specialized for switched reluctance motor drives,2014,"A vector controlled-switched reluctance motor (SRM) drive system is proposed in this paper. As the unipolar excitation current, the sinusoidal current with a DC offset is applied to each circuit in the proposed technique. The excitation current consists of the DC and AC components. These components generate the (virtual) rotor flux and rotating stator field, hence, the vector control system for the SRM drive can be developed in the same way as conventional AC machines. The proposed technique provides the precise instantaneous torque control, linear torque-current control, and maximum torque per ampere (MTPA) control methods. The SRM drive can be easily controlled by a single parameter under the MTPA condition. The proposed vector control is verified by performing some experimental tests on a three-phase SRM.",10.1002/eej.22776
000ac127a9e270e66d99d77dd6dbf4bf5186ce52,Improvement in Software Defect Prediction Outcome Using Principal Component Analysis and Ensemble Machine Learning Algorithms,2018,,10.1007/978-3-030-03146-6_44
445c71912a718b6add8b79d01ee02aaadb51a5cb,A survey on machine learning techniques applied to source code,2023,,10.1016/j.jss.2023.111934
643eead5e4d0bfa8ae87311d2160535e05790f55,Data quality issues in software fault prediction: a systematic literature review,2022,,10.1007/s10462-022-10371-6
8f4ab8fd013ec000ef4c092ad80673543ace5aec,Software defect prediction using K-PCA and various kernel-based extreme learning machine: an empirical study,2020,": Predicting defects during software testing reduces an enormous amount of testing effort and help to deliver a high-quality software system. Owing to the skewed distribution of public datasets, software defect prediction (SDP) suffers from the class imbalance problem, which leads to unsatisfactory results. Overfitting is also one of the biggest challenges for SDP. In this study, the authors performed an empirical study of these two problems and investigated their probable solution. They have conducted 4840 experiments over five different classifiers using eight NASA projects and 14 PROMISE repository datasets. They suggested and investigated the varying kernel function of an extreme learning machine (ELM) along with kernel principal component analysis (K-PCA) and found better results compared with other classical SDP models. They used the synthetic minority oversampling technique as a sampling method to address class imbalance problems and k-fold cross-validation to avoid the overfitting problem. They found ELM-based SDP has a high receiver operating characteristic curve over 11 out of 22 datasets. The proposed model has higher precision and F -score values over ten and nine, respectively, compared with other state-of-the-art models. The Mathews correlation coefficient (MCC) of 17 datasets of the proposed model surpasses other classical models' MCC.",10.1049/iet-sen.2020.0119
870f2c241e59ac83d84c5bff8a4c56fe39e90357,Novel Grey Relational Feature Extraction Algorithm for Software Fault-Proneness Using BBO (B-GRA),2020,,10.1007/s13369-020-04445-2
d3fc9555b45cbf3f21118fd22e530cf005c8bce6,Software Defect Prediction Using Ensemble Learning: A Systematic Literature Review,2021,"Recent advances in the domain of software defect prediction (SDP) include the integration of multiple classification techniques to create an ensemble or hybrid approach. This technique was introduced to improve the prediction performance by overcoming the limitations of any single classification technique. This research provides a systematic literature review on the use of the ensemble learning approach for software defect prediction. The review is conducted after critically analyzing research papers published since 2012 in four well-known online libraries: ACM, IEEE, Springer Link, and Science Direct. In this study, five research questions covering the different aspects of research progress on the use of ensemble learning for software defect prediction are addressed. To extract the answers to identified questions, 46 most relevant papers are shortlisted after a thorough systematic research process. This study will provide compact information regarding the latest trends and advances in ensemble learning for software defect prediction and provide a baseline for future innovations and further reviews. Through our study, we discovered that frequently employed ensemble methods by researchers are the random forest, boosting, and bagging. Less frequently employed methods include stacking, voting and Extra Trees. Researchers proposed many promising frameworks, such as EMKCA, SMOTE-Ensemble, MKEL, SDAEsTSE, TLEL, and LRCR, using ensemble learning methods. The AUC, accuracy, F-measure, Recall, Precision, and MCC were mostly utilized to measure the prediction performance of models. WEKA was widely adopted as a platform for machine learning. Many researchers showed through empirical analysis that features selection, and data sampling was necessary pre-processing steps that improve the performance of ensemble classifiers.",10.1109/ACCESS.2021.3095559
c0805c0f272f253207e02cc54076722084f41b81,Predominant Musical Instrument Classification based on Spectral Features,2019,"This work aims to examine one of the cornerstone problems of Musical Instrument Retrieval (MIR), in particular, instrument classification. IRMAS (Instrument recognition in Musical Audio Signals) data set is chosen for this purpose. The data includes musical clips recorded from various sources in the last century, thus having a wide variety of audio quality. We have presented a very concise summary of past work in this domain. Having implemented various supervised learning algorithms for this classification task, SVM classifier has outperformed the other state-of-the-art models with an accuracy of 79%. We also implemented Unsupervised techniques out of which Hierarchical Clustering has performed well.",10.1109/SPIN48934.2020.9071125
1968d81206a3345bc41566213fe63e15ac6ff777,Music instrument recognition using deep convolutional neural networks,2019,,10.1007/s41870-019-00285-y
e0cbad8f82752d6c8dfd5d866e70d56125a09590,An Effective Feature Calculation For Analysis & Classification of Indian Musical Instruments Using Timbre Measurement,2015,"Musical instrument recognition is significant field in the research of computer music which is related to the modelling of sounds. Analysing & synthesing the structure of musical note is of importance both for modelling music signals and their automatic computer-based recognition.
 Musical sound is produce by five dimensions: pitch, loudness, duration, spatialization, and timbre. First four parameter can be controlled but timbre remains difficult. Timbre then naturally became the main subject of this work. It is important property of sound that separate one music instrument from another and independent of pitch and volume. This work presents a system for identifying a specific musical instrument from monophonic recordings. The system proposed in this paper has been trained and tested with three Indian musical instruments samples. Instruments include flute, harmonium and sitar, which are most commonly used in Indian classical music. The Statistical and spectral parameter are used for the classification of the sounds in Indian Musical Instruments. The SVM classifier proves its ability in automatic and accurate classification of Indian Musical Instrument. Using separately recorded notes as test sets, we were able to achieve average accuracy as high as 88.88 % for SVM that deciding if a note is played by the sitar or others.",10.1145/2818567.2818586
000b6bd77cb3b90c436b1e77b69354f5b41adb0e,TLRec:Transfer Learning for Cross-Domain Recommendation,2017,"In the era of big data, the available information on the Internet has overwhelmed the human processing capabilities in some commercial applications. Recommendation techniques are indispensable to predict user ratings on items in terms of historical data and deal with the information overload. In many applications, the problem of data sparsity usually results in overfitting and fails to give desirable performance. Therefore, many works have started to investigate the techniques of cross-domain recommendation to overcome the challenge. However, it is not trivial. In this paper, we propose a transfer learning algorithm, named TLRec, for cross-domain recommendation, which exploits the overlapped users and items as a bridge to link different domains and implements knowledge transfer. We learn parameters based on the defined empirical prediction error, smoothness and regularization of user and item latent vectors. We also establish a relation between TLRec and vertex vectoring on bipartite graphs. The experimental result illustrates that TLRec has promising performance and outperforms several state-of-the art approaches on a real dataset.",10.1109/ICBK.2017.30
186aeaf08e545a62fd975ac56068d9b5f2043ad9,Data Scarcity in Recommendation Systems: A Survey,2023,"The prevalence of online content has led to the widespread adoption of recommendation systems (RSs), which serve diverse purposes such as news, advertisements, and e-commerce recommendations. Despite their significance, data scarcity issues have significantly impaired the effectiveness of existing RS models and hindered their progress. To address this challenge, the concept of knowledge transfer, particularly from external sources like pre-trained language models, emerges as a potential solution to alleviate data scarcity and enhance RS development. However, the practice of knowledge transfer in RSs is intricate. Transferring knowledge between domains introduces data disparities, and the application of knowledge transfer in complex RS scenarios can yield negative consequences if not carefully designed. Therefore, this article contributes to this discourse by addressing the implications of data scarcity on RSs and introducing various strategies, such as data augmentation, self-supervised learning, transfer learning, broad learning, and knowledge graph utilization, to mitigate this challenge. Furthermore, it delves into the challenges and future direction within the RS domain, offering insights that are poised to facilitate the development and implementation of robust RSs, particularly when confronted with data scarcity. We aim to provide valuable guidance and inspiration for researchers and practitioners, ultimately driving advancements in the field of RS.",10.1145/3639063
ef8b54c52d391b3c06099aa2265bfcd159088d24,Provisioning a cross-domain recommender system using an adaptive adversarial network model,2023,,10.1007/s00500-023-09360-w
472a2bcc097f367686ac2edffff3d720babfea36,A cross-platform recommendation system from Facebook to Instagram,2023,"
Purpose
The purpose of this paper is to provide a cross-platform recommendation system that recommends the most suitable public Instagram accounts to Facebook users.


Design/methodology/approach
We collect data from both Facebook and Instagram and then propose a similarity matching mechanism for recommending the most appropriate Instagram accounts to Facebook users. By removing the data disparity between the two heterogeneous platforms and integrating them, the system is able to make more accurate recommendations.


Findings
The results show that the method proposed in this paper can recommend suitable public Instagram accounts to Facebook users with very high accuracy.


Originality/value
To the best of the authors’ knowledge, this is the first study to propose a recommender system to recommend Instagram public accounts to Facebook users. Second, our proposed method can integrate heterogeneous data from two different platforms to generate collaborative recommendations. Furthermore, our cross-platform system reveals an innovative concept of how multiple platforms can promote their respective platforms in a unified, cooperative and collaborative manner.
",10.1108/el-09-2022-0210
cd635c8052a68b18b1e5c16107fa688289651f1a,CFCR: A Convolution and Fusion Model for Cross-platform Recommendation,2021,"With the emergence of various online platforms, associating different platforms is playing an increasingly important role in many applications. Cross-platform recommendation aims to improve recommendation accuracy through associating information from different platforms. Existing methods do not fully exploit high-order nonlinear connectivity information in cross-domain recommendation scenario and suffer from domain-incompatibility problem. In this paper, we propose an end-to-end convolution and fusion model for cross-platform recommendation (CFCR). The proposed CFCR model utilizes Graph Convolution Networks (GCN) to extract user and item features on graphs from different platforms, and fuses cross-platform information by Multimodal AutoEncoder (MAE) with common latent user features. Therefore, the high-order connectivity information is preserved to the most extent and domain-invariant user representations are automatically obtained. The domain-incompatible information is spontaneously discarded to avoid messing up the cross-platform association. Extensive experiments for the proposed CFCR model on real-world dataset demonstrate its advantages over existing cross-platform recommendation methods in terms of various evaluation metrics.",10.1145/3469877.3495639
39f451a22837a0fce6946c7df3c8a7cab11771ea,Collaborative Filtering Recommendation Based on Multi-Domain Semantic Fusion,2020,"Collaborative filtering based on single domains has become widely used in today's recommendation system. Nevertheless, it has two problems that need to be solved, i.e., the cold start problem and the data sparseness problem. As the result, cross-domain recommendation technology has emerged, which aims at integrating user preference characteristics from different domains. This paper proposes a collaborative filtering recommendation method based on multi-domain semantic fusion (CF-MDS). CF-MDS achieves cross-domain item similarity calculation through semantic analysis and ontology and integrates data from different domains iteratively based on domain relevance to rate users on target domain items and to produce a cross-domain user-item rating matrix. Collaborative filtering technology is then combined with multi-domain fusion recommendation algorithm. Experimental results show that the proposed method can deal effectively with the cold start problem and data sparsity problem that exist in traditional recommendation systems as well as can improve the diversity of recommendation. Compared to other cross-domain recommendation methods, the proposed method can better meet personal needs of users and also improve the accuracy of recommendation.",10.1109/COMPSAC48688.2020.00041
f0e8ab83768a4a8aaf319460d4a70383415a0cc8,Any Privacy Risk if Nobody's Personal Information Being Collected?,2019,,10.1007/978-981-15-2767-8_31
000bc721cefe3f2ba2425e8b95b042dc1e9778f9,Applying Machine Learning Techniques to Improve Linux Process Scheduling,2005,"In this work we use Machine Learning (ML) techniques to learn the CPU time-slice utilization behavior of known programs in a Linux system. Learning is done by an analysis of certain static and dynamic attributes of the processes while they are being run. Our objective was to discover the most important static and dynamic attributes of the processes that can help best in prediction of CPU burst times which minimize the process TaT (Turn-around-Time). In our experimentation we modify the Linux Kernel scheduler (version 2.4.20-8) to allow scheduling with customized time slices. The ""Waikato Environment for Knowledge Analysis"" (Weka), an open source machine-learning tool is used to find the most suitable ML method to characterize our programs. We experimentally find that the C'4.5 Decision Tree algorithm most effectively solved the problem. We find that predictive scheduling could reduce TaT in the range of 1.4% to 5.8%. This was due to a reduction in the number of context switches needed to complete the process execution. We find our result interesting in the context that generally operating systems presently never make use of a program's previous execution history in their scheduling behavior.",10.1109/TENCON.2005.300837
000c021cc296b72bb72fa71e3091af594a6c9c87,Effects of genetic variation on the dynamics of neurodegeneration in Alzheimer's disease,2014,"Although many genetic markers are identified as being associated with Alzheimer's disease (AD), not much is known about their association with the structural changes that happen as the disease progresses. In this study, we investigate the genetic etiology of neurodegeneration in AD by associating genetic markers with atrophy profiles obtained using patient data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort. The atrophy profiles were quantified using a linear least-squares regression model over the span of patient enrollment, and used as imaging features throughout the analysis. A subset of the imaging features were selected for genetic association based on their ability to discriminate between healthy individuals and AD patients in a Support Vector Machines (SVM) classifier. Each imaging feature was associated with single-nucleotide polymorphisms (SNPs) using a linear model that included age and cognitive impairment scores as covariates to correct for normal disease progression. After false discovery rate correction, we observed 53 significant associations between SNPs and our imaging features, including associations of ventricular enlargement with SNPs on estrogen receptor 1 (ESR1) and sortilin-related VPS10 domain containing receptor 1 (SORCS1), hippocampal atrophy with SNPs on ESR1, and cerebral atrophy with SNPs on transferrin (TF) and amyloid beta precursor protein (APP). This study provides important insights into genetic predictors of specific types of neurodegeneration that could potentially be used to improve the efficacy of treatment strategies for the disease and allow the development of personalized treatment plans based on each patient's unique genetic profile.",10.1109/EMBC.2014.6944121
ffa6358e22a4a64e405bc265ffe7af433c159891,Finding memo: versatile interactions of the VPS10p-Domain receptors in Alzheimer’s disease,2022,"The family of VPS10p-Domain (D) receptors comprises five members named SorLA, Sortilin, SorCS1, SorCS2 and SorCS3. While their physiological roles remain incompletely resolved, they have been recognized for their signaling engagements and trafficking abilities, navigating a number of molecules between endosome, Golgi compartments, and the cell surface. Strikingly, recent studies connected all the VPS10p-D receptors to Alzheimer’s disease (AD) development. In addition, they have been also associated with diseases comorbid with AD such as diabetes mellitus and major depressive disorder. This systematic review elaborates on genetic, functional, and mechanistic insights into how dysfunction in VPS10p-D receptors may contribute to AD etiology, AD onset diversity, and AD comorbidities. Starting with their functions in controlling cellular trafficking of amyloid precursor protein and the metabolism of the amyloid beta peptide, we present and exemplify how these receptors, despite being structurally similar, regulate various and distinct cellular events involved in AD. This includes a plethora of signaling crosstalks that impact on neuronal survival, neuronal wiring, neuronal polarity, and synaptic plasticity. Signaling activities of the VPS10p-D receptors are especially linked, but not limited to, the regulation of neuronal fitness and apoptosis via their physical interaction with pro- and mature neurotrophins and their receptors. By compiling the functional versatility of VPS10p-D receptors and their interactions with AD-related pathways, we aim to further propel the AD research towards VPS10p-D receptor family, knowledge that may lead to new diagnostic markers and therapeutic strategies for AD patients.",10.1186/s13024-022-00576-2
34b6eaccd2f33fc7fa47194b80717c90ca9c39d6,Modification of the gut microbiome to combat neurodegeneration,2019,"Abstract The gut microbiome was extensively researched for its biological variety and its potential role in propagating diseases outside of the gastrointestinal (GI) tract. Recently, a lot of effort was focused on comprehending the gut-brain axis and the bizarre communication between the GI system and the nervous system. Ample amount of studies being carried out also revealed the involvement of the gut microbiome in enhancing the degree of many neurological disorders, including neurodegenerative diseases. It was widely observed that there were distinct microbiome profiles and dysbiosis within patients suffering from Alzheimer’s disease, Parkinson’s disease, amyotrophic lateral sclerosis, and multiple sclerosis. Various approaches to re-establish the balance of the gut microbiome, from antibiotic therapy, fecal microbiota transplant, or ingestion of psychobiotics, are discussed within this review within the specific context of combating neurodegenerative diseases. Present studies and clinical trials indicate that although there is an immense potential of gut microbiome modification to be preventive or therapeutic, there are still many intercalated components of the gut-brain axis at play and thus, more research needs to be carried out to delineate microbiome factors that may potentially alleviate symptoms of neurodegeneration.",10.1515/revneuro-2019-0005
991ccd62c8ad721b6d64dbc3be215f8cda060dc0,Perceived Racial Discrimination and DNA Methylation Among African American Women in the InterGEN Study,2018,"Introduction: Experiences of racial discrimination have been associated with poor health outcomes. Little is known, however, about how perceived racial discrimination influences DNA methylation (DNAm) among African Americans (AAs). We examined the association of experiences of discrimination with DNAm among AA women in the Intergenerational Impact of Genetic and Psychological Factors on Blood Pressure (InterGEN) study. Methods: The InterGEN study examines the effects of genetic and psychological factors on blood pressure among AA women and their children. Measures include the Major Life Discrimination (MLD) and the Race-Related Events (RES) scales. In the present analysis, we examined discrimination and DNAm at baseline in the InterGEN study. The 850K EPIC Illumina BeadChip was used for evaluating DNAm in this epigenome-wide association study (EWAS). Results: One hundred and fifty-two women contributed data for the RES-EWAS analysis and 147 for the MLD-EWAS analysis. Most were 30–39 years old, nonsmokers, had some college education, and had incomes <US$15,000/year. After controlling for age, smoking, and cell composition, MLD was significantly associated with DNAm at nine CpG (regions of DNA where a cytosine nucleotide is followed by a guanine nucleotide) sites (false discovery rate [FDR]-corrected p < .05). For the RES-EWAS analysis, no DNAm sites passed the epigenome-wide significance level after genomic control, though suggestive associations were observed at CpG sites after genomic control (raw p < 10−5). Conclusion: We observed significant epigenetic associations between disease-associated genes (e.g., schizophrenia, bipolar disorder, and asthma) and perceived discrimination as measured by the MLD Scale. Future health disparities research should include epigenetics in high-risk populations to elucidate functional consequences induced by the psychosocial environment.",10.1177/1099800417748759
6e58d41d7753766304c26b1f4f57801996fcfc7f,Neuroimaging Feature Terminology: A Controlled Terminology for the Annotation of Brain Imaging Features,2017,"Ontologies and terminologies are used for interoperability of knowledge and data in a standard manner among interdisciplinary research groups. Existing imaging ontologies capture general aspects of the imaging domain as a whole such as methodological concepts or calibrations of imaging instruments. However, none of the existing ontologies covers the diagnostic features measured by imaging technologies in the context of neurodegenerative diseases. Therefore, the Neuro-Imaging Feature Terminology (NIFT) was developed to organize the knowledge domain of measured brain features in association with neurodegenerative diseases by imaging technologies. The purpose is to identify quantitative imaging biomarkers that can be extracted from multi-modal brain imaging data. This terminology attempts to cover measured features and parameters in brain scans relevant to disease progression. In this paper, we demonstrate the systematic retrieval of measured indices from literature and how the extracted knowledge can be further used for disease modeling that integrates neuroimaging features with molecular processes.",10.3233/JAD-161148
3ef1aa70d22148011d5fd8d05a64babd9f61c895,Recent publications from the Alzheimer's Disease Neuroimaging Initiative: Reviewing progress toward improved AD clinical trials,2017,,10.1016/j.jalz.2016.11.007
e59ad10bd9048e0ccfc93a518387a132c23a79a9,Supports for Testing Memory Error Handling Code of In-memory Key Value Stores,2024,"Modern memory devices such as DRAM are prone to errors that occur because of unintended bit flips during their operation. Since memory errors severely impact in-memory key-value stores (KVSes), software mechanisms for hardening them against memory errors are being explored. However, it is hard to efficiently test the memory error handling code due to its characteristics: the code is event-driven, the handlers depend on the memory object, and in-memory KVSes manage various objects in huge memory space. This paper presents MemFI that supports runtime tests for the memory error handlers of in-memory KVSes. Our approach performs the software fault injection of memory errors at the memory object level to trigger the target handler while smoothly carrying out tests on the same running state. To show the effectiveness of MemFI, we integrate error handling mechanisms into a real-world in-memory KVS, memcached 1.6.9, and check its behavior using the MemFI prototype. The results show that the MemFI-based runtime tests allow us to check the behavior of the error handling mechanisms. We also show its efficiency by comparing it to other fault injection approaches based on a trial model.",10.1109/EDCC61798.2024.00020
a61eec9b66890e81f31d000f29048ab9d73860d9,TreeSLS: A Whole-system Persistent Microkernel with Tree-structured State Checkpoint on NVM,2023,"Whole-system persistence promises simplified application deployment and near-instantaneous recovery. This can be implemented using single-level store (SLS) through periodic checkpointing of ephemeral state to persistent devices. However, traditional SLSs suffer from two main issues on checkpointing efficiency and external synchrony, which are critical for low-latency services with persistence need. In this paper, we note that the decentralized state of microkernel-based systems can be exploited to simplify and optimize state checkpointing. To this end, we propose TreeSLS, a whole-system persistent microkernel that simplifies the whole-system state maintenance to a capability tree and a failure-resilient checkpoint manager. TreeSLS further exploits the emerging non-volatile memory to minimize checkpointing pause time by eliminating the distinction between ephemeral and persistent devices. With efficient state maintenance, TreeSLS further proposes delayed external visibility to provide transparent external synchrony with little overhead. Evaluation on microbenchmarks and real-world applications (e.g., Memcached, Redis and RocksDB) show that TreeSLS can complete a whole-system persistence in around 100 μs and even take a checkpoint every 1 ms with reasonable overhead to applications.",10.1145/3600006.3613160
fd32aa0d204a9b5746087599a8a83277c1f88cb6,A quantitative evaluation of persistent memory hash indexes,2023,,10.1007/s00778-023-00812-1
99b3ef8a399d60bd683414f75961b82913abff7e,Graceful ECC-uncorrectable Error Handling in the Operating System Kernel,2022,"Memory errors that can be detected but cannot be fixed by ECC modules, called ECC-uncorrectable errors, have a serious impact on the operating system (OS) kernel, the primary software layer to control hardware and applications. When an ECC-uncorrectable error corrupts the OS kernel's memory contents and causes the OS kernel to stop, all of the running applications also stop because they are no longer scheduled. The standard means of recovery from ECC-uncorrectable errors in the OS kernel is to reboot the kernel itself. Such a reboot involves rebooting all the running applications whose memory contents are not damaged; in turn, this triggers restoration of their running states, which is non-trivial in modern big-memory applications. This paper presents Ev6, an OS kernel that survives ECC-uncorrectable errors in the kernel memory space. Ev6 prunes its in-memory objects damaged by ECC-uncorrectable errors and reconstructs its internal structures to keep the OS kernel running as long as possible. We prototyped Ev6 on xv6 and conducted several experiments. The experimental results show that Ev6 gracefully handles our fault injections with trivial runtime and memory space overheads.",10.1109/ISSRE55969.2022.00021
cbabc9267d0d0bb9ca81006624b2cf3bcc2496fa,Meta's Next-generation Realtime Monitoring and Analytics Platform,2022,"Unlike traditional database systems where data and system availability are tied together, there is a wide class of systems targeting realtime monitoring and analytics over structured logs where these properties can be decoupled. In these systems, responsiveness and freshness of data are often more important than perfectly complete answers. One such system is Meta's Scuba [2].
 Historically, Scuba has favored system availability along with speed and freshness of results over data completeness and durability. While these choices allowed Scuba to grow from terabyte scale to petabyte scale and continue onboarding a variety of use cases, they also came at an operational cost of dealing with incomplete data and managing data loss.
 
 In this paper, we present the next generation of Scuba's architecture, codenamed
 Kraken
 , which decouples storage management from the query serving system and introduces a single, durable source of truth. This enables tangible improvements to system fault tolerance and query performance while still respecting tolerable bounds of client observed data freshness. We also describe the journey of how we deployed Kraken into full production as we gradually turned off the older system with no user-visible down time.
",10.14778/3554821.3554841
000ce80a8d4b0cf6be199cf97d8912b9dc9b44b2,VITAS : Guided Model-based VUI Testing of VPA Apps,2022,"Virtual personal assistant (VPA) services, e.g. Amazon Alexa and Google Assistant, are becoming increasingly popular recently. Users interact with them through voice-based apps, e.g. Amazon Alexa skills and Google Assistant actions. Unlike the desktop and mobile apps which have visible and intuitive graphical user interface (GUI) to facilitate interaction, VPA apps convey information purely verbally through the voice user interface (VUI), which is known to be limited in its invisibility, single mode and high demand of user attention. This may lead to various problems on the usability and correctness of VPA apps. In this work, we propose a model-based framework named Vitas to handle VUI testing of VPA apps. Vitas interacts with the app VUI, and during the testing process, it retrieves semantic information from voice feedbacks by natural language processing. It incrementally constructs the finite state machine (FSM) model of the app with a weighted exploration strategy guided by key factors such as the coverage of app functionality. We conduct a large-scale testing on 41,581 VPA apps (i.e., skills) of Amazon Alexa, the most popular VPA service, and find that 51.29% of them have weaknesses. They largely suffer from problems such as unexpected exit/start, privacy violation and so on. Our work reveals the immaturity of the VUI designs and implementations in VPA apps, and sheds light on the improvement of several crucial aspects of VPA apps.",10.1145/3551349.3556957
309632fe3feee8f85b2ca98772bce31b2a3ede06,Measuring privacy policy compliance in the Alexa ecosystem: In-depth analysis,2024,,10.1016/j.cose.2024.103963
9b48f505f41e22b37201a841a15ba69b6e78cadb,Understanding GDPR Non-Compliance in Privacy Policies of Alexa Skills in European Marketplaces,2024,"Amazon Alexa is one of the largest Voice Personal Assistant (VPA) platforms and it allows third-party developers to publish their voice apps, named skills, to the Alexa skill store. To satisfy the needs of European users, Amazon Alexa has established multiple skill marketplaces in Europe and allows developers to publish skills in their native languages. Skills in European marketplaces are required to comply with GDPR (General Data Protection Regulation), which imposes strict obligations on data collection and processing. Skills that involve data collection should provide a privacy policy to disclose the data practice to users and meet GDPR requirements. In this work, we analyze the privacy policies of skills in European marketplaces, focusing on whether skills' privacy policies and data collection behaviors comply with GDPR. We collect a large-scale dataset that includes skills in all European marketplaces with privacy policies. To classify whether a sentence in a privacy policy provides GDPR information, we gather a labeled dataset including skills' privacy policy sentences and use it to train a BERT model. Then, we analyze the GDPR compliance of European skills. Using a dynamic testing tool based on ChatGPT, we check whether skills' privacy policies comply with GDPR and are consistent with the actual data collection behaviors. Surprisingly, we find that 67% of the privacy policies fail to comply with GDPR and don't provide necessary GDPR-related information. For 1,187 skills with data collection behaviors, we observe that 603 skills (50.8%) don't provide a complete privacy policy and 1,128 skills (95%) have GDPR non-compliance issues in their privacy policies. Meanwhile, we find that the GDPR has a positive influence on European privacy policies.",10.1145/3589334.3645409
6ba419bee306050143eb5601899678de8611300a,Are Your Requests Your True Needs? Checking Excessive Data Collection in VPA Apps,2024,"Virtual personal assistants (VPA) services encompass a large number of third-party applications (or apps) to enrich their function-alities. These apps have been well examined to scrutinize their data collection behaviors against their declared privacy policies. Nonetheless, it is often overlooked that most users tend to ignore privacy policies at the installation time. Dishonest developers thus can exploit this situation by embedding excessive declarations to cover their data collection behaviors during compliance auditing. In this work, we present Pico, a privacy inconsistency detector, which checks the VPA app's privacy compliance by analyzing (in)consistency between data requested and data essential for its functionality. Pico understands the app's functionality topics from its publicly available textual data, and leverages advanced GPT-based language models to address domain-specific challenges. Based on the counterparts with similar functionality, suspicious data collection can be detected through the lens of anomaly detection. We apply Pico to understand the status quo of data-functionality com-pliance among all 65,195 skills in the Alexa app store. Our study reveals that 21.7% of the analyzed skills exhibit suspicious data collection, including Top 10 popular Alexa skills that pose threats to 54,116 users. These findings should raise an alert to both developers and users, in the compliance with the purpose limitation principle in data regulations.",10.1145/3597503.3639107
f4ba0defbf903dae5fe7e427b6a77d0c4d7948eb,Help Them Understand: Testing and Improving Voice User Interfaces,2024,"Voice-based virtual assistants are becoming increasingly popular. Such systems provide frameworks to developers for building custom apps. End-users can interact with such apps through a Voice User Interface (VUI), which allows the user to use natural language commands to perform actions. Testing such apps is not trivial: The same command can be expressed in different semantically equivalent ways. In this paper, we introduce VUI-UPSET, an approach that adapts chatbot-testing approaches to VUI-testing. We conducted an empirical study to understand how VUI-UPSET compares to two state-of-the-art approaches (i.e., a chatbot testing technique and ChatGPT) in terms of (i) correctness of the generated paraphrases, and (ii) capability of revealing bugs. To this aim, we analyzed 14,898 generated paraphrases for 40 Alexa Skills. Our results show that VUI-UPSET generates more bug-revealing paraphrases than the two baselines with, however, ChatGPT being the approach generating the highest percentage of correct paraphrases. We also tried to use the generated paraphrases to improve the skills. We tried to include in the voice interaction models of the skills (i) only the bug-revealing paraphrases, (ii) all the valid paraphrases. We observed that including only bug-revealing paraphrases is sometimes not sufficient to make all the tests pass.",10.1145/3654438
a71b15a8e8b3d7c34d9769c9644efdfad07299c3,SkillScanner: Detecting Policy-Violating Voice Applications Through Static Analysis at the Development Phase,2023,"The Amazon Alexa marketplace is the largest Voice Personal Assistant (VPA) platform with over 100,000 voice applications (i.e., skills) published to the skills store. In an effort to maintain the quality and trustworthiness of voice-apps, Amazon Alexa has implemented a set of policy requirements to be adhered to by third-party skill developers. However, recent works reveal the prevalence of policy-violating skills in the current skills store. To understand the causes of policy violations in skills, we first conduct a user study with 34 third-party skill developers focusing on whether they are aware of the various policy requirements defined by the Amazon Alexa platform. Our user study results show that there is a notable gap between VPA's policy requirements and skill developers' practices. As a result, it is inevitable that policy-violating skills will be published. To prevent the inflow of new policy-breaking skills to the skills store from the source, it is critical to identify potential policy violations at the development phase. In this work, we design and develop SkillScanner, an efficient static code analysis tool to facilitate third-party developers to detect policy violations early in the skill development lifecycle. To evaluate the performance of SkillScanner, we conducted an empirical study on 2,451 open source skills collected from GitHub. SkillScanner effectively identified 1,328 different policy violations from 786 skills. Our results suggest that 32% of these policy violations are introduced through code duplication (i.e., code copy and paste). In particular, we found that 42 skill code examples from potential Alexa's official accounts (e.g., ''alexa'' and ''alexa-samples'' on GitHub) contain policy violations, which lead to 81 policy violations in other skills due to the copy-pasted code snippets from these Alexa's code examples.",10.1145/3576915.3616650
1c18f1462ae084471a09cd7f16eade0cf59adf5d,Investigating Users’ Understanding of Privacy Policies of Virtual Personal Assistant Applications,2023,"The increasingly popular virtual personal assistant (VPA) services, e.g., Amazon Alexa and Google Assistant, enable third-party developers to create and release VPA apps for end users to access through smart speakers. Given that VPA apps handle sensitive personal data, VPA service providers require developers to release a privacy policy document to declare their data handling practice. The privacy policies are regarded as legal or semi-legal documents, which are usually lengthy and complex for users to understand. In this work, we conducted a subjective study to investigate the level of users’ understanding of the privacy policies, targeting the VPA apps (i.e., skills) of Amazon Alexa, the most popular VPA service. Our study focused on technical terms, one of the greatest hurdles to users’ understanding. We found that 84.2% of our participants faced difficulty in understanding technical terms appeared in the skills’ privacy policies, even for participants with IT background. Additionally, 64.3% of them reported that explanations for the technical terms are generally lacking. To address this issue, we proposed two principles, i.e., domain-specificity principle and implication-oriented principle, to guide skill developers in creating easy-to-understand privacy policies. We evaluated their effectiveness by creating explanation sentences for 23 representative terms and examining users’ understanding through a second user study. Our results show that using explanation sentences based on these principles can significantly improve users’ understanding.",10.1145/3579856.3590335
edc40b571fd299386f8149f01e6983d0fd0f240f,Scrutinizing Privacy Policy Compliance of Virtual Personal Assistant Apps,2022,"A large number of functionality-rich and easily accessible applications have become popular among various virtual personal assistant (VPA) services such as Amazon Alexa. VPA applications (or VPA apps for short) are accompanied by a privacy policy document that informs users of their data handling practices. These documents are usually lengthy and complex for users to comprehend, and developers may intentionally or unintentionally fail to comply with them. In this work, we conduct the first systematic study on the privacy policy compliance issue of VPA apps. We develop Skipper, which targets Amazon Alexa skills. It automatically depicts the skill into the declared privacy profile by analyzing their privacy policy documents with Natural Language Processing (NLP) and machine learning techniques, and derives the behavioral privacy profile of the skill through a black-box testing. We conduct a large-scale analysis on all skills listed on Alexa store, and find that a large number of skills suffer from the privacy policy noncompliance issues.",10.1145/3551349.3560416
000d02535797fd8ca7b0f04edcbf356e089a5338,A Spatio-Temporal Graph Convolutional Network for Gesture Recognition from High-Density Electromyography,2023,"Accurate hand gesture prediction is crucial for effective upper-limb prosthetic limbs control. As the high flexibility and multiple degrees of freedom exhibited by human hands, there has been a growing interest in integrating deep networks with high-density surface electromyography (HD-sEMG) grids to enhance gesture recognition capabilities. However, many existing methods fall short in fully exploit the specific spatial topology and temporal dependencies present in HD-sEMG data. Additionally, these studies are often limited number of gestures and lack generality. Hence, this study introduces a novel gesture recognition method, named STGCN-GR, which leverages spatio-temporal graph convolution networks for HD-sEMG-based human-machine interfaces. Firstly, we construct muscle networks based on functional connectivity between channels, creating a graph representation of HD-sEMG recordings. Subsequently, a temporal convolution module is applied to capture the temporal dependences in the HD-sEMG series and a spatial graph convolution module is employed to effectively learn the intrinsic spatial topology information among distinct HD-sEMG channels. We evaluate our proposed model on a public HD-sEMG dataset comprising a substantial number of gestures (i.e., 65). Our results demonstrate the remarkable capability of the STGCN-GR method, achieving an impressive accuracy of 91.07% in predicting gestures, which surpasses state-of-the-art deep learning methods applied to the same dataset.",10.1109/M2VIP58386.2023.10413402
000d0f73efd55bfaf20386ceefbdd567bb09524c,SoRank: incorporating social information into learning to rank models for recommendation,2014,"Most existing learning to rank based recommendation methods only use user-item preferences to rank items, while neglecting social relations among users. In this paper, we propose a novel, effective and efficient model, SoRank, by integrating social information among users into listwise ranking model to improve quality of ranked list of items. In addition, with linear complexity to the number of observed ratings, SoRank is able to scale to very large dataset. Experimental results on publicly available dataset demonstrate the effectiveness of SoRank.",10.1145/2567948.2577333
31a2887c35ca62a31349f45c4856e7e0a200759f,Systematic Review of Contextual Suggestion and Recommendation Systems for Sustainable e-Tourism,2021,"Agenda 2030 of Sustainable Development Goals (SDGs) 9 and 11 recognizes tourism as one of the central industries to global development to tackle global challenges. With the transformation of information and communication technologies (ICT), e-tourism has evolved globally to establish commercial relationships using the Internet for offering tourism-related products, including giving personalised suggestions. The contextual suggestion has emerged as a modified recommendation system that is integrated with information-retrieval techniques within large databases to provide tourists with a list of suggestions based on contexts, such as location, time of day, or day of the week (weekdays or weekends). This study surveyed literature in the field of contextual suggestion and recommendation systems with a focus on e-tourism. The concerns linked with approaches used in contextual suggestion and recommendation systems are highlighted in this systematic review, while motivations, recommendations, and practical implications in e-tourism are also discussed in this paper. A query search using the keywords “contextual suggestion system”, “recommendation system”, and “tourism” identified 143 relevant articles published from 2012 to 2020. Four major repositories are considered for searching, namely, (i) Science Direct, (ii) Scopus, (iii) IEEE, and (iv) Web of Science. This review was carried out under the protocols of four phases, namely, (i) query searching in major article repositories, (ii) removal of duplicates, (iii) scan of title and abstract, and (iv) complete reading of articles. To identify the gaps in current research, a taxonomy analysis was exemplified into categories and subcategories. The main categories were highlighted as (i) review articles, (ii) model/framework, and (iii) applications. Critical analysis was carried out on the basis of the available literature on the limitations of approaches used in contextual suggestion and recommendation systems. In conclusion, the approaches used are mainly based on content-based filtering, collaborative filtering, preference-based product ranking, and language modelling. The evaluation measures for the contextual suggestion system include precision, normalized discounted cumulative, and mean reciprocal rank, while test collections comprise Internet resources. Given that the tourism industry contributed to the environmental and social-economic development, contextual suggestion and recommendation systems have presented themselves to be relevant in integrating and achieving SDG 9 and SDG 11 in many ways such as web-based e-services by the government sector and smart gadgets based on reliable and real-time data and information for city planners as well as law enforcement personnel in a sustainable city.",10.3390/SU13158141
f6c9b52a78ab2cd4788d49d2d7a61627bec2b3c3,A Knowledge-Fusion Ranking System with an Attention Network for Making Assignment Recommendations,2020,"In recent decades, more teachers are using question generators to provide students with online homework. Learning-to-rank (LTR) methods can partially rank questions to address the needs of individual students and reduce their study burden. Unfortunately, ranking questions for students is not trivial because of three main challenges: (1) discovering students' latent knowledge and cognitive level is difficult, (2) the content of quizzes can be totally different but the knowledge points of these quizzes may be inherently related, and (3) ranking models based on supervised, semisupervised, or reinforcement learning focus on the current assignment without considering past performance. In this work, we propose KFRank, a knowledge-fusion ranking model based on reinforcement learning, which considers both a student's assignment history and the relevance of quizzes with their knowledge points. First, we load students' assignment history, reorganize it using knowledge points, and calculate the effective features for ranking in terms of the relation between a student's knowledge cognitive and the question. Then, a similarity estimator is built to choose historical questions, and an attention neural network is used to calculate the attention value and update the current study state with knowledge fusion. Finally, a rank algorithm based on a Markov decision process is used to optimize the parameters. Extensive experiments were conducted on a real-life dataset spanning a year and we compared our model with the state-of-the-art ranking models (e.g., ListNET and LambdaMART) and reinforcement-learning methods (such as MDPRank). Based on top-k nDCG values, our model outperforms other methods for groups of average and weak students, whose study abilities are relatively poor and thus their behaviors are more difficult to predict.",10.1155/2020/6748430
000d21240aff6bc045cb81755b532552432fdb69,Signal Detection and Classification in Shared Spectrum: A Deep Learning Approach,2021,"Accurate identification of the signal type in shared-spectrum networks is critical for efficient resource allocation and fair coexistence. It can be used for scheduling transmission opportunities to avoid collisions and improve system throughput, especially when the environment changes rapidly. In this paper, we develop deep neural networks (DNNs) to detect coexisting signal types based on In-phase/Quadrature (I/Q) samples without decoding them. By using segments of the samples of the received signal as input, a Convolutional Neural Network (CNN) and a Recurrent Neural Network (RNN) are combined and trained using categorical cross-entropy (CE) optimization. Classification results for coexisting Wi-Fi, LTE LAA, and 5G NR-U signals in the 5-6 GHz unlicensed band show high accuracy of the proposed design. We then exploit spectrum analysis of the I/Q sequences to further improve the classification accuracy. By applying Short-time Fourier Transform (STFT), additional information in the frequency domain can be presented as a spectrogram. Accordingly, we enlarge the input size of the DNN. To verify the effectiveness of the proposed detection framework, we conduct over-the-air (OTA) experiments using USRP radios. The proposed approach can achieve accurate classification in both simulations and hardware experiments.",10.1109/INFOCOM42981.2021.9488834
36a1d00f5a77b47ba7ebb5c51578f1dc3f0f7256,"AI-Empowered Multiple Access for 6G: A Survey of Spectrum Sensing, Protocol Designs, and Optimizations",2024,"With the rapidly increasing number of bandwidth-intensive terminals capable of intelligent computing and communication, such as smart devices equipped with shallow neural network (NN) models, the complexity of multiple access (MA) for these intelligent terminals is increasing due to the dynamic network environment and ubiquitous connectivity in sixth-generation (6G) systems. Traditional MA design and optimization methods are gradually losing ground to artificial intelligence (AI) techniques that have proven their superiority in handling complexity. AI-empowered MA and its optimization strategies aimed at achieving high quality-of-service (QoS) are attracting more attention, especially in the area of latency-sensitive applications in 6G systems. In this work, we aim to: 1) present the development and comparative evaluation of AI-enabled MA; 2) provide a timely survey focusing on spectrum sensing, protocol design, and optimization for AI-empowered MA; and 3) explore the potential use cases of AI-empowered MA in the typical application scenarios within 6G systems. Specifically, we first present a unified framework of AI-empowered MA for 6G systems by incorporating various promising machine learning (ML) techniques in spectrum sensing, resource allocation, MA protocol design, and optimization. We then introduce AI-empowered MA spectrum sensing related to spectrum sharing and spectrum interference management. Next, we discuss the AI-empowered MA protocol designs and implementation methods by reviewing and comparing the state of the art and further explore the optimization algorithms related to dynamic resource management, parameter adjustment, and access scheme switching. Finally, we discuss the current challenges, point out open issues, and outline potential future research directions in this field.",10.1109/JPROC.2024.3417332
6aa08ece7113d0bbf96644e3a61f5be416f4358f,Sums: Sniffing Unknown Multiband Signals under Low Sampling Rates,2024,"Due to sophisticated deployments of all kinds of wireless networks (e.g., 5G, Wi-Fi, Bluetooth, LEO satellite, etc.), multiband signals distribute in a large bandwidth (e.g., from 70 MHz to 8 GHz). Consequently, for network monitoring and spectrum sharing applications, a sniffer for extracting physical layer information, such as structure of packet, with low sampling rate (especially, sub-Nyquist sampling) can significantly improve their cost- and energy-efficiency. However, to achieve a multiband signals sniffer is really a challenge. To this end, we propose Sums, a system that can sniff and analyze multiband signals in a blind manner. Our Sums takes advantage of hardware and algorithm co-design, multi-coset sub-Nyquist sampling hardware, and a multi-task deep learning framework. The hardware component breaks the Nyquist rule to sample GHz bandwidth, but only pays for a 50 MSPS sampling rate. Our multi-task learning framework directly tackles the sampling data to perform spectrum sensing, physical layer protocol recognition, and demodulation for deep inspection from multiband signals. Extensive experiments demonstrate that Sums achieves higher accuracy than the state-of-theart baselines in spectrum sensing, modulation classification, and demodulation. As a result, our Sums can help researchers and end-users to diagnose or troubleshoot their problems of wireless infrastructures deployments in practice.",10.48550/arXiv.2405.15705
ab4249e9f758ede75ee22ccf28d3717c418072e6,Emerging Technologies for 6G Non-Terrestrial-Networks: From Academia to Industrial Applications,2024,"Terrestrial networks form the fundamental infrastructure of modern communication systems, serving more than 4 billion users globally. However, terrestrial networks are facing a wide range of challenges, from coverage and reliability to interference and congestion. As the demands of the 6G era are expected to be much higher, it is crucial to address these challenges to ensure a robust and efficient communication infrastructure for the future. To address these problems, Non-terrestrial Network (NTN) has emerged to be a promising solution. NTNs are communication networks that leverage airborne (e.g., unmanned aerial vehicles) and spaceborne vehicles (e.g., satellites) to facilitate ultra-reliable communications and connectivity with high data rates and low latency over expansive regions. This article aims to provide a comprehensive survey on the utilization of network slicing, Artificial Intelligence/Machine Learning (AI/ML), and Open Radio Access Network (ORAN) to address diverse challenges of NTNs from the perspectives of both academia and industry. Particularly, we first provide an in-depth tutorial on NTN and the key enabling technologies including network slicing, AI/ML, and ORAN. Then, we provide a comprehensive survey on how network slicing and AI/ML have been leveraged to overcome the challenges that NTNs are facing. Moreover, we present how ORAN can be utilized for NTNs. Finally, we highlight important challenges, open issues, and future research directions of NTN in the 6G era.",10.1109/OJCOMS.2024.3418574
000d797e163371d16daeaec893cf4d6e37003db2,A Capacitor-Less CMOS Neuron Circuit for Neuromemristive Networks,2019,"CMOS neuron circuits used to implement neuromorphic chips require extensive circuitry to program the memristive cell, thus eliminating most of the density advantage gained by the adoption of memristive synapses. This paper presents a CMOS neuron circuit that provides a compact and cost-efficient programming interface in which semiconductor capacitors are replaced by a memristive device. The neuron circuit also features an adjustable firing threshold with a strict control of the power consumption during the learning process.",10.1109/NEWCAS44328.2019.8961278
159a99b5af3260ab571dc86010e6576e101544d0,STATE: A Test Structure for Rapid Prediction of Resistive RAM Electrical Parameter Variability,2022,"Resistive RAM (RRAM) design optimization and reliability monitoring is essential not only to gain market share in the highly competitive emerging memory sector, but also to enable future high-capacity and power-efficient brain-inspired systems, beyond the capabilities of today’s hardware. Common problems with RRAM are related to high variability in operating conditions and low yield. Although research has taken steps to resolve these issues, variability remains a major hurdle for the wide spread of the technology. In this paper, a novel test structure consisting of an array of non-addressable IT-IR RRAM memory cells with parallel connection of all memory elements is introduced. The test structure can be used as a powerful tool for process variation monitoring during a new process technology introduction and also for marginal cell populations detection during process maturity. The test structure is designed to measure RRAM parameters of interest based on a simple measurement methodology: from the transfer characteristic measured under the select transistor clamping bias, it is possible to obtain accurate information on the RRAM switching parameters as well as the ON/OFF resistance values.",10.1109/ISCAS48785.2022.9937716
377d44f890140e0dcd36fa32290b3814d666f2b6,Multi-Level Control of Resistive RAM (RRAM) Using a Write Termination to Achieve 4 Bits/Cell in High Resistance State,2021,"RRAM density enhancement is essential not only to gain market share in the highly competitive emerging memory sector but also to enable future high-capacity and power-efficient brain-inspired systems, beyond the capabilities of today’s hardware. In this paper, a novel design scheme is proposed to realize reliable and uniform multi-level cell (MLC) RRAM operation without the need of any read verification. RRAM quad-level cell (QLC) capability with 4 bits/cell is demonstrated for the first time. QLC is implemented based on a strict control of the cell programming current of 1T-1R HfO2-based RRAM cells. From a design standpoint, a self-adaptive write termination circuit is proposed to control the RESET operation and provide an accurate tuning of the analog resistance value of each cell of a memory array. The different resistance levels are obtained by varying the compliance current in the RESET direction. Impact of variability on resistance margins is simulated and analyzed quantitatively at the circuit level to guarantee the robustness of the proposed MLC scheme. The minimal resistance margin reported between two consecutive states is 2.1 kΩ along with an average energy consumption and latency of 25 pJ/cell and 1.65 μs, respectively.",10.3390/electronics10182222
192f03596a0cd314a7a397dd7f6c6e163824c75a,Density Enhancement of RRAMs using a RESET Write Termination for MLC Operation,2021,"Multi-Level Cell (MLC) technology can greatly reduce Resistive RAM (RRAM) die sizes to achieve a breakthrough in cost structure. In this paper, a novel design scheme is proposed to realize reliable and uniform MLC RRAM operation without the need of any read verification. MLC is implemented based on a strict control of the cell programming currents of 1T-1R HfO2-based RRAM cells. Specifically, a self-adaptive write termination circuit is proposed to control the RRAM RESET current. Eight different resistance states are obtained by varying the compliance current which is defined as the minimal current allowed by the termination circuit in the RESET direction.",10.23919/DATE51398.2021.9473967
000db9915add5a825b593326acc98f24775f776e,Multiple priority dispatching rules for the job shop scheduling problem,2015,In this paper we focus on the Job Shop Scheduling Problem (JSSP) using Priority Dispatching Rules. Simulation model for makespan optimization is proposed using different Dispatching Rules (DR) for each machine in the shop floor. Collected results are used for learning base construction. This database will be used to develop an inference model able to select the best DR for every new scheduling problem. This preliminary study shows advantages of using different DR and also saving progress JSSP data.,10.1109/CEIT.2015.7232991
7472ff8b09063a2e2d5aa43d8305ed7060b1eeb5,A Deep Reinforcement Learning Framework Based on an Attention Mechanism and Disjunctive Graph Embedding for the Job-Shop Scheduling Problem,2023,"The job-shop scheduling problem (JSSP) is a classical NP-hard combinatorial optimization problem, and the operating efficiency of manufacturing system is affected directly by the quality of its scheduling scheme. In this article, a novel deep reinforcement learning framework is proposed for solving the classical JSSP, where each machine has to process each job exactly once. This method based on an attention mechanism and disjunctive graph embedding, and a sequence-to-sequence pattern is used to model the JSSP in the framework. A disjunctive graph embedding process based on node2vec is used to learn the disjunctive graph representations containing JSSP characteristics, thereby generalizing the model considerably. An improved transformer architecture based on a multihead attention mechanism is used to generate solutions. Containing a parallel-computing encoder and a recurrent-computing decoder, it is adept at learning long-range dependencies and effective at solving large-scale scheduling problems. Experimental results verified the effectiveness of the proposed method.",10.1109/TII.2022.3167380
79b2367b4ad78cd22eb5c9e1f6a81620077d084b,Selection of dispatching rules evolved by genetic programming in dynamic unrelated machines scheduling based on problem characteristics,2022,,10.1016/j.jocs.2022.101649
eb009311ec785844e49ac3bc24c23e3c21007efe,Multi-Criteria Optimization in Operations Scheduling Applying Selected Priority Rules,2021,"The utilization of a specific priority rule in scheduling operations in flexible job shop systems strongly influences production goals. In a context of production control in real practice, production performance indicators are evaluated always en bloc. This paper addresses the multi-criteria evaluating five selected conflicting production objectives via scalar simulation-based optimization related to applied priority rule. It is connected to the discrete-event simulation model of a flexible job shop system with partially interchangeable workplaces, and it investigates the impact of three selected priority rules—FIFO (First In First Out), EDD (Earliest Due Date), and STR (Slack Time Remaining). In the definition of the multi-criteria objective function, two scalarization methods—Weighted Sum Method and Weighted Product Method—are employed in the optimization model. According to the observations, EDD and STR priority rules outperformed the FIFO rule regardless of the type of applied multi-criteria method for the investigated flexible job shop system. The results of the optimization experiments also indicate that the evaluation via applying multi-criteria optimization is relevant for identifying effective solutions in the design space when the specific priority rule is applied in the scheduling operations.",10.3390/APP11062783
553f14bca493edf3de05af576bd68044ee79c20f,"Multiple dispatching rules allocation in real time using data mining, genetic algorithms, and simulation",2020,,10.1007/s10951-020-00664-5
2808f416ce680be15e36576c65acd50622c19ff0,Extracting priority rules for dynamic multi-objective flexible job shop scheduling problems using gene expression programming,2018,"In this paper, two new approaches are proposed for extracting composite priority rules for scheduling problems. The suggested approaches use simulation and gene expression programming and are able to evolve specific priority rules for all dynamic scheduling problems in accordance with their features. The methods are based on the idea that both the proper design of the function and terminal sets and the structure of the gene expression programming approach significantly affect the results. In the first proposed approach, modified and operational features of the scheduling environment are added to the terminal set, and a multigenic system is used, whereas in the second approach, priority rules are used as automatically defined functions, which are combined with the cellular system for gene expression programming. A comparison shows that the second approach generates better results than the first; however, all of the extracted rules yield better results than the rules from the literature, especially for the defined multi-objective function consisting of makespan, mean lateness and mean flow time. The presented methods and the generated priority rules are robust and can be applied to all real and large-scale dynamic scheduling problems.",10.1080/00207543.2018.1543964
2cc40447281b4d8604c7c7dda22c5ecb53812e73,Review of job shop scheduling research and its new perspectives under Industry 4.0,2017,,10.1007/s10845-017-1350-2
000e2688b1830dc2b4c7d9cd54447d5759ceadde,Detection and Classification of Retinal Diseases in Spectral Domain Optical Coherence Tomography Images based on SURF descriptors,2018,"Optical Coherence Tomography (OCT) is a non-invasive eye-imaging modality for detecting macular edema both in its early and advanced stages. The main aim of this work is to present the automatic detection of edema of the retinal layers particularly around the macula in diabetic patients. After detection and extracting certain features in the OCT retinal images a classification of the type of Diabetic Macular Edema is done. In this method during preprocessing stage we remove the speckle noise followed by flattening and cropping of the image is done. Then this is followed by Speeded up robust feature extraction. The extracted features are then classified using Support Vector Machine binary classifier as normal or abnormal and thus having Diabetic Macular Edema. This technique has been applied for 25 normal and 45 abnormal OCT images. The results show that this method accurately detected edema diseases in between the layers in the retinal. Then we could classify them using Support Vector Machine as normal or abnormal. Experimental results shows that an average retinal disease detection accuracy of 99% for Support Vector Machine (SVM) classifier. Thus, this algorithm can be used by ophthalmologists in early detection of Macular Edema.",10.1109/ICSCAN.2018.8541254
9badb1277c4ad05546629e7e540e0fb9b9929576,Detection of Choroidal Neovascularization (CNV) in Retina OCT Images Using VGG16 and DenseNet CNN,2021,,10.1007/s11277-021-09086-8
2e4dea5d2d4dc047e77a0b7f809259e6a20df556,Social- and self-perception of designers’ professional identity,2020,"ABSTRACT Designers’ Professional Identity (DPI) is a social- and self-perceptive construct that describes how designers understand themselves as professionals. DPI guides development throughout a designer’s career by shaping professionalism, role assumptions, responsibilities, values and behaviour. DPI links two sets of elements: Personal Attributes and Design Skills. However, little is known about how designers perceive themselves in comparison to other critical actors affecting DPI: educators and managers. While differing perceptions between educators and managers is acknowledged, there is a critical need for more detailed understanding of these differences in comparison to how designers perceive themselves. This study uses semi-structured interviews with designers, design professors, and design managers to shed light on differences in perception of DPI. Analysis of the data highlights critical differences between the three groups. We described these differences with respect to three thematic perspectives on DPI: Technique, Creativity and Rapport. This provides important contributions to understanding DPI, with implications for education and practice.",10.1080/09544828.2019.1676883
000f18a98b1a305d5ff07972b2a63849f9b26908,A Generalized Framework for Automatic Scripting Language Parallelization,2017,"Computational scientists are typically not expert programmers, and thus work in easy to use dynamic languages. However, they have very high performance requirements, due to their large datasets and experimental setups. Thus, the performance required for computational science must be extracted from dynamic languages in a manner that is transparent to the programmer. Current approaches to optimize and parallelize dynamic languages, such as just-in-time compilation and highly optimized interpreters, require a huge amount of implementation effort and are typically only effective for a single language. However, scientists in different fields use different languages, depending upon their needs.This paper presents techniques to enable automatic extraction of parallelism within scripts that are universally applicable across multiple different dynamic scripting languages. The key insight is that combining a script with its interpreter, through program specialization techniques, will embed any parallelism within the script into the combined program that can then be extracted via automatic parallelization techniques. Additionally, this paper presents several enhancements to existing speculative automatic parallelization techniques to handle the dependence patterns created by the specialization process. A prototype of the proposed technique, called Partial Evaluation with Parallelization (PEP), is evaluated against two open-source script interpreters with 6 input linear algebra kernel scripts each. The resulting geomean speedup of 5.10× on a 24-core machine shows the potential of the generalized approach in automatic extraction of parallelism in dynamic scripting languages.",10.1109/PACT.2017.28
8d42fb68be9c84cdf238ab177ce6650e01821549,Quantifying the Semantic Gap Between Serial and Parallel Programming,2021,"Automatic parallelizing compilers are often constrained in their transformations because they must conservatively respect data dependences within the program. Developers, on the other hand, often take advantage of domain-specific knowledge to apply transformations that modify data dependences but respect the application's semantics. This creates a semantic gap between the parallelism extracted automatically by compilers and manually by developers. Although prior work has proposed programming language extensions to close this semantic gap, their relative contribution is unclear and it is uncertain whether compilers can actually achieve the same performance as manually parallelized code when using them. We quantify this semantic gap in a set of sequential and parallel programs and leverage these existing programming-language extensions to empirically measure the impact of closing it for an automatic parallelizing compiler. This lets us achieve an average speedup of 12.6× on an Intel-based 28-core machine, matching the speedup obtained by the manually parallelized code. Further, we apply these extensions to widely used sequential system tools, obtaining 7.1× speedup on the same system.",10.1109/IISWC53511.2021.00024
a4499adb70b2e62991e79902e5a4199c2083edf6,SCAF: a speculation-aware collaborative dependence analysis framework,2020,"Program analysis determines the potential dataflow and control flow relationships among instructions so that compiler optimizations can respect these relationships to transform code correctly. Since many of these relationships rarely or never occur, speculative optimizations assert they do not exist while optimizing the code. To preserve correctness, speculative optimizations add validation checks to activate recovery code when these assertions prove untrue. This approach results in many missed opportunities because program analysis and thus other optimizations remain unaware of the full impact of these dynamically-enforced speculative assertions. To address this problem, this paper presents SCAF, a Speculation-aware Collaborative dependence Analysis Framework. SCAF learns of available speculative assertions via profiling, computes their full impact on memory dependence analysis, and makes this resulting information available for all code optimizations. SCAF is modular (adding new analysis modules is easy) and collaborative (modules cooperate to produce a result more precise than the confluence of all individual results). Relative to the best prior speculation-aware dependence analysis technique, by computing the full impact of speculation on memory dependence analysis, SCAF dramatically reduces the need for expensive-to-validate memory speculation in the hot loops of all 16 evaluated C/C++ SPEC benchmarks.",10.1145/3385412.3386028
c9a91a893d6b02ff19854a8cdbfe6e7cec5e4f0d,"Reflections on the compatibility, performance, and scalability of parallel Python",2019,"Today's hardware is increasingly parallel, and to increase performance, applications must be able to use this parallelism. Hence, programming languages must provide the means for parallel execution. The language Python offers a multithreading, shared-memory model for concurrency. However, simultaneous execution of threads, i.e., parallel execution, is not a standard feature of current virtual machines (VM) for Python. Instead, the predominant Python VMs depend on a global interpreter lock, which serializes the execution. In a parallel VM, replicating Python's concurrency semantics is challenging. Today, there are three parallel VMs, which use one of two approaches to address the challenges: Jython, IronPython, and PyPy-STM. These VMs use two fundamentally different approaches to synchronize parallel execution under Python's concurrency semantics: Jython and IronPython use fine-grained locking, and PyPy-STM uses software transactional memory (STM). The two approaches result in different performance characteristics and levels of Python compatibility for these VMs. In this paper, we report on our experience with the three parallel VMs by comparing their compatibility, performance, and scalability. The comparison shows that fine-grained locking can yield better scalability than the STM approach. However, regarding the faithful reproduction of Python's concurrency semantics and the absolute performance, the STM approach currently has the advantage.",10.1145/3359619.3359747
afbbd472d04b1c1051b70f2ec245dd9cae9d7930,Hardware Multithreaded Transactions,2018,"Speculation with transactional memory systems helps pro- grammers and compilers produce profitable thread-level parallel programs. Prior work shows that supporting transactions that can span multiple threads, rather than requiring transactions be contained within a single thread, enables new types of speculative parallelization techniques for both programmers and parallelizing compilers. Unfortunately, software support for multi-threaded transactions (MTXs) comes with significant additional inter-thread communication overhead for speculation validation. This overhead can make otherwise good parallelization unprofitable for programs with sizeable read and write sets. Some programs using these prior software MTXs overcame this problem through significant efforts by expert programmers to minimize these sets and optimize communication, capabilities which compiler technology has been unable to equivalently achieve. Instead, this paper makes speculative parallelization less laborious and more feasible through low-overhead speculation validation, presenting the first complete design, implementation, and evaluation of hardware MTXs. Even with maximal speculation validation of every load and store inside transactions of tens to hundreds of millions of instructions, profitable parallelization of complex programs can be achieved. Across 8 benchmarks, this system achieves a geomean speedup of 99% over sequential execution on a multicore machine with 4 cores.",10.1145/3173162.3173172
305826910778ccbb0c39d830e90913a1fcef6c57,Language-Agnostic Optimization and Parallelization for Interpreted Languages,2017,,10.1007/978-3-030-35225-7_4
6b6cc76db0fd967f763502e0e9455e3acbba8ef4,Machine Learning-Based Stator Current Data-Driven PMSM Stator Winding Fault Diagnosis,2022,"Permanent magnet synchronous motors (PMSMs) have become one of the most important components of modern drive systems. Therefore, fault diagnosis and condition monitoring of these machines have been the subject of many studies in recent years. This article presents an intelligent stator current-data driven PMSM stator winding fault detection and classification method. Short-time Fourier transform is applied in the process of fault feature extraction from the stator phase current symmetrical components signal. Automation of the fault detection and classification process is carried out with the use of three selected machine learning algorithms: support vector machine, naïve Bayes classifier and multilayer perceptron. The concept and online verification of the original intelligent fault diagnosis system with the potential of a real industrial deployment are demonstrated. Experimental results are presented to evaluate the effectiveness of the proposed methodology.",10.3390/s22249668
474bc7708af82585aefdb9c402acb5cc1d1e4c5c,On-line Detection and Classification of PMSM Stator Winding Faults Based on Stator Current Symmetrical Components Analysis and the KNN Algorithm,2021,"The significant advantages of permanent magnet synchronous motors, such as very good dynamic properties, high efficiency and power density, have led to their frequent use in many drive systems today. However, like other types of electric motors, they are exposed to various types of faults, including stator winding faults. Stator winding faults are mainly inter-turn short circuits and are among the most common faults in electric motors. In this paper, the possibility of using the spectral analysis of symmetrical current components to extract fault symptoms and the machine-learning-based K-Nearest Neighbors (KNN) algorithm for the detection and classification of the PMSM stator winding fault is presented. The impact of the key parameters of this classifier on the effectiveness of stator winding fault detection and classification is presented and discussed in detail, which has not been researched in the literature so far. The proposed solution was verified experimentally using a 2.5 kW PMSM, the construction of which was specially prepared for carrying out controlled inter-turn short circuits.",10.3390/electronics10151786
34546f41d8a06e37a651f86b3a23ad7c16023348,A New Entropy Bi-Cepstrum Based-Method for DC Motor Brush Abnormality Recognition,2017,"Abnormal arcs in dc motors are often associated with various potential failures or operational defaults. Although they may not directly led to motor breakdown, they can be causes to faults, further damages, and fire hazard. There can be arcs between brushes and rotors when motor running under normal condition, known as normal arcs. However, abnormal arcs, which are difficult to be visually distinguished from normal arcs, occur when there is loosen or contamination of brushes. Therefore, detecting the existence and identifying the type of unusual arcs can be applied as an effective method for brush condition monitoring. This paper presents a detection strategy for abnormality in brush based on the online electromagnetic field (EMF) analysis with advanced feature extraction techniques. The techniques aim at finding the unusual changes in EMF to identify abnormal arc among normal ones. Entropy bi-cepstrum applied as feature extraction method is an inverse spectrum of cumulant. Bi-cepstrum is insensitive to noise, and entropy reflects the complexity of the target signal. In the experiment, three typical types of unusual arcs occurring in brush area are successfully identified, and the result shows the accuracy as high as 91.4%. The new strategy with algorithms can serve as a very useful tool for abnormality recognition of the motor brush.",10.1109/JSEN.2016.2635641
616717b3467f03fe106446e6c2ee15e875087d4b,A review of Permanent Magnet Synchronous Motor fault diagnosis,2014,"This paper presents a review of Permanent Magnet Synchronous Motor (PMSM) fault diagnosis methods. Firstly, PMSM usual faults including electrical, mechanical, and magnetic faults are listed. In the third part, the various signal processing methods for PMSM are summarized. Finally, the artificial intelligence methods for PMSM fault diagnosis are reviewed, such as artificial neural network, fuzzy logic.",10.1109/ITEC-AP.2014.6940870
c2335c67d9d40444232234ed34208742678676b6,Demagnetization fault investigation in permanent magnet synchronous motor,2014,"Condition monitoring of electrical machines problems is essential for guaranteeing high motor performance, efficiency, and reliability. In this case the demagnetization fault detection in a permanent-magnet synchronous machine (PMSM) is investigated in this paper as a common fault. Demagnetization can be complete, or partial, on a certain region of the pole. Also it can be in form of symmetric or asymmetric. In this base, this paper presents a fault detection survey of PMSM under different types of demagnetization states. Finite element method (FEM) used for simulation and consideration of different fault states under fault mentioned.",10.1109/PEDSTC.2014.6799448
000f8fb442e7eee012c43f4c0a123ae060af645b,"Reality television, fan behavior, and online communities of practice",2007,"In this paper, I describe participation in reality television online communities as a case of ""cultural convergence"" (Ito, in press; Jenkins, 2006) across ""old"" fan fiction and ""new"" online community practices. The ways in which reality TV fans engage as media producers parallels the ways in which researchers who study other new media such as video games describe the rich discourse, enduring community, and media mixing required to participate in these settings (Ito, in press; Steinkuehler, 2006). I argue here that the characterization of these worlds as communities of practice (Lave & Wenger, 1991), the dominant analytic framework for online communities, does not fully capture the way learning (and therefore becoming) happens in reality TV online communities and suggest ways to reframe this model. Finally, I propose directions for future research focused on understanding reality TV online fan communities as informal learning environments that require participants to engage in rich cognitive and sociocultural media literacy activities.",10.3115/1599600.1599646
8ec7fb565aedf9f583d0e2b328b7541e549484fa,Growing Their Own: Legitimate Peripheral Participation for Computational Learning in an Online Fandom Community,2017,"Online communities dedicated to the creation of fanworks (e.g., fiction or art inspired by media such as books or television shows) often serve as communities of practice for learning communication, artistic, and technical skills. In studying one successful fan fiction archive that was designed and built entirely by (predominantly women) fans, we observed processes of legitimate peripheral participation (LPP) in which some of these fans began in peripheral roles and came to be more involved in the technical aspects of the archive over time. In addition to outlining positive outcomes, we discuss the challenges of supporting learning within this CoP, particularly with respect to the burden on experts. We discuss potential implications and solutions for the problem of expert scarcity in CoPs, and propose that LPP within fan communities can be leveraged for broadening participation in computing among women.",10.1145/2998181.2998210
ec9a3ff3df59ed0769dda084a1ca654b67fe8b37,Social networking and education: emerging research within CSCL,2009,"In this paper I introduce a youth-initiated practice: online social networking that is transforming our society in important ways and has vast implications for learning research and education. I introduce the social and technical features that characterize social networking systems and outline results from emerging research that suggests the social and intellectual practices in which participants naturally engage and how these relate to the competencies increasingly valued in formal education. Next, I discuss one research projects which I am currently pursuing that build on early work and suggest how educational programs might employ such practices to advantage. Finally, I discuss what I see as the educative value of this technology in certain contexts and suggest a course for future research and development. My overall goals are to inform other researchers interested in pursuing similar projects and to stimulate interdisciplinary conversation about where such agendas fit within and advance the aims of CSCL.",10.3115/1600053.1600119
00102c0a275cfd235e1e2af44ea94a3ba5ead870,Multi-task learning of deep neural networks for joint automatic speaker verification and spoofing detection,2019,"With the development of spoofing technologies, automatic speaker verification (ASV) systems have encountered serious challenges on security. In order to address this problem, many anti-spoofing countermeasures have been explored. There are two intuitive recipes to protect an ASV system from spoofing. The first one is to use a cascaded structure where spoofing detection is performed firstly and ASV is subsequently conducted only on the attempts which have passed the spoofing detection. The other one is to perform spoofing detection and ASV jointly. The discriminate reliably of the joint system has been proven to be more advantageous than cascaded systems with traditional methods, not only in accuracy, but also in convenience and computational efficiency. In this paper, we proposed a multi-task learning approach based on deep neural network to make a joint system of ASV and anti-spoofing. The performance of different acoustic features and structures of deep neural networks has been investigated on the ASVspoof 2017 version 2.0 dataset. The experimental results showed that the joint equal error rate (EER) of our approach was reduced by 0.55% compared to a joint system with Gaussian back-end fusion baseline.",10.1109/APSIPAASC47483.2019.9023289
001052d42535c0f72d748b37076e71ced3a61f1f,A Component-Based Framework for Distributed Control Systems,2006,"The paper presents a two-level software framework for distributed embedded applications. At the top level, an application is conceived as a composition of embedded actors that communicate transparently by exchanging labeled messages (signals), independent of their allocation onto network nodes. Signals are exchanged at precisely specified time instants, in accordance with the concept of distributed timed multitasking (DTM). The combination of actors, signal-based communication and DTM provides a framework for the development of open yet predictable embedded systems. At the lower level of specification, actors are modeled as software objects that are configured from executable components - basic, composite and modal function blocks, as well as supervisory state machines. Actor behaviour is specified with a hybrid executable model - a clocked event-driven state machine operating in conjunction with modal function blocks, which can be used to implement a broad range of applications such as sequential, continuous and hybrid control systems",10.1109/EUROMICRO.2006.6
907fbb97d6a26ddc12d8880f0639318f29c065b4,COTS-Based Architectural Framework for Reliable Real-Time Control Applications in Manufacturing,2020,"The challenge of keeping the development and implementation of real-time control systems reliable and efficient and at the same time, low-cost and low-energy, is getting harder. This is because system designers and developers are faced with the dependability, inflexibility and often high-cost of specialized or custom-built hardware and software components. This research attempts to tackle issues such as the reliability and efficiency of real-time control systems and advance further the current state-of-the-art. For this purpose, a strong emphasis is placed on finding novel efficient solutions based on standardized and commercially available off-the-shelf hardware/software components. In this direction, this research applies credible and feasible methodologies (e.g., model-based design, component-based design, formal verification, real-time scheduling, prototyping, and validation) in an innovative enhanced way. As an important outcome, a versatile integrative design approach and architectural framework (VIDAF) is proposed, which supports the development and implementation of reliable real-time control systems and applications using commercial off-the-shelf (COTS) components. The feasibility and applicability of the proposed system’s architecture are evaluated and validated through a system application in embedded real-time control in manufacturing. The research outcomes are expected to have a positive impact on emerging areas such as the Industrial Internet of Things (IIoT).",10.3390/app10093228
0010963f374d625ad7fb42b64eb30de2244863d5,Semi-Supervised Learning Based on Reference Model for Low-resource TTS,2022,"Most previous neural text-to-speech (TTS) methods are mainly based on supervised learning methods, which means they depend on a large training dataset and hard to achieve comparable performance under low-resource conditions. To ad-dress this issue, we propose a semi-supervised learning method for neural TTS in which labeled target data is limited, which can also resolve the problem of exposure bias in the previous auto-regressive models. Specifically, we pre-train the reference model based on Fastspeech2 with much source data, fine-tuned on a limited target dataset. Meanwhile, pseudo labels generated by the original reference model are used to guide the fine-tuned model's training further, achieve a regularization effect, and reduce the overfitting of the fine-tuned model during training on the limited target data. Experimental results show that our proposed semi-supervised learning scheme with limited target data significantly improves the voice quality for test data to achieve naturalness and robustness in speech synthesis.",10.1109/MSN57253.2022.00156
a1fac1c5fe5e09f8e5c0dd6acb314a373f766ba9,Towards Spontaneous Style Modeling with Semi-supervised Pre-training for Conversational Text-to-Speech Synthesis,2023,"The spontaneous behavior that often occurs in conversations makes speech more human-like compared to reading-style. However, synthesizing spontaneous-style speech is challenging due to the lack of high-quality spontaneous datasets and the high cost of labeling spontaneous behavior. In this paper, we propose a semi-supervised pre-training method to increase the amount of spontaneous-style speech and spontaneous behavioral labels. In the process of semi-supervised learning, both text and speech information are considered for detecting spontaneous behaviors labels in speech. Moreover, a linguistic-aware encoder is used to model the relationship between each sentence in the conversation. Experimental results indicate that our proposed method achieves superior expressive speech synthesis performance with the ability to model spontaneous behavior in spontaneous-style speech and predict reasonable spontaneous behavior from text.",10.21437/interspeech.2023-1754
0010fabe2a25b36c5bcf6057baafb61a40e37e3b,Multi-style adaptive training for robust cross-lingual spoken language understanding,2013,"Given the increasingly available machine translation (MT) services nowadays, one efficient strategy for cross-lingual spoken language understanding (SLU) is to first translate the input utterance from the second language into the primary language, and then call the primary language SLU system to decode the semantic knowledge. However, errors introduced in the MT process create a condition similar to the “mismatch” condition encountered in robust speech recognition. Such mismatch makes the performance of cross-lingual SLU far from acceptable. Motivated by successful solutions developed in robust speech recognition, we in this paper propose a multi-style adaptive training method to improve the robustness of the SLU system for cross-lingual SLU tasks. For evaluation, we created an English-Chinese bilingual ATIS database, and then carried out a series of experiments on that database to experimentally assess the proposed methods. Experimental results show that, without relying on any data in the second language, the proposed method significantly improves the performance on a cross-lingual SLU task while producing no degradation for input in the primary language. This greatly facilitates porting SLU to as many languages as there are MT systems without any human effort. We further study the robustness of this approach to another type of mismatch condition, caused by speech recognition errors, and demonstrate its success also.",10.1109/ICASSP.2013.6639292
564992c12097a87c279a90a1459eb72bbd007090,Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation,2022,"Multilingual task-oriented dialogue (ToD) facilitates access to services and information for many (communities of) speakers. Nevertheless, its potential is not fully realized, as current multilingual ToD datasets—both for modular and end-to-end modeling—suffer from severe limitations. 1) When created from scratch, they are usually small in scale and fail to cover many possible dialogue flows. 2) Translation-based ToD datasets might lack naturalness and cultural specificity in the target language. In this work, to tackle these limitations we propose a novel outline-based annotation process for multilingual ToD datasets, where domain-specific abstract schemata of dialogue are mapped into natural language outlines. These in turn guide the target language annotators in writing dialogues by providing instructions about each turn’s intents and slots. Through this process we annotate a new large-scale dataset for evaluation of multilingual and cross-lingual ToD systems. Our Cross-lingual Outline-based Dialogue dataset (cod) enables natural language understanding, dialogue state tracking, and end-to-end dialogue evaluation in 4 diverse languages: Arabic, Indonesian, Russian, and Kiswahili. Qualitative and quantitative analyses of cod versus an equivalent translation-based dataset demonstrate improvements in data quality, unlocked by the outline-based approach. Finally, we benchmark a series of state-of-the-art systems for cross-lingual ToD, setting reference scores for future work and demonstrating that cod prevents over-inflated performance, typically met with prior translation-based ToD datasets.",10.1162/tacl_a_00539
14c940926a046e6c33cc4eca9ea73cac1bcfa347,From Masked Language Modeling to Translation: Non-English Auxiliary Tasks Improve Zero-shot Spoken Language Understanding,2021,"The lack of publicly available evaluation data for low-resource languages limits progress in Spoken Language Understanding (SLU). As key tasks like intent classification and slot filling require abundant training data, it is desirable to reuse existing data in high-resource languages to develop models for low-resource scenarios. We introduce xSID, a new benchmark for cross-lingual (x) Slot and Intent Detection in 13 languages from 6 language families, including a very low-resource dialect. To tackle the challenge, we propose a joint learning approach, with English SLU training data and non-English auxiliary tasks from raw text, syntax and translation for transfer. We study two setups which differ by type and language coverage of the pre-trained embeddings. Our results show that jointly learning the main tasks with masked language modeling is effective for slots, while machine translation transfer works best for intent classification.",10.18653/V1/2021.NAACL-MAIN.197
d29036946152bddf950fec7a08c2828a8a8f902e,Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems,2021,"In task-oriented dialogue (ToD), a user holds a conversation with an artificial agent  with the aim of completing a concrete task. Although this technology represents one of  the central objectives of AI and has been the focus of ever more intense research and  development efforts, it is currently limited to a few narrow domains (e.g., food ordering,  ticket booking) and a handful of languages (e.g., English, Chinese). This work provides an  extensive overview of existing methods and resources in multilingual ToD as an entry point  to this exciting and emerging field. We find that the most critical factor preventing the  creation of truly multilingual ToD systems is the lack of datasets in most languages for  both training and evaluation. In fact, acquiring annotations or human feedback for each  component of modular systems or for data-hungry end-to-end systems is expensive and  tedious. Hence, state-of-the-art approaches to multilingual ToD mostly rely on (zero- or  few-shot) cross-lingual transfer from resource-rich languages (almost exclusively English),  either by means of (i) machine translation or (ii) multilingual representations. These  approaches are currently viable only for typologically similar languages and languages with  parallel / monolingual corpora available. On the other hand, their effectiveness beyond these  boundaries is doubtful or hard to assess due to the lack of linguistically diverse benchmarks  (especially for natural language generation and end-to-end evaluation). To overcome this  limitation, we draw parallels between components of the ToD pipeline and other NLP tasks,  which can inspire solutions for learning in low-resource scenarios. Finally, we list additional  challenges that multilinguality poses for related areas (such as speech, fluency in generated  text, and human-centred evaluation), and indicate future directions that hold promise to  further expand language coverage and dialogue capabilities of current ToD systems. ",10.1613/jair.1.13083
aee8581c60d8c812bd31503fab00f9df8d88a766,Zero-Shot Cross-lingual Semantic Parsing,2021,"Recent work in cross-lingual semantic parsing has successfully applied machine translation to localize parsers to new languages. However, these advances assume access to high-quality machine translation systems and word alignment tools. We remove these assumptions and study cross-lingual semantic parsing as a zero-shot problem, without parallel data (i.e., utterance-logical form pairs) for new languages. We propose a multi-task encoder-decoder model to transfer parsing knowledge to additional languages using only English-logical form paired data and in-domain natural language corpora in each new language. Our model encourages language-agnostic encodings by jointly optimizing for logical-form generation with auxiliary objectives designed for cross-lingual latent representation alignment. Our parser performs significantly above translation-based baselines and, in some cases, competes with the supervised upper-bound.",10.18653/v1/2022.acl-long.285
001188339c776654004094183ab01c3e67e48a43,Component-based hierarchical state machine — A reusable and flexible game AI technology,2011,"Finite State Machine (FSM) is the most common used technique to create intelligent character behaviors in video games. But conventional FSM technique has many limitations in game development. The main innovation of this paper is the introduction of software component technology to the implementation of FSMs in game development, which modularizes the states and transitions of FSMs completely. Compared with the conventional FSMs, this technique has three advantages. First, high-level and complex intelligent behaviors can be constructed from a set of low-level and simple behavior rapidly. Second, high-level game designing can be decoupled from low-level game AI programming. Third, game characters equipped with this AI system can exhibit more flexibility and adaptability to the changing game environment.",10.1109/ITAIC.2011.6030340
54662da5de373d5bfc2c4f6144303d53c17f7006,Hierarchical Decision-Making Framework for Multiple UCAVs Autonomous Confrontation,2023,"Autonomous decision-making for air confrontation between unmanned combat aerial vehicles remains hard to be designed due to dynamic situations and complex interactions. Rule-based decision-making methods provide a powerful solution with better interpretability. However, various hand-crafted rules may result in conflicts and poor scalability issues. To overcome this problem, this work proposes a hierarchical decision-making framework called State-Event-Condition-Action (SECA), which integrates the finite state machine and event-condition-action frameworks. This framework provides three products for system design: the SECA model–an abstract model of rules; the SECA state chart–a graphical visualization of rules; and the SECA rule description–a machine-readable format for practical deployment. The SECA framework offers several advantages, including convenient deployment, high efficiency, better logicality, and scalability. Simulation results demonstrate that the SECA framework enables autonomous decision-making in air confrontation scenarios and outperforms the event-condition-action framework in terms of computational time and cost-effectiveness. Furthermore, the generalization test in robot navigation tasks verifies its potential applicability to other domains with different background knowledge.",10.1109/TVT.2023.3285223
2ba2ca5d1ff7532a987ca5e4224e257b32f5e140,Hardware implementations of software programs based on hierarchical finite state machine models,2013,,10.1016/j.compeleceng.2013.07.019
702d3bbc9f64c7d7f6c3be5d122ec27415b0952f,SeGTE: A serious game to train and evaluate basic life support,2014,"Basic life support (BLS) is the level of medical care which is used for victims of life-threatening illnesses or injuries until they can be given full medical care at a hospital. It can be provided by trained medical personnel, including emergency medical technicians, paramedics, and by laypersons who have received BLS training. BLS is generally used in the pre-hospital setting, and can be provided without medical equipment. The ability to respond to an emergency situation can be the difference between life and death. Acknowledging this fact has made decision-makers, governments and Non Government Organizations (NGO) to make a priority to spread this knowledge and skills to the general population. Currently, BLS is taught in a standard course provided by the Red Course or certified entities, where the pedagogical content given to the students is a mix of theoretical and practical training where life-size mannequin are used. In this paper we argue that serious games could help spread this knowledge through the general population and it could also be used to refresh the knowledge of people that have been certified in BLS in the past. In order to test this hypothesis we have developed the SeGTE game and performed an evaluation of its effectiveness on conveying such pedagogical content.",10.5220/0004661504380444
12fc3be3e509f873e464f52440482cc4aa711dcf,The Framework of a Life Support Simulation Application,2012,,10.1016/J.PROCS.2012.10.083
0011a555a82a60a71dd70076bcdd0f23402e7d7c,Could Decision Trees Improve the Classification Accuracy and Interpretability of Loan Granting Decisions?,2010,"The paper compares the classification performance rate of eight models: logistic regression (LR), neural network (NN), radial basis function neural network (RBFNN), support vector machine (SVM), case-base reasoning (CBR), and three decision trees (DTs). We build models and test their classification accuracy rates on a historical data set provided by a German financial institution. The data set contains 21 financial attributes of 1000 customers. Though at the time of loan application all individuals deemed to the institution to be qualified to obtain a loan, 300 of them defaulted upon a loan and 700 paid it off. To obtain reliable and unbiased error estimates for each of the eight models we apply 10-fold cross-validation and repeat an experiment 10 times. We found that in the overall classification accuracy rates at 0.5 probability cut-off, two of the three DT models significantly outperformed (at alpha=0.05) the other remaining models. We then concentrate our attention on DT models and compare their performance at 0.3 and 0.7 cut-off levels which are more likely to be used by financial institutions. The DT models not only classify better than the other models, but the knowledge they learn in the form of if-then rules is easy to interpret, makes sense, and might be of value to financial institutions which may have to explain the reasons for a loan denial.",10.1109/HICSS.2010.124
4cbaf5067f3e62bdc5fd07d0acefa0b2aed7aaee,Anomaly Detection in Railway Sensor Data Environments: State-of-the-Art Methods and Empirical Performance Evaluation,2024,"To date, significant progress has been made in the field of railway anomaly detection using technologies such as real-time data analytics, the Internet of Things, and machine learning. As technology continues to evolve, the ability to detect and respond to anomalies in railway systems is once again in the spotlight. However, railway anomaly detection faces challenges related to the vast infrastructure, dynamic conditions, aging infrastructure, and adverse environmental conditions on the one hand, and the scale, complexity, and critical safety implications of railway systems on the other. Our study is underpinned by the three objectives. Specifically, we aim to identify time series anomaly detection methods applied to railway sensor device data, recognize the advantages and disadvantages of these methods, and evaluate their effectiveness. To address the research objectives, the first part of the study involved a systematic literature review and a series of controlled experiments. In the case of the former, we adopted well-established guidelines to structure and visualize the review. In the second part, we investigated the effectiveness of selected machine learning methods. To evaluate the predictive performance of each method, a five-fold cross-validation approach was applied to ensure the highest accuracy and generality. Based on the calculated accuracy, the results show that the top three methods are CatBoost (96%), Random Forest (91%), and XGBoost (90%), whereas the lowest accuracy is observed for One-Class Support Vector Machines (48%), Local Outlier Factor (53%), and Isolation Forest (55%). As the industry moves toward a zero-defect paradigm on a global scale, ongoing research efforts are focused on improving existing methods and developing new ones that contribute to the safety and quality of rail transportation. In this sense, there are at least four avenues for future research worth considering: testing richer data sets, hyperparameter optimization, and implementing other methods not included in the current study.",10.3390/s24082633
d0ec065f7f3a7239612de00859b75efabd3c9f7f,Stacking ensemble method for personal credit risk assessment in Peer-to-Peer lending,2023,,10.1016/j.asoc.2023.110302
4f85463b404876bc2d21a8f108817d25ca6288af,Classification Methods Applied to Credit Scoring With Collateral,2020,"Credit operations are indispensable in the organizational development of financial institutions. However, misconduct in these operations occurs, and this can lead to financial loss. These consequences are caused by incorrectly granting credit or incorrectly assigning customer ratings and can compromise a credit portfolio. The result shows that support vector machine is the most commonly used classifier for credit scores, and while the system performs well, it does not apply approaches with collateral. The analysis includes 84 studies in this article to propose using statistical methodology to conduct a meta-analysis to compare the results of classification methods. It shows some cases that consider various probability distributions and also survival data. It also elaborates that collateral is not the first approach for credit scoring. The credit scoring system can then give several starting credit scores according to the classifier the user wants to use.",10.1109/JSYST.2019.2937552
28dc34d817fe7047343f7a125b4ecf5379c2ca81,An Ontology-Based Model for Credit Scoring Knowledge in Microfinance: Towards a Better Decision Making,2020,"In developing countries, microfinance actors mobilized during the covid-19 pandemic to support the activities of their most vulnerable clients. In this context, the main concern of microfinance institutions is to minimize credit risk by adopting the most reliable scoring system possible. There are many dimensions to consider. In the literature, credit-scoring models essentially base on the financial dimension and neglect others deemed relevant. The study presented in this paper is based on a review of several models to identify aspects related to credit score in a microfinance context, in order to build an ontological model presenting the dimensions having an impact on credit score and their interrelations. The proposed model will help these institutions in their decision-making and in particular in the evaluation of the granting of loans.",10.1109/IS48319.2020.9199981
0011ef9fd18888cef786ffb0a6ef215f09dc1e03,Anomalous Path Detection for Spatial Crowdsourcing-Based Indoor Navigation System,2018,"Indoor navigation system provides customized path planning for requesters who are unfamiliar with the indoor environment, such as shopping mall and airport. Spatial crowd-sourcing technology can be applied to indoor navigation to offer fundamental services related to location. However, spatial crowdsourcing-based indoor navigation is vulnerable to the intrusion of injected anomalous paths from attackers. In this paper, we propose an anomalous path detection (APD) scheme to classify attackers according to their reputation management and abnormal trajectory sequence. Specifically, we first develop a crowdsourcing system to support the indoor location service using the fog as the spatial crowdsourcing server. Then, we identify two levels of attackers, i.e., the malicious responders and the semi-honest responders in the indoor environment according to their attacking purposes. Through the responders' historical records from the fog server, we analyze a series of trajectory sequences consisting of the distance between the current position and the destination to distinguish the semi-honest responders from the normal. In addition, we propose a semi-supervised learning with hidden Markov model (HMM) to detect the semi-honest responders. Finally, the extensive simulations show that the APD scheme can achieve higher accuracy with the acceptable false rate.",10.1109/GLOCOM.2018.8647174
99165d1f75e67895c7bbc8270ced6f92beebbaa1,Cloud Platforms for Context-Adaptive Positioning and Localisation in GNSS-Denied Scenarios—A Systematic Review,2021,"Cloud Computing and Cloud Platforms have become an essential resource for businesses, due to their advanced capabilities, performance, and functionalities. Data redundancy, scalability, and security, are among the key features offered by cloud platforms. Location-Based Services (LBS) often exploit cloud platforms to host positioning and localisation systems. This paper introduces a systematic review of current positioning platforms for GNSS-denied scenarios. We have undertaken a comprehensive analysis of each component of the positioning and localisation systems, including techniques, protocols, standards, and cloud services used in the state-of-the-art deployments. Furthermore, this paper identifies the limitations of existing solutions, outlining shortcomings in areas that are rarely subjected to scrutiny in existing reviews of indoor positioning, such as computing paradigms, privacy, and fault tolerance. We then examine contributions in the areas of efficient computation, interoperability, positioning, and localisation. Finally, we provide a brief discussion concerning the challenges for cloud platforms based on GNSS-denied scenarios.",10.3390/s22010110
d1f1d4c74cd03d7560e740578ce88e8d0c1d3d32,A Game-Theoretical Approach for Secure Crowdsourcing-Based Indoor Navigation System With Reputation Mechanism,2021,"At present, the crowdsourcing-based indoor navigation system (CINS) has attracted extensive attention from both industry and academia owing to its low-cost and high-accuracy performance. Unfortunately, the system that relies on crowdsourced data is vulnerable to the collusion attack, which leads to severe security issues. To address the security issues in the CINS, we propose to utilize a fully trusted fog server platform to advocate secure transactions between service requesters and responders. First, we propose a novel reputation incentive mechanism based on the behaviors of responders. Then, we employ the offensive and defensive game to model the interactions between the fog server platform and the responders, whereby a social welfare optimization problem is formulated to maximize the social welfare of the system. Next, the game equilibriums are found by using the replicator dynamic equation while the game stability is discussed. Finally, the simulation results show that the proposed mechanism can effectively encourage responders to provide positive navigation services and obtain more social welfare of the system compared with the conventional mechanisms.",10.1109/JIOT.2021.3111999
f794f0cc1b55ea8cf26bf0372bc7ba6740ca8e83,Blockchain-Based Data Security for Artificial Intelligence Applications in 6G Networks,2020,"The sixth generation (6G) networks are expected to provide a fully connected world with terrestrial wireless and satellite communications integration. The design concept of 6G networks is to leverage artificial intelligence (Ai) to promote the intelligent and agile development of network services. intelligent services inevitably involve the processing of large amounts of data, such as storage, computing, and analysis, such that the data may be vulnerable to tampering or contamination by attackers. in this article, we propose a blockchain-based data security scheme for Ai applications in 6G networks. Specifically, we first introduce the 6G architecture (i.e., a space-air-ground-underwater integrated network). Then we discuss two Ai-enabled applications, indoor positioning and autonomous vehicle, in the context of 6G. Through a case study of an indoor navigation system, we demonstrate the effectiveness of blockchain in data security. The integration of Ai and blockchain is developed to evaluate and optimize the quality of intelligent service. Finally, we discuss several open issues about data security in the upcoming 6G networks.",10.1109/MNET.021.1900629
48615c3cba178c4193d5106b59205861217fb52f,Abnormal Crowd Traffic Detection for Crowdsourced Indoor Positioning in Heterogeneous Communications Networks,2020,"WiFi fingerprint-based indoor positioning system emerges to provide fundamental location-related service in heterogeneous communications networks. It relies on crowdsourcing technology in the collection of received signal strength (RSS) to dynamically update fingerprint database. However, this crowdsourced indoor positioning system is vulnerable to the intrusion of dishonest users (i.e., attackers). Attackers may manipulate the number of users who submit RSS fingerprints, and finally mislead the evaluation of crowd traffic evaluation. In this paper, we propose an abnormal crowd traffic detection (ACTD) scheme to identify attackers according to their abnormal RSS sensing behaviors. Specifically, a fog server is explored to serve as the crowdsourcing platform to perform data storage and detection. We first categorize attackers into three different levels according to their real geographical locations and collusion. Then, through the analysis of pseudonym changing behavior in RSS submission, we propose a rarity-based outlier detection to classify attackers of level-1. Furthermore, we propose a variable-length Markov model, i.e., probabilistic suffix tree (PST), to detect the colluded users who are not at the target point of interest (POI). In addition, a metric learning algorithm is developed to detect the collusion of AP organizer based on RSS fingerprint distance difference. The extensive simulation results show that the ACTD scheme can effectively resist attackers with high accuracy and appropriately deal with traffic evaluation from RSS fingerprint information.",10.1109/TNSE.2020.3014380
55f0404b6e67405f2fa0ef7ba3550f4fa86c5631,Defending Malicious Check-In Using Big Data Analysis of Indoor Positioning System: An Access Point Selection Approach,2020,"The integration of WiFi fingerprint-based indoor positioning technology and big data analysis emerges as a new research prospect. Through the analysis of big data collected from users’ submission, we can discover many other applications of fingerprint positioning. A popular application is the check-in to point of interest (POI) for its crowd traffic evaluation according to the volume of received signal strength (RSS) fingerprints submitted by users. However, this crowd traffic evaluation method may be susceptible to the intrusion of malicious check-in behaviors. Attackers who are not at the target POI submit the self-modification RSS fingerprints that can be located at the target POI in order to illegally increase its crowd traffic. To this end, we propose a malicious check-in defense scheme based on the access point (AP) selection to resist attackers who aim to successfully initiate the fingerprint modification. Specifically, the distance between different POIs in fingerprint space is firstly developed for AP selection. Then, in order to increase the robustness of selected AP subset, we propose the mutual information among different classes as a selection condition. Through the multiobjective optimization and Pareto optimality, we can obtain the best AP subset to participate in the computation of positioning algorithm. Furthermore, the optimal modified fingerprint is searched by the level set method (LSM), which can be utilized to measure the costs of attackers and the robustness of the system. In addition, we propose an iterative weight updating method based on classification error to learn the optimal weight in order to balance the positioning accuracy and robustness. We finally carry out extensive simulations to validate that the POI crowd traffic can be assessed in terms of the RSS fingerprint-related information and our proposed scheme can perform high robustness to resist malicious check-in.",10.1109/TNSE.2020.3014384
3377cd2febd3162747072d433cd8df9e6652b124,Bayesian dynamic profiling and optimization of important ranked energy from gray level co-occurrence (GLCM) features for empirical analysis of brain MRI,2022,"Accurate classification of brain tumor subtypes is important for prognosis and treatment. Researchers are developing tools based on static and dynamic feature extraction and applying machine learning and deep learning. However, static feature requires further analysis to compute the relevance, strength, and types of association. Recently Bayesian inference approach gains attraction for deeper analysis of static (hand-crafted) features to unfold hidden dynamics and relationships among features. We computed the gray level co-occurrence (GLCM) features from brain tumor meningioma and pituitary MRIs and then ranked based on entropy methods. The highly ranked Energy feature was chosen as our target variable for further empirical analysis of dynamic profiling and optimization to unfold the nonlinear intrinsic dynamics of GLCM features extracted from brain MRIs. The proposed method further unfolds the dynamics and to detailed analysis of computed features based on GLCM features for better understanding of the hidden dynamics for proper diagnosis and prognosis of tumor types leading to brain stroke.",10.1038/s41598-022-19563-0
8b9411fc2a95899be9a4a543736f6d4348a64759,Future Challenges of Particulate Matters (PMs) Monitoring by Computing Associations Among Extracted Multimodal Features Applying Bayesian Network Approach,2022,"ABSTRACT The particulate matter (PM) is emitted from diverse sources and affects the human health very badly. In the past, researchers applied different automated computational tools in the predication of PM. Accurate prediction of PM requires more relevant features and feature importance. In this research, we first extracted the multimodal features from time domain standard deviation average (SDAPM), standard deviation of standard deviation (SDSD), standard deviation of particulate matter (SDPM), root-mean square of standard deviation (RMSSD), and nonlinear dynamical measure wavelet entropy (WE) – Shannon, norm, threshold, multiscale entropy based on KD tree (MSEKD), and multiscale approximate entropy (MAEnt). We then applied the intelligent-based Bayesian inference approach to compute the strength of relationship among multimodal features. We also computed total incoming and outgoing forces between the features (nodes). The results reveal that there was a very highly significant correlation (p-value <0.05) between the selected nodes. The highest total force was yielded by WE-norm followed by SDAPM and SDPM. The association will further help to investigate that which extracted features are more positively or negatively correlated and associated with each other. The results revealed that the proposed methodology can further provide deeper insights into computing the association among the features.",10.1080/08839514.2022.2112545
9a1ec46c7afe7e06b5d09d68040269a1cb2f8d50,Finding lncRNA-Protein Interactions Based on Deep Learning With Dual-Net Neural Architecture,2021,"The identification of lncRNA-protein interactions (LPIs) is important to understand the biological functions and molecular mechanisms of lncRNAs. However, most computational models are evaluated on a unique dataset, thereby resulting in prediction bias. Furthermore, previous models have not uncovered potential proteins (or lncRNAs) interacting with a new lncRNA (or protein). Finally, the performance of these models can be improved. In this study, we develop a Deep Learning framework with Dual-net Neural architecture to find potential LPIs (LPI-DLDN). First, five LPI datasets are collected. Second, the features of lncRNAs and proteins are extracted by Pyfeat and BioTriangle, respectively. Third, these features are concatenated as a vector after dimension reduction. Finally, a deep learning model with dual-net neural architecture is designed to classify lncRNA-protein pairs. LPI-DLDN is compared with six state-of-the-art LPI prediction methods (LPI-XGBoost, LPI-HeteSim, LPI-NRLMF, PLIPCOM, LPI-CNNCP, and Capsule-LPI) under four cross validations. The results demonstrate the powerful LPI classification performance of LPI-DLDN. Case study analyses show that there may be interactions between RP11-439E19.10 and Q15717, and between RP11-196G18.22 and Q9NUL5. The novelty of LPI-DLDN remains, integrating various biological features, designing a novel deep learning-based LPI identification framework, and selecting the optimal LPI feature subset based on feature importance ranking.",10.1109/TCBB.2021.3116232
2c9873f06b99fb867a0545a13a9f0fe0e33b1326,Analysis of Force Profile Features in Spinal Manipulation Therapy,2023,"Spinal manipulation therapy (SMT) is widely used as an intervention for musculoskeletal conditions. However, the automated detection and analysis of force profile features in SMT have received limited attention. This study aims to address this research gap by developing a toolbox for the automatic detection and annotation of force-time profile features in SMT. For validation purposes, we will investigate the correlation between these features and characteristics of patient vignettes. Force data was collected from 1233 SMT interventions using a commercially available pressure sensor. With the aggregation of three feature selection methods (Chi squared, MRMR, and ReliefF), the results indicate a significant increase in maximum thrust speed for mentally envisioned athletic male patients compared to elderly females ( $p< 0.01$ ). To the best of our knowledge, this study is the first of its kind, representing a pioneering exploration of automated force profile analysis in SMT. The findings hold immense potential to advance technology, support training of manual interventions, and facilitate the development of objective treatment feedback tools. The observed correlations between the extracted features and patient characteristics provide valuable insights for personalized SMT approaches.",10.1109/ACCESS.2023.3332754
00133d41d5ecef1a9d046d2b92bb2a23a335cb7c,TempLe: Learning Template of Transitions for Sample Efficient Multi-task RL,2020,"Transferring knowledge among various environments is important for efficiently learning multiple tasks online. Most existing methods directly use the previously learned models or previously learned optimal policies to learn new tasks. However, these methods may be inefficient when the underlying models or optimal policies are substantially different across tasks. In this paper, we propose Template Learning (TempLe), a PAC-MDP method for multi-task reinforcement learning that could be applied to tasks with varying state/action space without prior knowledge of inter-task mappings. TempLe gains sample efficiency by extracting similarities of the transition dynamics across tasks even when their underlying models or optimal policies have limited commonalities. We present two algorithms for an ``online'' and a ``finite-model'' setting respectively. We prove that our proposed TempLe algorithms achieve much lower sample complexity than single-task learners or state-of-the-art multi-task methods. We show via systematically designed experiments that our TempLe method universally outperforms the state-of-the-art multi-task methods (PAC-MDP or not) in various settings and regimes.",10.1609/aaai.v35i11.17174
40bd70d2d176d2112259b7350cccfc058e6b7afa,Value function optimistic initialization with uncertainty and confidence awareness in lifelong reinforcement learning,2023,,10.1016/j.knosys.2023.111036
aeeaf09a81bcf2ee5b8b4c41b97c34e4c20ab008,Scaling Up Q-Learning via Exploiting State–Action Equivalence,2023,"Recent success stories in reinforcement learning have demonstrated that leveraging structural properties of the underlying environment is key in devising viable methods capable of solving complex tasks. We study off-policy learning in discounted reinforcement learning, where some equivalence relation in the environment exists. We introduce a new model-free algorithm, called QL-ES (Q-learning with equivalence structure), which is a variant of (asynchronous) Q-learning tailored to exploit the equivalence structure in the MDP. We report a non-asymptotic PAC-type sample complexity bound for QL-ES, thereby establishing its sample efficiency. This bound also allows us to quantify the superiority of QL-ES over Q-learning analytically, which shows that the theoretical gain in some domains can be massive. We report extensive numerical experiments demonstrating that QL-ES converges significantly faster than (structure-oblivious) Q-learning empirically. They imply that the empirical performance gain obtained by exploiting the equivalence structure could be massive, even in simple domains. To the best of our knowledge, QL-ES is the first provably efficient model-free algorithm to exploit the equivalence structure in finite MDPs.",10.3390/e25040584
e046eaa83e43c5663332eac4f9a6797931e8c6cd,A novel fuzzy ARTMAP with area of influence,2020,,10.1016/j.neucom.2020.11.053
ba31105df62c9118f22ad70637aa7dc8c11a80dd,A Survey of Adaptive Resonance Theory Neural Network Models for Engineering Applications,2019,,10.1016/j.neunet.2019.09.012
9df1b2509eb2e81da2bf3414b52e402a20dbf89e,OnARTMAP: A Fuzzy ARTMAP-based Architecture,2018,,10.1016/j.neunet.2017.11.012
4bc598267a691eb3b7a59197d82f2bda3a421c54,Bayesian ARTMAP for regression,2013,,10.1016/j.neunet.2013.04.006
001431f69d0268a4497f39002c412c728ff44276,Evolutionary Extreme Learning Machine Based Weighted Nearest-Neighbor Equality Classification,2015,"Feature significance plays an important role in the classification tasks. The performance of a classifier would be degraded due to the existence of the irrelevant features, which are often inevitable in the real applications. In order to distinguish the impacts implicated in the features and improve the performances of the classification methods, this paper presents a hybrid learning approach, entitled evolutionary extreme learning machine based weighted nearest-neighbor equality algorithm (EE-WNNE). In such method, the measure of the significance levels of the features are induced by the weights on the related links associated with the individual input nodes in the evolutionary extreme learning machine (E-ELM) algorithm. These feature weights are utilized to implement a weighted nearest-neighbor equality method to perform the subsequent classification tasks. Systematic experimental results demonstrate that the proposed approach generally outperform many state-of-the-art classification techniques.",10.1109/IHMSC.2015.181
cef530e1a4b270262e1d17327711d4c59a7d99a5,A review on extreme learning machine,2021,"Extreme learning machine (ELM) is a training algorithm for single hidden layer feedforward neural network (SLFN), which converges much faster than traditional methods and yields promising performance. In this paper, we hope to present a comprehensive review on ELM. Firstly, we will focus on the theoretical analysis including universal approximation theory and generalization. Then, the various improvements are listed, which help ELM works better in terms of stability, efficiency, and accuracy. Because of its outstanding performance, ELM has been successfully applied in many real-time learning tasks for classification, clustering, and regression. Besides, we report the applications of ELM in medical imaging: MRI, CT, and mammogram. The controversies of ELM were also discussed in this paper. We aim to report these advances and find some future perspectives.",10.1007/s11042-021-11007-7
29330205c83a1752da4fbbc36ba4d34cef2d55bf,An advanced approach for optimal wind power generation prediction intervals by using self-adaptive evolutionary extreme learning machine,2018,,10.1016/J.RENENE.2018.03.035
d40a3d4b53c613fbd487c2d8fc02b59ae7038035,Proposed Consistent Exception Handling for the BLAS and LAPACK,2022,"Numerical exceptions, which may be caused by overflow, operations like division by 0 or sqrt(−1), or conver-gence failures, are unavoidable in many cases, in particular when software is used on unforeseen and difficult inputs. As more aspects of society become automated e.g., self-driving cars, health monitors, and cyber-physical systems more generally, it is becoming increasingly important to design software that is resilient to exceptions, and that responds to them in a consistent way. Consistency is needed to allow users to build higher-level software that is also resilient and consistent (and so on recursively). In this paper we explore the design space of consistent exception handling for the widely used BLAS and LAPACK linear algebra libraries, pointing out a variety of instances of inconsistent exception handling in the current versions, and propose a new design that balances consistency, complexity, ease of use, and performance. Some compromises are needed, because there are preexisting inconsistencies that are outside our control, including in or between existing vendor BLAS implementations, different programming languages, and even compilers for the same programming language. And user requests from our surveys are quite diverse. We also propose our design as a possible model for other numerical software, and welcome comments on our design choices.",10.1109/Correctness56720.2022.00006
10c6e5070faa24d2f7a823eff0c40f93d4ef8291,Predoo: precision testing of deep learning operators,2021,"Deep learning(DL) techniques attract people from various fields with superior performance in making progressive breakthroughs. To ensure the quality of DL techniques, researchers have been working on testing and verification approaches. Some recent studies reveal that the underlying DL operators could cause defects inside a DL model. DL operators work as fundamental components in DL libraries. Library developers still work on practical approaches to ensure the quality of operators they provide. However, the variety of DL operators and the implementation complexity make it challenging to evaluate their quality. Operator testing with limited test cases may fail to reveal hidden defects inside the implementation. Besides, the existing model-to-library testing approach requires extra labor and time cost to identify and locate errors, i.e., developers can only react to the exposed defects. This paper proposes a fuzzing-based operator-level precision testing approach to estimate individual DL operators' precision errors to bridge this gap. Unlike conventional fuzzing techniques, valid shape variable inputs and fine-grained precision error evaluation are implemented. The testing of DL operators is treated as a searching problem to maximize output precision errors. We implement our approach in a tool named Predoo and conduct an experiment on seven DL operators from TensorFlow. The experiment result shows that Predoo can trigger larger precision errors compared to the error threshold declared in the testing scripts from the TensorFlow repository.",10.1145/3460319.3464843
72f9ccea442fdbd7501c6684996d3c2033e0ba76,A Practical Approach to Verification of Floating-Point C/C++ Programs with math.h/cmath Functions,2020,"Verification of C/C++ programs has seen considerable progress in several areas, but not for programs that use these languages’ mathematical libraries. The reason is that all libraries in widespread use come with no guarantees about the computed results. This would seem to prevent any attempt at formal verification of programs that use them: without a specification for the functions, no conclusion can be drawn statically about the behavior of the program. We propose an alternative to surrender. We introduce a pragmatic approach that leverages the fact that most math.h/cmath functions are almost piecewise monotonic: as we discovered through exhaustive testing, they may have glitches, often of very small size and in small numbers. We develop interval refinement techniques for such functions based on a modified dichotomic search, which enable verification via symbolic execution based model checking, abstract interpretation, and test data generation. To the best of our knowledge, our refinement algorithms are the first in the literature to be able to handle non-correctly rounded function implementations, enabling verification in the presence of the most common implementations. We experimentally evaluate our approach on real-world code, showing its ability to detect or rule out anomalous behaviors.",10.1145/3410875
778f04e582e6916db7297b331f19b80461551a3d,Discovering discrepancies in numerical libraries,2020,"Numerical libraries constitute the building blocks for software applications that perform numerical calculations. Thus, it is paramount that such libraries provide accurate and consistent results. To that end, this paper addresses the problem of finding discrepancies between synonymous functions in different numerical libraries as a means of identifying incorrect behavior. Our approach automatically finds such synonymous functions, synthesizes testing drivers, and executes differential tests to discover meaningful discrepancies across numerical libraries. We implement our approach in a tool named FPDiff, and provide an evaluation on four popular numerical libraries: GNU Scientific Library (GSL), SciPy, mpmath, and jmat. FPDiff finds a total of 126 equivalence classes with a 95.8% precision and 79% recall, and discovers 655 instances in which an input produces a set of disagreeing outputs between function synonyms, 150 of which we found to represent 125 unique bugs. We have reported all bugs to library maintainers; so far, 30 bugs have been fixed, 9 have been found to be previously known, and 25 more have been acknowledged by developers.",10.1145/3395363.3397380
57f2a6f30e5bbebb4dbe774367dd037b2ef953bb,Efficient Generation of Error-Inducing Floating-Point Inputs via Symbolic Execution,2020,"Floating point is widely used in software to emulate arithmetic over reals. Unfortunately, floating point leads to rounding errors that propagate and accumulate during execution. Generating inputs to maximize the numerical error is critical when evaluating the accuracy of floating-point code. In this paper, we formulate the problem of generating high error-inducing floating-point inputs as a code coverage maximization problem solved using symbolic execution. Specifically, we define inaccuracy checks to detect large precision loss and cancellation. We inject these checks at strategic program locations to construct specialized branches that, when covered by a given input, are likely to lead to large errors in the result. We apply symbolic execution to generate inputs that exercise these specialized branches, and describe optimizations that make our approach practical. We implement a tool named FPGen and present an evaluation on 21 numerical programs including matrix computation and statistics libraries. We show that FPGen exposes errors for 20 of these programs and triggers errors that are, on average, over 2 orders of magnitude larger than the state of the art.",10.1145/3377811.3380359
261e27ffa7678bc4fc043486594a0e72b25e91fa,Correct approximation of IEEE 754 floating-point arithmetic for program verification,2019,"Verification of programs using floating-point arithmetic is challenging on several accounts. One of the difficulties of reasoning about such programs is due to the peculiarities of floating-point arithmetic: rounding errors, infinities, non-numeric objects (NaNs), signed zeroes, denormal numbers, different rounding modes, etc. One possibility to reason about floating-point arithmetic is to model a program computation path by means of a set of ternary constraints of the form and use constraint propagation techniques to infer new information on the variables’ possible values. In this setting, we define and prove the correctness of algorithms to precisely bound the value of one of the variables x, y or z, starting from the bounds known for the other two. We do this for each of the operations and for each rounding mode defined by the IEEE 754 binary floating-point standard, even in the case the rounding mode in effect is only partially known. This is the first time that such so-called filtering algorithms are defined and their correctness is formally proved. This is an important slab for paving the way to formal verification of programs that use floating-point arithmetics.",10.1007/s10601-021-09322-9
f5f47322e3209d2d5de71ab21ed300a9f93281ff,Exploiting community structure for floating-point precision tuning,2018,"Floating-point types are notorious for their intricate representation. The effective use of mixed precision, i.e., using various precisions in different computations, is critical to achieve a good balance between accuracy and performance. Unfortunately, reasoning about mixed precision is difficult even for numerical experts. Techniques have been proposed to systematically search over floating-point variables and/or program instructions to find a faster, mixed-precision version of a given program. These techniques, however, are characterized by their black box nature, and face scalability limitations due to the large search space. In this paper, we exploit the community structure of floating-point variables to devise a scalable hierarchical search for precision tuning. Specifically, we perform dependence analysis and edge profiling to create a weighted dependence graph that presents a network of floating-point variables. We then formulate hierarchy construction on the network as a community detection problem, and present a hierarchical search algorithm that iteratively lowers precision with regard to communities. We implement our algorithm in the tool HiFPTuner, and show that it exhibits higher search efficiency over the state of the art for 75.9% of the experiments taking 59.6% less search time on average. Moreover, HiFPTuner finds more profitable configurations for 51.7% of the experiments, with one known to be as good as the global optimum found through exhaustive search.",10.1145/3213846.3213862
0014a057ebdeca672b1cdee8104cca4dc928ef3e,Training Deformable Part Models with Decorrelated Features,2013,"In this paper, we show how to train a deformable part model (DPM) fast-typically in less than 20 minutes, or four times faster than the current fastest method-while maintaining high average precision on the PASCAL VOC datasets. At the core of our approach is ""latent LDA,"" a novel generalization of linear discriminant analysis for learning latent variable models. Unlike latent SVM, latent LDA uses efficient closed-form updates and does not require an expensive search for hard negative examples. Our approach also acts as a springboard for a detailed experimental study of DPM training. We isolate and quantify the impact of key training factors for the first time (e.g., How important are discriminative SVM filters? How important is joint parameter estimation? How many negative images are needed for training?). Our findings yield useful insights for researchers working with Markov random fields and part-based models, and have practical implications for speeding up tasks such as model selection.",10.1109/ICCV.2013.375
75d63ceceae638750f4a4ed5f98044075ec66232,Convolutional Neural Network for Blur Images Detection as an Alternative for Laplacian Method,2020,"With the prevalence of digital cameras, the number of digital images increases quickly, which raises the demand for non-manual image quality assessment. While there are many methods considered useful for detecting blurriness, in this paper we propose and evaluate a new method that uses a deep convolutional neural network, which can determine whether an image is blurry or not. Experimental results demonstrate the effectiveness of the proposed scheme and are compared to deterministic methods using the confusion matrix.",10.1109/SSCI47803.2020.9308594
eaa308e5e0a620ea5f9df56dfd522ee517ecb529,Real-time sow behavior detection based on deep learning,2019,,10.1016/J.COMPAG.2019.104884
0014c6eaa38d7a4da24dd0258572a13c45b93b16,Learning Correlation Structures for Vision Transformers,2024,"We introduce a new attention mechanism, dubbed structural self-attention (StructSA), that leverages rich correlation patterns naturally emerging in key-query interactions of attention. StructSA generates attention maps by recog-nizing space-time structures of key-query correlations via convolution and uses them to dynamically aggregate lo-cal contexts of value features. This effectively leverages rich structural patterns in images and videos such as scene layouts, object motion, and inter-object relations. Using StructSA as a main building block, we develop the structural vision transformer (StructViT) and evaluate its effective-ness on both image and video classification tasks, achieving state-of-the-art results on ImageNet-I K, Kinetics-400, Something-Something VI & V2, Diving-48, and FineGym.",10.1109/CVPR52733.2024.01792
001528e5987d8864a0187dff279eafdce0fbe703,RUMBA-Mouse: Rapid User Mouse-Behavior Authentication Using a CNN-RNN Approach,2020,"Mouse behavior analysis has become an increasingly attractive area to biometric researchers in recent years. Many mouse behavior based user authentication schemes have been proposed in the past decades. However, most of them rely on statistical analysis or heuristic feature extraction of the mouse behavior. In this paper, we present a CNN-RNN combined neural network model for mouse behavior based user authentication, which takes raw sequential mouse data as input rather than relies on heuristic feature extraction. Additionally, we integrate the model with a practical framework of static user authentication and evaluate it on a real dataset. The results show our approach yields a 3.16% EER and a 99.39% AUC, with a short authentication delay of 6.11 seconds on average, which demonstrates the effectiveness and practicality of applying deep learning techniques for static mouse behavior based user authentication. Furthermore, by modifying the activation maximization method, we study and visualize the features learned by different layers of our neural network.",10.1109/CNS48642.2020.9162287
9c04bf14dc03a4290d16e936166a611aac770ae4,Artificial Intelligence Meets Kinesthetic Intelligence: Mouse-based User Authentication based on Hybrid Human-Machine Learning,2022,"Current mainstream biometric user authentication approaches are based on passive measurements of the subject's characteristics, and usually come with less-than-satisfactory accuracy. This paper takes a unique approach to biometric authentication. Specifically, instead of training a machine learning algorithm to recognize a legitimate user, the paper proposes a hybrid type of training, in which the legitimate user is also trained to use a customized instance of the machine. The user thus achieves a level of artificially-induced expertise to interact with the machine, which makes the user easier to recognize. We implement this concept in a mouse-based user authentication system, in which we produce customized machine instances by introducing an angle offset to the standard mouse. Human subjects then rely on their kinesthetic intelligence to achieve motor learning and visual-motor adaptation to the modified mouse. We design a 7-week IRB-approved experiment, collect data from 18 human subjects over this period, and evaluate the proposed approach with two existing state-of-the-art mouse-based authentication schemes. We find that, in both schemes, our approach significantly outperforms the baseline in which a regular unaltered mouse is used. Somewhat surprisingly, results also show that our approach improves the authentication performance even when both legitimate and non-legitimate users are trained to exactly the same instance of customized machine (i.e., the same mouse angle offset). In addition, we also observe that users can generally maintain their learned expertise even after one week of washout, which further demonstrates the practicality of the approach. Finally, we present a practical strategy to manage the enrollment of users in such a proposed system.",10.1145/3488932.3523257
0fdfaa6cf51661504ae1d2af70b2bd3a42e9478a,"MAUSPAD: Mouse-based Authentication Using Segmentation-based, Progress-Adjusted DTW",2020,"Biometric user authentication is at the core of multifactor authentication, and mouse-based biometric authentication comes at no additional cost for most computer systems. This paper describes a mouse-based user authentication scheme, called MAUSPAD, which uses a novel progress-adjusted dynamic time warping (PADTW) algorithm, along with a segmentation algorithm, to accurately and meaningfully measure the differences between observed data and reference data. By introducing a new concept, which we call progress, into standard DTW, the new PADTW can have better control of the warping and mapping process and hence is more suitable for comparing time-stamped spatial sequences such as mouse cursor movements. Furthermore, in order to preserve the important but transient details in the cursor movement (which may be critical in identifying a specific user), we apply a segmentation algorithm to divide each reference cursor movement into multiple smaller segments, and measure the differences between cursor movements at the segment level. Evaluation results on two mouse-behavior datasets show that MAUSPAD yields the best overall performance among tested schemes, and demonstrate the effectiveness of PADTW over DTW, and segmentation over non-segmentation. The processing techniques developed herein can be extended to applications that rely on sequence comparison, and where relevant sequence information spans multiple semantic domains.",10.1109/TrustCom50675.2020.00065
b7b9606177eb106feb35b3619332763d5543a177,User Re-Authentication via Mouse Movements and Recurrent Neural Networks,2024,": Behavioral biometrics can determine whether a user interaction has been performed by a legitimate user or an impersonator. In this regard, user re-authentication based on mouse movements has emerged as a reliable and accessible solution, without being intrusive or requiring any explicit input from the user other than regular interactions. Previous work has reported remarkably good classification performance when predicting imper-sonated mouse movements, however, it has relied on manual data preprocessing or ad-hoc feature extraction methods. In this paper, we design and contrast different recurrent neural networks that take as input raw mouse movements, represented by discrete sequences of coordinate derivatives (coordinate offsets relative to time), as a mean of user re-authentication that could be used on web platforms. We show that a 2-layer BiGRU model outperforms state-of-the-art approaches while being much simpler and more efficient. Our software and models are publicly available.",10.5220/0012296600003648
0015948be940cc9d976ef3874be9cad30b4710be,Visual Quality Enhancement Of Images Under Adverse Weather Conditions,2018,"The visual quality of an image captured by vision systems can degrade significantly under adverse weather conditions. In this paper we propose a deep learning based solution to improve the visual quality of images captured under rainy and foggy circumstances, which are among the prominent and common weather conditions that attribute to bad image quality. Our convolutional neural network(CNN), NVDeHazenet learns to predict both the original signal as well as the atmospheric light to finally restore image quality. It outperforms the existing state of the art methods by evaluation on both synthetic data as well as real world hazy images. The deraining CNN, NVDeRainNet shows similar performance on existing rain datasets as the state of the art. On natural rain images NVDeRainNet shows better than state of the art performance. We show the use of perceptual loss to improve the visual quality of results. These networks require considerable amount of data under adverse weather conditions and their respective ground truth for training. For this purpose we use a weather simulation framework to simulate synthetic rainy and foggy environments. This data is augmented with existing rain datasets to train the networks.",10.1109/ITSC.2018.8569536
3bf289262439bc9e024d6d590d26cc517534b517,Enhancing the robustness of the convolutional neural networks for traffic sign detection,2021,"The detection of traffic signs in clean and noise-free images has been investigated by numerous researchers; however, very few of these works have focused on noisy environments. While in the real world, for different reasons (e.g. the speed and acceleration of a vehicle and the roughness around it), the input images of the convolutional neural networks (CNNs) could be extremely noisy. Contrary to other research works, in this paper, we investigate the robustness of the deep learning models against the synthetically modeled noises in the detection of small objects. To this end, the state-of-the-art architectures of Faster-RCNN Resnet101, R-FCN Resnet101, and Faster-RCNN Inception Resnet V2 are trained by means of the Tsinghua-Tencent 100K database, and the performances of the trained models on noisy data are evaluated. After verifying the robustness of these models, different training scenarios (1 – Modeling various climatic conditions, 2 – Style randomization, and 3 – Augmix augmentation) are used to enhance the model robustness. The findings indicate that these scenarios result in up to 13.09%, 12%, and 13.61% gains in the mentioned three networks by means of the mPC metric. They also result in 11.74%, 8.89%, and 7.27% gains in the rPC metric, demonstrating that improvement in robustness does not lead to performance drop on the clean data.",10.1177/09544070211042961
656b089967c544d32d0c0e6afdde2671aad3b04e,Towards Robust CNN-based Object Detection through Augmentation with Synthetic Rain Variations,2019,"Convolutional Neural Networks (CNNs) achieve high accuracy in vision-based object detection tasks. For their usage in the automotive domain, CNNs have to be robust against various kinds of natural distortions caused by different weather conditions while state-of-the-art datasets like KITTI lack these challenging scenarios. Our approach automatically identifies corner cases where CNNs fail and improves their robustness by automated augmentation of the training data with synthetic rain variations including falling rain with brightness reduction as well as raindrops on the windshield. Our method achieves higher performance upon validation against a real rain dataset compared with state-of-the art data augmentation techniques like Gaussian noise (GN) or Salt-and-Pepper noise (SPN).",10.1109/ITSC.2019.8917269
b97d8b27224efb6655091393f27441b6ed8e6395,Benchmarking the Robustness of Semantic Segmentation Models with Respect to Common Corruptions,2019,"When designing a semantic segmentation model for a real-world application, such as autonomous driving, it is crucial to understand the robustness of the network with respect to a wide range of image corruptions. While there are recent robustness studies for full-image classification, we are the first to present an exhaustive study for semantic segmentation, based on many established neural network architectures. We utilize almost 400,000 images generated from the Cityscapes dataset, PASCAL VOC 2012, and ADE20K. Based on the benchmark study, we gain several new insights. Firstly, many networks perform well with respect to real-world image corruptions, such as a realistic PSF blur. Secondly, some architecture properties significantly affect robustness, such as a Dense Prediction Cell, designed to maximize performance on clean data only. Thirdly, the generalization capability of semantic segmentation models depends strongly on the type of image corruption. Models generalize well for image noise and image blur, however, not with respect to digitally corrupted data or weather corruptions.",10.1007/s11263-020-01383-2
00159a3f909534e328276ac4a4424a100802708c,Regularization techniques for fine-tuning in neural machine translation,2017,"We investigate techniques for supervised domain adaptation for neural machine translation where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset. In this scenario, overfitting is a major challenge. We investigate a number of techniques to reduce overfitting and improve transfer learning, including regularization techniques such as dropout and L2-regularization towards an out-of-domain prior. In addition, we introduce tuneout, a novel regularization technique inspired by dropout. We apply these techniques, alone and in combination, to neural machine translation, obtaining improvements on IWSLT datasets for English→German and English→Russian. We also investigate the amounts of in-domain training data needed for domain adaptation in NMT, and find a logarithmic relationship between the amount of training data and gain in BLEU score.",10.18653/v1/D17-1156
efad4f0d7d54f0802bf101089338251f20d78064,How Much Data is Enough Data? Fine-Tuning Large Language Models for In-House Translation: Performance Evaluation Across Multiple Dataset Sizes,2024,"In this study, we explore the effectiveness of fine-tuning Large Language Models (LLMs), particularly Llama 3 8B Instruct, using translation memories (TMs) for hyper-specific machine translation (MT) tasks. Decoder-only LLMs have shown impressive performance in MT due to their ability to learn from extensive datasets and generate high quality translations. However, LLMs often struggle with the nuances and style required for organisation-specific translation so we leverage TMs, which store human translated segments, as a valuable resource to enhance translation accuracy and efficiency. We investigate the impact of fine-tuning the Llama 3 model using TMs from a specific organisation in the software sector. Our experiments cover five translation directions across languages of varying resource levels (English to Brazilian Portuguese, Czech, German, Finnish, and Korean). We analyse diverse sizes of training datasets (1k to 100k+ segments) to evaluate their influence on translation quality. We fine-tune separate models for each training set and evaluate their performance based on automatic metrics, BLEU, chrF++, TER, and COMET. Our findings reveal improvement in translation performance with larger datasets across all metrics. On average, BLEU and COMET scores increase by 13 and 25 points respectively on the largest training set against the baseline model. Notably, there is a performance deterioration in comparison with the baseline model when fine-tuning on only 1k and 2k examples; however, we observe a substantial improvement as the training dataset size increases. The study highlights the potential of integrating TMs with LLMs to create bespoke translation models tailored to the specific needs of businesses, therefore enhancing translation quality and reducing turn-around times. This approach offers a valuable insight for organisations seeking to leverage TMs and LLMs for optimal translation outcomes, specially in narrower domains.",10.48550/arXiv.2409.03454
6ca961736ea1fa1fb92a895ee9890a49473f921b,Optimization Techniques for Sentiment Analysis Based on LLM (GPT-3),2024,"With the rapid development of natural language processing (NLP) technology, large-scale pre-trained language models such as GPT-3 have become a popular research object in NLP field. This paper aims to explore sentiment analysis optimization techniques based on large pre-trained language models such as GPT-3 to improve model performance and effect and further promote the development of natural language processing (NLP). By introducing the importance of sentiment analysis and the limitations of traditional methods, GPT-3 and Fine-tuning techniques are introduced in this paper, and their applications in sentiment analysis are explained in detail. The experimental results show that the Fine-tuning technique can optimize GPT-3 model and obtain good performance in sentiment analysis task. This study provides an important reference for future sentiment analysis using large-scale language models.",10.54254/2755-2721/67/2024ma0060
c05c72e0f180e7c13809138550205578eb084cfb,Randomized Sparse Neural Galerkin Schemes for Solving Evolution Equations with Deep Networks,2023,"Training neural networks sequentially in time to approximate solution fields of time-dependent partial differential equations can be beneficial for preserving causality and other physics properties; however, the sequential-in-time training is numerically challenging because training errors quickly accumulate and amplify over time. This work introduces Neural Galerkin schemes that update randomized sparse subsets of network parameters at each time step. The randomization avoids overfitting locally in time and so helps prevent the error from accumulating quickly over the sequential-in-time training, which is motivated by dropout that addresses a similar issue of overfitting due to neuron co-adaptation. The sparsity of the update reduces the computational costs of training without losing expressiveness because many of the network parameters are redundant locally at each time step. In numerical experiments with a wide range of evolution equations, the proposed scheme with randomized sparse updates is up to two orders of magnitude more accurate at a fixed computational budget and up to two orders of magnitude faster at a fixed accuracy than schemes with dense updates.",10.48550/arXiv.2310.04867
458dd28b79d32a1d7f0287f56f486f4059909329,"Digital Health Data, a Way to Take Under Control the Quality During the Elaboration Processes",2022,"In this work we explore the uncertainty in measured attributes of events, correlating with other events happening in a near time window. The conceptual framework of the method is the event graph concept. Our objective is to filter out the unreliable events and obtain an optimized fraction of data, in order to compute decent statistics, to be able to evaluate the data and build derived measures. Developing our framework, we proceed with some experiments using data from wearable devices, both synthetic data sets and a public available benchmark.",10.1109/SITIS57111.2022.00015
001724979a91f382172c885b17b6ffa91532f089,Implicit softmax transforms for dimensionality reduction,2008,"This paper develops implicit softmax transforms (IST) which are dimensionality reducing transforms that are defined implicitly by minimisation of a weighted sum of Kullback-Leib- ler distances (WKL). The parameters of an IST are trained in combination with the parameters of the polynomial exponents of softmax functions. The resulting gradient of the WKL can be efficiently calculated and the computational effort scales well with the size of the training set. The paper compares IST's to PCA and LDA in classification experiments with two different types of classifiers on three different datasets, two of them from the UCI machine learning repository. It is shown that IST's outperform PCA and LDA in a majority of the cases. In one case reducing the dimension with an IST even gives an improvement over the high dimensional baseline system.",10.1109/ICASSP.2008.4518024
d54ecc745b88f0b53c9556a864197f8435620545,Detecting Zero-Day Attacks Using Contextual Relations,2014,,10.1007/978-3-319-08618-7_36
0017372ff28b2330352af629d2e4901ec4ffafce,On the Co-evolution of ML Pipelines and Source Code - Empirical Study of DVC Projects,2021,"The growing popularity of machine learning (ML) applications has led to the introduction of software engineering tools such as Data Versioning Control (DVC), MLFlow and Pachyderm that enable versioning ML data, models, pipelines and model evaluation metrics. Since these versioned ML artifacts need to be synchronized not only with each other, but also with the source and test code of the software applications into which the models are integrated, prior findings on co-evolution and coupling between software artifacts might need to be revisited. Hence, in order to understand the degree of coupling between ML-related and other software artifacts, as well as the adoption of ML versioning features, this paper empirically studies the usage of DVC in 391 Github projects, 25 of which in detail. Our results show that more than half of the DVC files in a project are changed at least once every one-tenth of the project’s lifetime. Furthermore, we observe a tight coupling between DVC files and other artifacts, with 1/4 pull requests changing source code and 1/2 pull requests changing tests requiring a change to DVC files. As additional evidence of the observed complexity associated with adopting ML-related software engineering tools like DVC, an average of 78% of the studied projects showed a non-constant trend in pipeline complexity.",10.1109/SANER50967.2021.00046
2ae73d1207e01813b143f500bc40309816828076,A Large-Scale Study of ML-Related Python Projects,2024,"The rise of machine learning (ML) for solving current and future problems increased the production of ML-enabled software systems. Unfortunately, standardized tool chains for developing, employing, and maintaining such projects are not yet mature, which can mainly be attributed to a lack of understanding of the properties of ML-enabled software. For instance, it is still unclear how to manage and evolve ML-specific assets together with other software-engineering assets. In particular, ML-specific tools and processes, such as those for managing ML experiments, are often perceived as incompatible with practitioners' software engineering tools and processes. To design new tools for developing ML-enabled software, it is crucial to understand the properties and current problems of developing these projects by eliciting empirical data from real projects, including the evolution of the different assets involved. Moreover, while studies in this direction have recently been conducted, identifying certain types of ML-enabled projects (e.g., experiments, libraries and software systems) remains a challenge for researchers. We present a large-scale study of over 31,066 ML projects found on GitHub, with an emphasis on their development stages and evolution. Our contributions include a dataset, together with empirical data providing an overview of the existing project types and analysis of the projects' properties and characteristics, especially regarding the implementation of different ML development stages and their evolution. We believe that our results support researchers, practitioners, and tool builders conduct follow-up studies and especially build novel tools for managing ML projects, ideally unified with traditional software-engineering tools.",10.1145/3605098.3636056
f8b7cb4c42fe03cb1dc66dbee6e0a36e0d162844,Scalable Evaluation Pipeline of CNN-Based Perception for Robotic Sensor Data Under Different Environment Conditions,2023,"Deep learning impacted a wide variety of perception applications for autonomous mobile robots. In classic computer vision benchmark tests, new algorithms keep appearing that outperform each other. However, these benchmark tests cannot be generalized, so that the specific application must be considered for the selection of sensors and algorithms. Especially in the agricultural domain, environmental conditions like weather and vegetation significantly influence the reliability of sensor systems. Therefore, it is essential to test different sensor modalities and algorithms in the operational design domain. This motivates the need for an evaluation framework which has the flexibility to compare and validate various perception algorithms, sensors suites, and data samples with a focus on different conditions. This paper proposes a pipeline combining a test environment (AI-TEST-FIELD), a semantic environment representation (SEEREP), and an inference server (Triton) for an automatic evaluation of different CNN-based perception algorithms under various environment conditions. Recurring and comparable recordings of raw sensor data with identical scenarios and objects can be performed on the test field, with the only difference being the environmental conditions. The inference results are inferred once and stored alongside the sensor data in SEEREP. Thus, they can be queried efficiently based on the environment conditions to generate (partially overlapping) subsets of the whole dataset. It is demonstrated how this pipeline can be used to apply the CNN-inference just once on the data, and how the queried subsets can subsequently be used to evaluate the performance in different environment conditions.",10.1109/ECMR59166.2023.10256352
e80fdf6055749517397eaf2c07ddc5b3615c9410,MGit: A Model Versioning and Management System,2023,"Models derived from other models are extremely common in machine learning (ML) today. For example, transfer learning is used to create task-specific models from""pre-trained""models through finetuning. This has led to an ecosystem where models are related to each other, sharing structure and often even parameter values. However, it is hard to manage these model derivatives: the storage overhead of storing all derived models quickly becomes onerous, prompting users to get rid of intermediate models that might be useful for further analysis. Additionally, undesired behaviors in models are hard to track down (e.g., is a bug inherited from an upstream model?). In this paper, we propose a model versioning and management system called MGit that makes it easier to store, test, update, and collaborate on model derivatives. MGit introduces a lineage graph that records provenance and versioning information between models, optimizations to efficiently store model parameters, as well as abstractions over this lineage graph that facilitate relevant testing, updating and collaboration functionality. MGit is able to reduce the lineage graph's storage footprint by up to 7x and automatically update downstream models in response to updates to upstream models.",10.48550/arXiv.2307.07507
1e2aa53f7f8f3fe0d8e9ba0e51e5036c0864fbc4,What Constitutes the Deployment and Runtime Configuration System? An Empirical Study on OpenStack Projects,2023,"Modern software systems are designed to be deployed in different configured environments (e.g., permissions, virtual resources, network connections) and adapted at runtime to different situations (e.g., memory limits, enabling/disabling features, database credentials). Such a configuration during the deployment and runtime of a software system is implemented via a set of configuration files, which together constitute what we refer to as a “configuration system.” Recent research efforts investigated the evolution and maintenance of configuration files. However, they merely focused on a limited part of the configuration system (e.g., specific infrastructure configuration files or Dockerfiles), and their results do not generalize to the whole configuration system. To cope with such a limitation, we aim to better capture and understand what files constitute a configuration system. To do so, we leverage an open card sort technique to qualitatively study 1,756 configuration files from OpenStack, a large and widely studied open source software ecosystem. Our investigation reveals the existence of nine types of configuration files, which cover the creation of the infrastructure on top of which OpenStack will be deployed, along with other types of configuration files used to customize OpenStack after its deployment. These configuration files are interconnected while being used at different deployment stages. For instance, we observe specific configuration files used during the deployment stage to create other configuration files that are used in the runtime stage. We also observe that identifying and classifying these types of files is not straightforward, as five out of the nine types can be written in similar programming languages (e.g., Python and Bash) as regular source code files. We also found that the same file extensions (e.g., Yaml) can be used for different configuration types, making it difficult to identify and classify configuration files. Thus, we first leverage a machine learning model to identify configuration from non-configuration files, which achieved a median area under the curve (AUC) of 0.91, a median Brier score of 0.12, a median precision of 0.86, and a median recall of 0.83. Thereafter, we leverage a multi-class classification model to classify configuration files based on the nine configuration types. Our multi-class classification model achieved a median weighted AUC of 0.92, a median Brier score of 0.04, a median weighted precision of 0.84, and a median weighted recall of 0.82. Our analysis also shows that with only 100 labeled configuration and non-configuration files, our model reached a median AUC higher than 0.69. Furthermore, our configuration model requires a minimum of 100 configuration files to reach a median weighted AUC higher than 0.75.",10.1145/3607186
0017b46755fcf120d447c439d9ac5cefbb9ba0de,Building a chain of high-speed VNFs in no time: Invited Paper,2018,"To cope with the growing performance needs of appliances in datacenters or the network edge, current middle-box functionalities such as firewalls, NAT, DPI, content-aware optimizers or load-balancers are often implemented on multiple (perhaps virtual) machines. In this work, we design a system able to run a pipeline of VNFs with a high level of parallelism to handle many flows. We provide the user facilities to define the traffic class of interest for the VNF, a definition of session to group the packets such as the TCP 4-tuples, and the amount of space per sessions. The system will then synthesize the classification and build a unique, efficient flow table. We build an abstract view of flows and use it to implement support for seamless inspection and modification of the content of any flow (such as TCP or HTTP), automatically reflecting a consistent view, across layers, of flows modified on-the-fly. Our prototype gives rise to a user-space software NFV data-plane enabling easy implementation of middlebox functionalities, as well as the deployment of complex scenarios. Our prototype implementation is able to handle our testbed limit of −34 Gbps of HTTP requests (for 8-KB files) through a service chain of multiples stateful VNFs, on a single Xeon core.",10.1109/HPSR.2018.8850742
0cd5892b6b67255a639a14c646accbf60ff33db9,SAFE-ME: Scalable and Flexible Policy Enforcement in Middlebox Networks,2022,"The past decades have seen a proliferation of middlebox deployment in various scenarios, including backbone networks and cloud networks. Since flows have to traverse specific service function chains (SFCs) for security and performance enhancement, it becomes much complex for SFC routing due to routing loops, traffic dynamics and scalability requirement. The existing SFC routing solutions may consume many resources (e.g., TCAM) on the data plane and lead to massive overhead on the control plane, which decrease the scalability of middlebox networks. Due to SFC requirement and potential routing loops, solutions like traditional default paths (e.g., using ECMP) that are widely used in non-middlebox networks will no longer be feasible. In this paper, we present and implement a scalable and flexible middlebox policy enforcement (SAFE-ME) system to minimize the TCAM usage and control overhead. To this end, we design the smart tag operations for construction of default SFC paths with less TCAM rules in the data plane, and present lightweight SFC routing update with less control overhead for dealing with traffic dynamics in the control plane. We implement our solution and evaluate its performance with experiments on both physical platform (Pica8) and Programming Protocol-independent Packet Processors (P4) based data plane, as well as large-scale simulations. Both experimental and simulation results show that SAFE-ME can greatly improve scalability (e.g., TCAM cost, update delay, and control overhead) in middlebox networks, especially for large-scale clouds. For example, our system can reduce the control traffic overhead by about 85% while achieving almost the similar middlebox load, compared with state-of-the-art solutions.",10.1109/TNET.2022.3167169
b50ebcbbec47e6e3c8c98e5990c7dcafc7508409,Metron,2021,"Deployment of 100Gigabit Ethernet (GbE) links challenges the packet processing limits of commodity hardware used for Network Functions Virtualization (NFV). Moreover, realizing chained network functions (i.e., service chains) necessitates the use of multiple CPU cores, or even multiple servers, to process packets from such high speed links. Our system Metron jointly exploits the underlying network and commodity servers’ resources: (i) to offload part of the packet processing logic to the network, (ii) by using smart tagging to setup and exploit the affinity of traffic classes, and (iii) by using tag-based hardware dispatching to carry out the remaining packet processing at the speed of the servers’ cores, with zero inter-core communication. Moreover, Metron transparently integrates, manages, and load balances proprietary “blackboxes” together with Metron service chains. Metron realizes stateful network functions at the speed of 100GbE network cards on a single server, while elastically and rapidly adapting to changing workload volumes. Our experiments demonstrate that Metron service chains can coexist with heterogeneous blackboxes, while still leveraging Metron’s accurate dispatching and load balancing. In summary, Metron has (i) 2.75–8× better efficiency, up to (ii) 4.7× lower latency, and (iii) 7.8× higher throughput than OpenBox, a state-of-the-art NFV system.",10.1145/3465628
85c06da76c7e9fa953df5c7aa9bf2aae3006c102,High-speed Connection Tracking in Modern Servers,2021,"The rise of commodity servers equipped with high-speed network interface cards poses increasing demands on the efficient implementation of connection tracking, i.e., the task of associating the connection identifier of an incoming packet to the state stored for that connection. In this work, we thoroughly investigate and compare the performance obtainable by different implementations of connection tracking using high-speed real traffic traces. Based on a load balancer use case, our results show that connection tracking is an expensive operation, achieving at most 24 Gbps on a single core. Core-sharding and lock-free hash tables emerge as the only suitable multi-thread approaches for enabling 100 Gbps packet processing. In contrast to recent beliefs, we observe that newly proposed techniques to ""lazily"" delete connection states are not more effective than properly tuned traditional deletion techniques based on timer wheels.",10.1109/HPSR52026.2021.9481841
3babfe75c2831361e4df70e3001adfc7d14cb903,Emotion-augmented machine learning: Overview of an emerging domain,2017,"Emotion has been exposed as a crucial component of intelligent behaviour. Considerations in both neuroscience and psychology have identified emotion as playing a central role in various critical cognitive processes, such as attaining salience from the environment in order to support decision making, exploration-exploitation and broader adaptation. This paper provides an overview of some of the corroborative material in these fields, to then consider how emotion has been translated into machine learning. We identify emotion as being a promising endeavour for machine learning and expose Emotion-augmented Machine Learning (EML) as a frontier field in Artificial Intelligence and Affective Computing.",10.1109/ACII.2017.8273617
da981e00c5f4a2aa931d02ae2a3211b0a0ac1b63,Emotion in reinforcement learning agents and robots: a survey,2017,"This article provides the first survey of computational models of emotion in reinforcement learning (RL) agents. The survey focuses on agent/robot emotions, and mostly ignores human user emotions. Emotions are recognized as functional in decision-making by influencing motivation and action selection. Therefore, computational emotion models are usually grounded in the agent’s decision making architecture, of which RL is an important subclass. Studying emotions in RL-based agents is useful for three research fields. For machine learning (ML) researchers, emotion models may improve learning efficiency. For the interactive ML and human–robot interaction community, emotions can communicate state and enhance user investment. Lastly, it allows affective modelling researchers to investigate their emotion theories in a successful AI agent class. This survey provides background on emotion theory and RL. It systematically addresses (1) from what underlying dimensions (e.g. homeostasis, appraisal) emotions can be derived and how these can be modelled in RL-agents, (2) what types of emotions have been derived from these dimensions, and (3) how these emotions may either influence the learning efficiency of the agent or be useful as social signals. We also systematically compare evaluation criteria, and draw connections to important RL sub-domains like (intrinsic) motivation and model-based RL. In short, this survey provides both a practical overview for engineers wanting to implement emotions in their RL agents, and identifies challenges and directions for future emotion-RL research.",10.1007/s10994-017-5666-0
ac6ec6a266df0d8245959319d55caa1c133adf7e,Emotional agents - state of the art and applications,2015,"last decade, intensive research on emotional intelligence has advanced 
 significantly from its theoretical basis, analytical studies and processing 
 technology to exploratory applications in a wide range of real-life domains. 
 This paper brings new insights in the field of emotional, intelligent 
 software agents. The first part is devoted to an overview of the 
 state-of-the-art in emotional intelligence research with emphasis on 
 emotional agents. A wide range of applications in different areas like 
 modeling emotional agents, aspects of learning in emotional environments, 
 interactive emotional systems and so on are presented. After that we suggest 
 a systematic order of research steps with the idea of proposing an adequate 
 framework for several possible real-life applications of emotional agents. We 
 recognize that it is necessary to apply specific methods for dynamic data 
 analysis in order to identify and discover new knowledge from available 
 emotional information and data sets. The last part of the paper discusses 
 research activities for designing an agent-based architecture, in which 
 agents are capable of reasoning about and displaying some kind of emotions 
 based on emotions detected in human speech, as well as online documents. 
 [Projekat Ministarstva nauke Republike Srbije, br. OI174023: Intelligent 
 techniques and their integration into wide-spectrum decision support]",10.2298/CSIS141026047I
0018b02a968c3e279c83be92224b525ba8360a40,Demystifying the Performance of XDP BPF,2019,"High packet rates at $\geq 10\ \mathrm{GBit}/\mathrm{s}$ challenge the packet processing performance of network stacks. A common solution is to offload (parts of) the user-space packet processing to other execution environments, e.g., into the device driver (kernel-space), the NIC or even from virtual machines into the host operating system (OS), or any combination of those. While common wisdom states that offloading optimizes performance, neither benefits nor negative effects are comprehensively studied. In this paper, we aim to shed light on the benefits and shortcomings of eBPF/XDP-based offloading from the user-space to i) the kernel-space or ii) a smart NIC-including VM virtualization. We show that offloading can indeed optimize packet processing, but only if the task is small and optimized for the target environment. Otherwise, offloading can even lead to detrimental performance.",10.1109/NETSOFT.2019.8806651
e81d89a5ebb2e41b37c11cde3f9989c5758b9aa8,Are Kernel Drivers Ready For Accelerated Packet Processing Using AF_XDP?,2023,"AF_XDP is a new technology for accelerated packet processing that was introduced in Linux kernel 4.18. Unlike other kernel bypassing technologies, AF_XDP promises low operational efforts requiring no special execution environment and relying on the vendor kernel driver to achieve good performance. Our paper targets understanding the operational readiness of AF_XDP kernel drivers by comparing their maturity to their DPDK counterparts. We use the throughput and latency of both technologies for multiple frame sizes as metrics to study the mlx5 driver design, an example of enterprise-grade drivers available for AF_XDP and DPDK. Our results show that the AF_XDP mlx5 driver performs better than the traditional sockets API but is less mature in design to handle heavy traffic of all types than its user-space counterpart. We also contribute documentation of all drivers supporting AF_XDP and their limitations to familiarize the developers with the risks of porting an AF_XDP application from one driver to another.",10.1109/NFV-SDN59219.2023.10329590
c2ef1c7a0602da5daacdff813d759f0b0dafed43,A Survey on Mechanisms for Fast Network Packet Processing,2023,"As the Internet of Things becoming widely used, the requirement for high performance network is growing rapidly. Network performance is affected by data transmission based on physical link and packet processing handled by operating system. With the development of network hardware technology and wireless communication technology, network data transmission rate has been further improved, but the packet processing performance based on operating system cannot match it. There are several performance bottlenecks in the traditional packet processing, correspondingly there are various optimization mechanisms to solve these problems and further improve the performance. This paper analyzes some mainstream optimization mechanisms of packet processing based on software. Firstly, we analyze the architecture design of these mechanisms and the workflow when receiving packets. Then we compare the technical methods adopted by each mechanism and problems solved by each mechanism. Finally, we compare the application scenarios and the shortcomings of each mechanism.",10.1145/3603781.3603792
d6a9fea83f4edc32ca3b596d481176d53c40da0d,Securing Kubernetes Pods communicating over Weave Net through eBPF/XDP from DDoS attacks,2023,"Kubernetes, a container orchestration tool, can be vulnerable to many network threats. Distributed Denial-of-Service (DDoS) attack causes Kubernetes nodes and Pods/Containers inaccessible to users. In this work, we highlight that extended Berkeley Packet Filter/eXpress Data Path (eBPF/XDP) can protect Kubernetes Weave Net Pods from DDoS attacks by loading the XDP_DROP/FILTER program over the Weave Net VXLAN interface.",10.1145/3577923.3585049
552271f22a591b29a2cca23f4ba39ea3880c8e79,Fast In-kernel Traffic Sketching in eBPF,2023,"The extended Berkeley Packet Filter (eBPF) is an infrastructure that allows to dynamically load and run micro-programs directly in the Linux kernel without recompiling it. In this work, we study how to develop high-performance network measurements in eBPF. We take sketches as case-study, given their ability to support a wide-range of tasks while providing low-memory footprint and accuracy guarantees. We implemented NitroSketch, the state-of-the-art sketch for user-space networking and show that best practices in user-space networking cannot be directly applied to eBPF, because of its different performance characteristics. By applying our lesson learned we improve its performance by 40% compared to a naive implementation.",10.1145/3594255.3594256
f757944c3256a14a8d1d2f5d6911bd3c8d1e36a2,OXDP: Offloading XDP to SmartNIC for Accelerating Packet Processing,2023,"Traditional kernel network processing suffers from high delay and overhead, which has become the bottleneck of high-speed networks. A natural method to accelerate packet processing is to bypass the kernel network stack and process packets in user space directly, $e.g$., DPDK. However, due to many network functions are implemented in the kernel network stack, bypassing the stack means that we need to redesign the required functions elsewhere, leading to poor compatibility. One promising technology to address this problem is called eXpress Data Path (XDP), which can support high-performance packet processing while preserving the kernel stack. However, existing solutions mainly run XDP in software mode, resulting in relatively poor packet processing performance. Fortunately, with the development of programmable hardware, running XDP in hardware mode is a more promising approach. Thus, in this paper, we design and implement OXDP, the first-of-its-kind work on accelerating packet processing by offloading XDP to SmartNICs. Since today’s SmartNICs are still subject to some limitations regarding the rigid runtime environment, it is nontrivial to offload XDP to SmartNICs. To address this issue, OXDP performs best-effort offloading based on the primitive packet operations, thus maximizing the use of SmartNIC’s resources. Specifically, OXDP splits the forwarding function into two parts, one part offloading on SmartNIC with hardware XDP and the other part deploying on host. We evaluate the efficiency of OXDP with comprehensive experiments. Evaluation results show that the forwarding rate of OXDP can reach 18.7 Mpps, which improves $30 \times$ compared with the single-core performance of software XDP.",10.1109/ICPADS56603.2022.00103
593f679bb6541bffe28401d47c1a7c6ee411d9f1,Comparing User Space and In-Kernel Packet Processing for Edge Data Centers,2023,"Telecommunication operators are massively moving their network functions in small data centers at the edge of the network, which are becoming increasingly common. However, the high performance provided by commonly used technologies for data plane processing such as DPDK, based on kernel-bypass primitives, comes at the cost of rigid resource partitioning. This is unsuitable for edge data centers, in which efficiency demands both general-purpose applications and data-plane telco workloads to be executed on the same (shared) physical machines. In this respect, eBPF/XDP looks a more appealing solution, thanks to its capability to process packets in the kernel, achieving a higher level of integration with non-data plane applications albeit with lower performance than DPDK. In this paper we leverage the recent introduction of AF_XDP, an XDP-based technology that allows to efficiently steer packets in user space, to provide a thorough comparison of user space vs in-kernel packet processing in typical scenarios of a data center at the edge of the network. Our results provide useful insights on how to select and combine these technologies in order to improve overall throughput and optimize resource usage.",10.1145/3594255.3594257
d078dd095a36e2e46185814a8ef90285ebce6470,Address Generation Optimization for Embedded High-Performance Processors: A Survey,2008,,10.1007/s11265-008-0165-y
641170ec17d38d29775568e58305bd70d6fb1f05,Design of a Media Processor based on DSP Processor,2006,"A novel media processor aimed at processing picture, audio and video data has been proposed in the paper. The media processor is formed by a digital signal processing (DSP) processor in combination with a multimedia coprocessor. The instruction set of media processor consists of common DSP instructions and extended multimedia processing instructions. The multimedia co-processor is made up by several hardware modules which accelerate the processing of multimedia data. The media processor has strong multimedia processing capability and it can realize the real-time decoding of compressed JPEG picture, MP3 audio, MPEG2 MP@ML video data.",10.1109/ICCT.2006.341942
d33b2c172f2612c29e749275b624f656afa8fb8d,Evaluation of Bus Based Interconnect Mechanisms in Clustered VLIW Architectures,2005,,10.1007/s10766-007-0045-2
350e394d422ac4cd5fbe5d03bf25d44e41c47782,Developing Complex Systems Using Evolved Pattern Generators,2007,"Self-organization of connection patterns within brain areas of animals begins prenatally, and has been shown to depend on internally generated patterns of neural activity. The neural structures continue to develop postnatally through externally driven patterns, when the sensory systems are exposed to stimuli from the environment. The internally generated patterns have been proposed to give the neural system an appropriate bias so that it can learn reliably from complex environmental stimuli. This paper evaluates the hypothesis that complex artificial learning systems can benefit from a similar approach, consisting of initial training with patterns from an evolved pattern generator, followed by training with the actual training set. To test this hypothesis, competitive learning networks were trained for recognizing handwritten digits. The results demonstrate how the approach can improve learning performance by discovering the appropriate initial weight biases, thereby compensating for weaknesses of the learning algorithm. Due to the smaller evolutionary search space, this approach was also found to require much fewer generations than direct evolution of network weights. Since discovering the right biases efficiently is critical for solving large-scale problems with learning, these results suggest that internal training pattern generation is an effective method for constructing complex systems",10.1109/TEVC.2006.890272
5fffbd9271f4234652f375df111a3e97303fc52c,Evolutionary constructive induction,2005,"Feature construction in classification is a preprocessing step in which one or more new attributes are constructed from the original attribute set, the object being to construct features that are more predictive than the original feature set. Genetic programming allows the construction of nonlinear combinations of the original features. We present a comprehensive analysis of genetic programming (GP) used for feature construction, in which four different fitness functions are used by the GP and four different classification techniques are subsequently used to build the classifier. Comparisons are made of the error rates and the size and complexity of the resulting trees. We also compare the overall performance of GP in feature construction with that of GP used directly to evolve a decision tree classifier, with the former proving to be a more effective use of the evolutionary paradigm.",10.1109/TKDE.2005.182
14b3d2123e4cf5748c5dbafc1b04c160ef0fec43,Constructing good learners using evolved pattern generators,2005,"Self-organization of brain areas in animals begins prenatally, evidently driven by spontaneously generated internal patterns. The neural structures continue to develop postnatally when the sensory systems are exposed to stimuli from the environment. In this process, prenatal training may give the neural system the appropriate bias so that it can learn reliably under changing environmental stimuli. This paper evaluates the hypothesis that an artificial learning system can benefit from a similar approach, consisting of initial training with patterns from an evolved generator followed by training with the actual training set. Competitive learning networks were trained in recognizing handwritten digits in three ways: through environmental learning only, through evolution only, and through prenatal training with evolved pattern generators followed by environmental learning. The results demonstrate that the evolved pattern generator approach leads to better learning performance, suggesting that complex systems can be constructed effectively in this way.",10.1145/1068009.1068012
d77b929e29876c369effceb4ae6ec105a83e7a40,Machine Learning from Examples: Inductive and Lazy Methods,1998,,10.1016/S0169-023X(97)00053-0
33f85e5fc64f60be07a457271d869d30783dedd4,How to Shift Bias: Lessons from the Baldwin Effect,1996,"An inductive learning algorithm takes a set of data as input and generates a hypothesis as output. A set of data is typically consistent with an infinite number of hypotheses; therefore, there must be factors other than the data that determine the output of the learning algorithm. In machine learning, these other factors are called the bias of the learner. Classical learning algorithms have a fixed bias, implicit in their design. Recently developed learning algorithms dynamically adjust their bias as they search for a hypothesis. Algorithms that shift bias in this manner are not as well understood as classical algorithms. In this paper, we show that the Baldwin effect has implications for the design and analysis of bias shifting algorithms. The Baldwin effect was proposed in 1896 to explain how phenomena that might appear to require Lamarckian evolution (inheritance of acquired characteristics) can arise from purely Darwinian evolution. Hinton and Nowlan presented a computational model of the Baldwin effect in 1987. We explore a variation on their model, which we constructed explicitly to illustrate the lessons that the Baldwin effect has for research in bias shifting algorithms. The main lesson is that it appears that a good strategy for shift of bias in a learning algorithm is to begin with a weak bias and gradually shift to a strong bias.",10.1162/evco.1996.4.3.271
030e5d2d54cc97d20ba4e8c70d7772db42b8776c,Integrating Models of Knowledge and Machine Learning,1993,,10.1007/3-540-56602-3_157
001a72782dfef61589a8f897a4ae5dc4b49b5a5e,End-To-End Supervised Product Quantization for Image Search and Retrieval,2017,"Product Quantization, a dictionary based hashing method, is one of the leading unsupervised hashing techniques. While it ignores the labels, it harnesses the features to construct look up tables that can approximate the feature space. In recent years, several works have achieved state of the art results on hashing benchmarks by learning binary representations in a supervised manner. This work presents Deep Product Quantization (DPQ), a technique that leads to more accurate retrieval and classification than the latest state of the art methods, while having similar computational complexity and memory footprint as the Product Quantization method. To our knowledge, this is the first work to introduce a dictionary-based representation that is inspired by Product Quantization and which is learned end-to-end, and thus benefits from the supervised signal. DPQ explicitly learns soft and hard representations to enable an efficient and accurate asymmetric search, by using a straight-through estimator. Our method obtains state of the art results on an extensive array of retrieval and classification experiments.",10.1109/CVPR.2019.00518
09c3926ad0d6d4b4362ddcc7d759986631a2c9e4,Entropy-Optimized Deep Weighted Product Quantization for Image Retrieval,2024,"Hashing and quantization have greatly succeeded by benefiting from deep learning for large-scale image retrieval. Recently, deep product quantization methods have attracted wide attention. However, representation capability of codewords needs to be further improved. Moreover, since the number of codewords in the codebook depends on experience, representation capability of codewords is usually imbalanced, which leads to redundancy or insufficiency of codewords and reduces retrieval performance. Therefore, in this paper, we propose a novel deep product quantization method, named Entropy Optimized deep Weighted Product Quantization (EOWPQ), which not only encodes samples into the weighted codewords in a new flexible manner but also balances the codeword assignment, improving while balancing representation capability of codewords. Specifically, we encode samples using the linear weighted sum of codewords instead of a single codeword as traditionally. Meanwhile, we establish the linear relationship between the weighted codewords and semantic labels, which effectively maintains semantic information of codewords. Moreover, in order to balance the codeword assignment, that is, avoiding some codewords representing most samples or some codewords representing very few samples, we maximize the entropy of the coding probability distribution and obtain the optimal coding probability distribution of samples by utilizing optimal transport theory, which achieves the optimal assignment of codewords and balances representation capability of codewords. The experimental results on three benchmark datasets show that EOWPQ can achieve better retrieval performance and also show the improvement of representation capability of codewords and the balance of codeword assignment.",10.1109/TIP.2024.3359066
0c08725a79cb5dbb1354793a287810646dbefe52,Residual Quantization with Implicit Neural Codebooks,2024,"Vector quantization is a fundamental operation for data compression and vector search. To obtain high accuracy, multi-codebook methods represent each vector using codewords across several codebooks. Residual quantization (RQ) is one such method, which iteratively quantizes the error of the previous step. While the error distribution is dependent on previously-selected codewords, this dependency is not accounted for in conventional RQ as it uses a fixed codebook per quantization step. In this paper, we propose QINCo, a neural RQ variant that constructs specialized codebooks per step that depend on the approximation of the vector from previous steps. Experiments show that QINCo outperforms state-of-the-art methods by a large margin on several datasets and code sizes. For example, QINCo achieves better nearest-neighbor search accuracy using 12-byte codes than the state-of-the-art UNQ using 16 bytes on the BigANN1M and Deep1M datasets.",10.48550/arXiv.2401.14732
2d6182ec7a94bae41e22eab12febfd1a2e3d6367,HiHPQ: Hierarchical Hyperbolic Product Quantization for Unsupervised Image Retrieval,2024,"Existing unsupervised deep product quantization methods primarily aim for the increased similarity between different views of the identical image, whereas the delicate multi-level semantic similarities preserved between images are overlooked. Moreover, these methods predominantly focus on the Euclidean space for computational convenience, compromising their ability to map the multi-level semantic relationships between images effectively. To mitigate these shortcomings, we propose a novel unsupervised product quantization method dubbed Hierarchical Hyperbolic Product Quantization (HiHPQ), which learns quantized representations by incorporating hierarchical semantic similarity within hyperbolic geometry. Specifically, we propose a hyperbolic product quantizer, where the hyperbolic codebook attention mechanism and the quantized contrastive learning on the hyperbolic product manifold are introduced to expedite quantization. Furthermore, we propose a hierarchical semantics learning module, designed to enhance the distinction between similar and non-matching images for a query by utilizing the extracted hierarchical semantics as an additional training supervision. Experiments on benchmark image datasets show that our proposed method outperforms state-of-the-art baselines.",10.48550/arXiv.2401.07212
467227272889a8307c83a1b2ffd1daad6d8d8bfa,Expand-and-Quantize: Unsupervised Semantic Segmentation Using High-Dimensional Space and Product Quantization,2023,"Unsupervised semantic segmentation (USS) aims to discover and recognize meaningful categories without any labels. 
For a successful USS, two key abilities are required: 1) information compression and 2) clustering capability.
Previous methods have relied on feature dimension reduction for information compression, however, this approach may hinder the process of clustering.
In this paper, we propose a novel USS framework called Expand-and-Quantize Unsupervised Semantic Segmentation (EQUSS), which combines the benefits of high-dimensional spaces for better clustering and product quantization for effective information compression.
Our extensive experiments demonstrate that EQUSS achieves state-of-the-art results on three standard benchmarks.
In addition, we analyze the entropy of USS features, which is the first step towards understanding USS from the perspective of information theory.",10.48550/arXiv.2312.07342
a631f10fa7e100492e6f358d940dff2bebc423a4,JUNO: Optimizing High-Dimensional Approximate Nearest Neighbour Search with Sparsity-Aware Algorithm and Ray-Tracing Core Mapping,2023,"Approximate nearest neighbor (ANN) search is a widely applied technique in modern intelligent applications, such as recommendation systems and vector databases. Therefore, efficient and high-throughput execution of ANN search has become increasingly important. In this paper, we first characterize the state-of-the-art product quantization-based method of ANN search and identify a significant source of inefficiency in the form of unnecessary pairwise distance calculations and accumulations. To improve efficiency, we propose Juno, an end-to-end ANN search system that adopts a carefully designed sparsity- and locality-aware search algorithm. We also present an efficient hardware mapping that utilizes ray tracing cores in modern GPUs with pipelined execution on tensor cores to execute our sparsity-aware ANN search algorithm. Our evaluations on four datasets from 1 to 100 million search points demonstrate 2.2×-8.5× improvements in search throughput. Moreover, our algorithmic enhancements alone achieve a maximal 2.6× improvement on the hardware without the acceleration of the RT core.",10.1145/3620665.3640360
97a26b15405351b4bb70e28537ea59bef67d4d1d,Spherical Centralized Quantization for Fast Image Retrieval,2023,"Existing supervised quantization methods usually learn the quantizers from pair-wise, triplet, or anchor-based losses, which only capture their relationship locally without aligning them globally. This may cause an inadequate use of the entire space and a severe intersection among different semantics, leading to inferior retrieval performance. Furthermore, to enable quantizers to learn in an end-to-end way, current practices usually relax the non-differentiable quantization operation by substituting it with softmax, which unfortunately is biased, leading to an unsatisfying suboptimal solution. To address the above issues, we present Spherical Centralized Quantization (SCQ), which contains a Priori Knowledge based Feature (PKFA) module for the global alignment of feature vectors, and an Annealing Regulation Semantic Quantization (ARSQ) module for low-biased optimization. Specifically, the PKFA module first applies Semantic Center Allocation (SCA) to obtain semantic centers based on prior knowledge, and then adopts Centralized Feature Alignment (CFA) to gather feature vectors based on corresponding semantic centers. The SCA and CFA globally optimize the inter-class separability and intra-class compactness, respectively. After that, the ARSQ module performs a partial-soft relaxation to tackle biases, and an Annealing Regulation Quantization loss for further addressing the local optimal solution. Experimental results show that our SCQ outperforms state-of-the-art algorithms by a large margin (2.1%, 3.6%, 5.5% mAP respectively) on CIFAR-10, NUS-WIDE, and ImageNet with a code length of 8 bits. Codes are publicly available:https://github.com/zzb111/Spherical-Centralized-Quantization.",10.1109/TIP.2023.3265262
4bb89960ec808e5c2888c4a734bb7aa866b0ccca,Towards Codebook-Free Deep Probabilistic Quantization for Image Retrieval,2023,"As a classical feature compression technique, quantization is usually coupled with inverted indices for scalable image retrieval. Most quantization methods explicitly divide feature space into Voronoi cells, and quantize feature vectors in each cell into the centroids learned from data distribution. However, Voronoi decomposition is difficult to achieve discriminative space partition for semantic image retrieval. In this paper, we explore semantic-aware feature space partition by deep neural network instead of Voronoi cells. To this end, we propose a new deep probabilistic quantization method, abbreviated as DeepIndex, which constructs inverted indices without explicit centroid learning. In our method, the deep neural network takes an image as input and outputs its probability of being put into each inverted index list. During training, we progressively quantize each image into the inverted lists with the top-<inline-formula><tex-math notation=""LaTeX"">$T$</tex-math><alternatives><mml:math><mml:mi>T</mml:mi></mml:math><inline-graphic xlink:href=""li-ieq1-3324021.gif""/></alternatives></inline-formula> maximal probabilities, and calculate the reward of each trial based on retrieval accuracy. We optimize the deep neural network to maximize the probability of the inverted list with maximal reward. In this way, the retrieval performance is directly optimized, leading to a more semantically discriminative space partition than other quantization methods. The experiments on public image datasets demonstrate the effectiveness of our DeepIndex method on semantic image retrieval.",10.1109/TPAMI.2023.3324021
001a7e23bc64efa8adac1ca34066ac0935c64fc3,Resistance Distance and Control Performance for bittide Synchronization,2021,"We discuss control of bittide distributed systems, which are designed to provide logical synchronization between networked machines by observing data flow rates between adjacent systems at the physical network layer and controlling local reference clock frequencies. We analyze the performance of approximate proportional-integral control of the synchronization mechanism and develop a simple continuous-time model to show the resulting dynamics are stable for any positive choice of gains. We then construct explicit formulae to show that closed-loop performance measured using the L2 norm is a product of two terms, one depending only on resistance distances in the graph, and the other depending only on controller gains.",10.23919/ECC55457.2022.9838091
c5b4919856afef2fa6b252a04b1368f5789d68f5,Logical Synchrony Networks: A Formal Model for Deterministic Distribution,2024,"In the modelling of distributed systems, most Model of Computations (MoCs) rely on blocking communication to preserve determinism. A prominent example is Kahn Process Networks (KPNs), which supports non-blocking writes and blocking reads, and its implementable variant Finite FIFO Platforms (FFPs) which enforces boundedness using blocking writes. An issue with these models is that they mix process synchronisation with process execution, necessitating frequent blocking during synchronisation. This paper explores a recent alternative called bittide, which decouples the execution of a process from the synchronisation behaviour. Determinism and boundedness is preserved while enabling pipelined execution for better throughput. To understand the behaviour of these systems we define a formal model – a deterministic MoC called Logical Synchrony Networks (LSNs). LSNs describes a network of processes modelled as a graph, with edges representing invariant logical delays between a producer process and the corresponding consumer process. We show that this abstraction is satisfied by the KPN model, and subsequently by both the concrete FFPs and bittide architectures. Thus, we show that FFPs and bittide offer two ways of implementing deterministic distributed systems, with the latter being more performant.",10.1109/ACCESS.2024.3411017
e1d0153a3f705c8b25be6382a546654a00e08bdb,Logical Synchrony and the Bittide Mechanism,2023,"We introduce logical synchrony, a framework that allows distributed computing to be coordinated as tightly as in synchronous systems without the distribution of a global clock or any reference to universal time. We develop a model of events called a logical synchrony network, in which nodes correspond to processors and every node has an associated local clock which generates the events. We construct a measure of logical latency and develop its properties. A further model, called a multiclock network, is then analyzed and shown to be a refinement of the logical synchrony network. We present the bittide mechanism as an instantiation of multiclock networks, and discuss the clock control mechanism that ensures that buffers do not overflow or underflow. Finally we give conditions under which a logical synchrony network has an equivalent synchronous realization.",10.1109/TPDS.2024.3444739
95489215ea230f2d3229bde1c373e6d3889e0019,On Buffer Centering for Bittide Synchronization,2023,"We discuss distributed reframing control of bittide systems. In a bittide system, multiple processors synchronize by monitoring communication over the network. Processors remain in logical synchrony by controlling the timing of frame transmissions. The protocol for doing this relies upon an underlying dynamic control system where each node makes only local observations and performs no direct coordination with other nodes. In this paper we develop a control algorithm based on the idea of buffer centering, which allows all nodes to maintain small buffer offsets while also requiring very little state information. We demonstrate that with buffer centering we can achieve separate control of frequency and phase, allowing frequencies to be syntonized and also buffers to maintain desired offsets rather than combining their control via a proportional-integral controller. The minimalism of this approach offers the potential to simplify both boot processes and failure handling.",10.1109/CoDIT58514.2023.10284255
5eec352ccf6db9483e0d4dbc6068cac69fb7191b,Modeling and Control of bittide Synchronization,2021,"Distributed system applications rely on a fine-grain common sense of time. Existing systems maintain this by keeping each independent machine as close as possible to wall-clock time through a combination of software protocols and precision hardware references. This approach is expensive, requiring protocols to deal with asynchrony and its performance consequences. Moreover, at datacenter scale it is impractical to distribute a physical clock as is done on a chip or printed circuit board. In this paper we introduce a distributed system design that removes the need for physical clock distribution or mechanisms for maintaining close alignment to wall-clock time, and instead provides applications with a perfectly synchronized logical clock. We discuss the abstract frame model (AFM), a mathematical model that underpins the system synchronization. The model is based on the rate of communication between nodes in a topology without requiring a global clock. We show that there are families of controllers that satisfy the properties required for existence and uniqueness of solutions to the AFM, and give examples.",10.23919/ACC53348.2022.9867698
001a98b5acd9797c3394a48fdc7dac96b9159cac,Adaptation of bimanual assembly tasks using iterative learning framework,2015,"The paper deals with the adaptation of bimanual assembly tasks. First, the desired policy is shown by human demonstration using kinesthetic guidance, where both trajectories and interaction forces are captured. Captured entities are portioned to absolute and relative coordinates. During the execution, small discrepancies in object geometry as well as the influence of an imperfect control can result in large contact forces. Force control can diminish the above mentioned problems only to some extent. Therefore, we propose a framework that iteratively modifies the original demonstrated trajectory in order to increase the performance of the typical assembly tasks. The approach is validated on bimanual peg in a hole task using two KUKA LWR robots.",10.1109/HUMANOIDS.2015.7363457
4db088827b010f08419aa94ddf3dfb2520c4cd5d,Integrated Bi-Manual Motion Generation and Control shaped for Probabilistic Movement Primitives,2022,"This work introduces a novel cooperative control framework that allows for real-time reactiveness and adaptation whilst satisfying implicit constraints stemming from proba-bilistic/stochastic trajectories. Stemming from task-oriented sampling and/or task-oriented demonstrations, e.g., learning based on motion primitives, such trajectories carry additional information often neglected during real-time control deployment. In particular, methods such as probabilistic movement primitives offer the advantage to capture the inherent stochasticity in human demonstrations - which in turn reflects human's understanding about task-variability and adaption possibilities. This information, however, is often poorly exploited and, mostly, used during offline trajectory planning stage. Our work instead introduces a novel real-time motion-generation strategy that explicitly exploits such information to improve trajectories according to changes in the environmental condition and robot task-space topology. The proposed solution is particularly well-suited for bi-manual and coordinated systems where the increased kinematic complexity, tightly-coupled constraints and reduced workspace have detrimental effects on the manipula-bility, joint-limits, and are even capable of causing unstable behavior and task-failure. Our methodology addresses these challenges, and improves performance and task-execution by taking the confidence range region explicitly into account whilst maneuvering towards better configurations. Furthermore, it can directly cope with different closed-chain kinematics and task-space topologies, resulting for instance from different grasps. Experimental evaluations on a bi-manual Franka panda robot show that the method can run in the inner control loop of the robot and enables successful execution of highly constrained tasks.",10.1109/Humanoids53995.2022.10000149
dccd0b8798304773062352fbc051c4f072da3cd7,Interactive Imitation Learning of Bimanual Movement Primitives,2022,"Performing bimanual tasks with dual robotic setups can drastically increase the impact on industrial and daily life applications. However, performing a bimanual task brings many challenges, such as synchronization and coordination of the single-arm policies. This article proposes the safe, interactive movement primitives learning (SIMPLe) algorithm, to teach and correct single or dual arm impedance policies directly from human kinesthetic demonstrations. Moreover, it proposes a novel graph encoding of the policy based on Gaussian process regression where the single-arm motion is guaranteed to converge close to the trajectory and then toward the demonstrated goal. Regulation of the robot stiffness according to the epistemic uncertainty of the policy allows for easily reshaping the motion with human feedback and/or adapting to external perturbations. We tested the SIMPLe algorithm on a real dual-arm setup where the teacher gave separate single-arm demonstrations and then successfully synchronized them only using kinesthetic feedback or where the original bimanual demonstration was locally reshaped to pick a box at a different height.",10.1109/TMECH.2023.3295249
c975498646cf833982ac3aa71457c5246f747d67,Kinesthetic teaching of bi-manual tasks with known relative constraints,2022,"Kinesthetic teaching allows the direct skill transfer from the human to the robot and has been widely used to teach single arm tasks intuitively. In the bi-manual case, simultaneously moving both end-effectors is challenging due to the high physical and cognitive load imposed to the user. Thus, previous works on bi-manual task teaching resort to less intuitive methods by teaching each arm separately. This in turn requires motion synthesis and synchronization before execution. In this work, we leverage knowledge from the relative task space to facilitate a kinesthetic demonstration by guiding both end-effectors which is more human-like and intuitive way for performing bi-manual tasks. Our method utilizes the notion of virtual fixtures and inertia minimization in the null space of the task. The controller is experimentally validated in a bi-manual task which involves the drawing of a preset line on a workpiece utilizing two KUKA IIWA7 R800 robots. Results from ten participants were compared with a gravity compensation scheme demonstrating improved performance.",10.1109/IROS47612.2022.9981196
dde42bddcc0c7fe402a1a537a97ff70b2a363df7,Semi-supervised recognition for artificial intelligence assisted pathology image diagnosis,2024,"The analysis and interpretation of cytopathological images are crucial in modern medical diagnostics. However, manually locating and identifying relevant cells from the vast amount of image data can be a daunting task. This challenge is particularly pronounced in developing countries where there may be a shortage of medical expertise to handle such tasks. The challenge of acquiring large amounts of high-quality labelled data remains, many researchers have begun to use semi-supervised learning methods to learn from unlabeled data. Although current semi-supervised learning models partially solve the issue of limited labelled data, they are inefficient in exploiting unlabeled samples. To address this, we introduce a new AI-assisted semi-supervised scheme, the Reliable-Unlabeled Semi-Supervised Segmentation (RU3S) model. This model integrates the ResUNet-SE-ASPP-Attention (RSAA) model, which includes the Squeeze-and-Excitation (SE) network, Atrous Spatial Pyramid Pooling (ASPP) structure, Attention module, and ResUNet architecture. Our model leverages unlabeled data effectively, improving accuracy significantly. A novel confidence filtering strategy is introduced to make better use of unlabeled samples, addressing the scarcity of labelled data. Experimental results show a 2.0% improvement in mIoU accuracy over the current state-of-the-art semi-supervised segmentation model ST, demonstrating our approach’s effectiveness in solving this medical problem.",10.1038/s41598-024-70750-7
51e09da838e9a524076118948d99180f1600ceba,"Improving the accuracy of text classification using stemming method, a case of non-formal Indonesian conversation",2020,"Stemming has long been used in data pre-processing to retrieve information by tracking affixed words back into their root. In an Indonesian setting, existing stemming methods have been observed, and the existing stemming methods are proven to result in high accuracy level. However, there are not many stemming methods for non-formal Indonesian text processing. This study introduces a new stemming method to solve problems in the non-formal Indonesian text data pre-processing. Furthermore, this study aims to improve the accuracy of text classifier models by strengthening stemming method. Using the Support Vector Machine algorithm, a text classifier model is developed, and its accuracy is checked. The experimental evaluation was done by testing 550 datasets in Indonesian using two different stemming methods. The results show that using the proposed stemming method, the text classifier model has higher accuracy than the existing methods with a score of 0.85 and 0.73, respectively. These results indicate that the proposed stemming methods produces a classifier model with a small error rate, so it will be more accurate to predict a class of objects. The existing Indonesian stemming methods are still oriented towards Indonesian formal sentences, therefore the method has limitations to be used in Indonesian non-formal sentences. This phenomenon underlies the suggestion of developing a corpus by normalizing Indonesian non-formal into formal to be used as a better stemming method. The impact of using the corpus as a stemming method is that it can improve the accuracy of the classifier model. In the future, the proposed corpus and stemming methods can be used for various purposes including text clustering, summarizing, detecting hate speech, and other text processing applications in Indonesian.",10.1186/s40537-021-00413-1
001cad7ee5ffe902e0695484abbf59bd892e448e,Induction and permanent-magnet synchronous machines for high-speed applications,2005,"Solid-rotor induction motors are mechanically very robust and therefore potential candidates for highspeed electric drives. Permanent magnet machines offer better electromagnetic characteristics. These two types of machine are compared for high-speed applications. Electromagnetic, thermal and mechanical aspects are discussed. The permanent magnet machines are considered to be the best solution for applications in which the surface speed of the rotor remains below 250 m/s. For very large speeds and powers, a solid-rotor induction motor is preferred",10.1109/ICEMS.2005.202668
bf20c8c74dc8fcdd0177d8168890f0a514093142,Microstructure and Mechanical Properties of Steel and Ni-Based Superalloy Joints for Rotors of High-Speed Electric Motors,2022,"High-speed electric motors, e.g., axially laminated anisotropic synchronous reluctance motors (ALA-SynRM), use a solid rotor manufactured by joining alternating layers of magnetic and non-magnetic metallic sheets. The strength of the dissimilar metallic joints is critical for the rotor’s ability to withstand the operating conditions of the high-speed electrical machine. In this work, various dissimilar metallic joint configurations that can be used in high-speed ALA-SynRM rotors are studied by analyzing the shear strength, microstructure, hardness, and composition of the joints. Metallic joints of structural steels and Inconel® alloys fabricated by vacuum brazing and hot isostatic pressing (HIP) are studied. Finite element analysis (FEA) was performed to calculate the maximum shear stress of the joints that were subjected to various high speed operating conditions. The shear strength of the test specimens was measured and compared with FEA results. The microstructure and chemical composition of the joints were studied by using optical microscopy, scanning electron microscopy (SEM) and energy dispersive spectroscopy (EDS) on SEM. The results show that the hot isostatic pressed S1100MC-IN718 joint achieved the highest ultimate shear strength (233.3 MPa) followed by vacuum brazed S355MC-IN600 joint (230.1 MPa) and HIP S355-IN718 (203.5 MPa), thereby showing that vacuum brazing and HIP can be viable manufacturing methods to fabricate a high-speed ALA-SynRM rotor.",10.3390/ma15196906
001d9273ad91fbe71c2c32fd926dc213e10d98c6,Energy Minimization for Federated Learning with IRS-Assisted Over-the-Air Computation,2021,"This paper investigates the deployment of federated learning (FL) over an over-the-air computation (AirComp) and intelligent reflecting surface (IRS) based wireless network. In the considered system, devices transmit locally trained machine learning (ML) models to the base station (BS) which aggregates the received ML models and generates a shared global ML model. The devices can directly transmit ML models to the BS or using IRS. Meanwhile, AirComp is used to aggregate ML models that are transmitted from the devices to the BS. To minimize the energy consumption of devices, an energy minimization problem is formulated, which jointly optimizes the device selection, phase shift matrix, decoding vector, and power control. To seek the solution, the original optimization problem is divided into four sub-problems. Then the fractional program, greedy algorithm, matrix derivation, and weighted minimum mean square error methods are used to compute the phase shift matrix, device selection vector, decoding vector, and transmit power, respectively. Simulation results show that the proposed algorithm can reduce 11.2% energy consumption of devices compared to an FL algorithm that is implemented at a network without any IRSs.",10.1109/ICASSP39728.2021.9414785
e2324ea23edf95daadc47bcb925c7ac774362445,The Analysis and Optimization of Volatile Clients in Over-the-Air Federated Learning,2024,"This paper investigates the implementation of Federated Learning (FL) in an over-the-air computation system with volatile clients, where each client operates under a limited energy budget and may unexpectedly drop out during local training sessions. The dropout of clients not only wastes energy but also diminishes their participation frequency, necessitating careful client selection by the server in each communication round. However, the diversity of training tasks and the random nature of client dropout present challenges such as the absence of an explicit objective function and the unavailability of client performance metrics. To address these challenges, we first analyze the convergence of the over-the-air federated learning system with volatile clients to identify the key factor influencing the model's convergence speed. Building upon this analysis, we propose an approximation of the objective function as the optimization goal for client selection. To mitigate energy waste, we introduce a dynamic client selection strategy termed DCSE, based on Exp3 with multiple plays and energy constraints, aiming to reconcile the dilemma of unknown local training states and limited resource constraints. Theoretical analysis demonstrates that our proposed solution maintains a constant bound on the difference from the optimal solution, affirming its theoretical feasibility. Furthermore, experimental results validate the effectiveness of the proposed strategy in enhancing FL by accelerating convergence speed, improving test accuracy, and reducing wasted energy.",10.1109/TMC.2024.3427709
fb3836690a361e39d476ed6db9ac671c1da1f410,Semi-Federated Learning for Connected Intelligence With Computing-Heterogeneous Devices,2024,"Federated learning (FL) is a promising distributed learning approach which enables multiple devices to collaboratively train deep neural networks in a privacy-preserving fashion. However, computing-limited devices in Internet of Things (IoT) networks are difficult to perform local model training, which is an obstacle to implementing connected intelligence at the network edge. In this article, we propose a semi-federated learning (SemiFL) framework to support computing-heterogeneous devices for collaborative model training, even if some local devices do not have computing capabilities. Specifically, in SemiFL, computing-limited devices upload raw data to the base station (BS), while computing-powerful devices upload model parameters to the BS. To characterize the achievable learning performance of SemiFL in practical IoT networks, we derive the convergence upper bound with nonindependent and identically distributed (non-IID) data. Furthermore, we formulate an optimization problem to minimize the computation and communication latency by jointly design the transmit power of IoT devices and the receive factor at the BS. Accordingly, we propose an two-stage optimization algorithm to solve the formulated nonconvex problem efficiently. Extensive simulation results show that our SemiFL framework outperforms traditional FL in both IID and non-IID settings. In addition, it is shown that the proposed algorithm achieves lower latency than benchmarks under different settings.",10.1109/JIOT.2024.3355160
2185f2423d8779aac1a28940f51a0c7dce840e04,Digital AirComp-Assisted Federated Edge Learning with Adaptive Quantization,2024,"Federated edge learning (FEEL) has been introduced for training machine learning models on distributed datasets for applications such as human monitoring. However, some challenges exist concerning the number of communication rounds and communication energy consumption involved in transmiting gradients during the training process. Moreover, over-the-air computation (AirComp) technology has gained attention recently, benefiting from superposition characteristic of wireless channels to compute functions over the air. While primarily designed for analog systems, there is potential for applying this technology in digital systems with embedded modulation schemes. This paper addresses these challenges by proposing a digital AirComp system for federated learning aggregation. The system employs a multi-bit quantization scheme to modulate gradients, adhering to a maximum transmission power constraint. An adaptive quantization scheme is also introduced, which considers the impact of quantization error and the error induced by additive white Gaussian noise. We derive closed-form expressions for pre-processing coefficients at devices and post-processing scaler at the access point (AP) to minimize the mean-squared error between high-precision and quantized qradients under the maximum transmission power constraint. Finally, the performance of the proposed scheme is evaluated in terms of achieved test accuracy, mean-squared error (MSE), and energy consumption, demonstrating its potential effectiveness compared to the benchmark schemes.",10.1109/ICHMS59971.2024.10555608
569e323c84bcc01fc6562ea2c7084c7a2a00e703,Multi-modal Machine Learning in Engineering Design: A Review and Future Directions,2023,"
 In the rapidly advancing field of multi-modal machine learning (MMML), the convergence of multiple data modalities has the potential to reshape various applications. This paper presents a comprehensive overview of the current state, advancements, and challenges of MMML within the sphere of engineering design. The review begins with a deep dive into five fundamental concepts of MMML:multi-modal information representation, fusion, alignment, translation, and co-learning. Following this, we explore the cutting-edge applications of MMML, placing a particular emphasis on tasks pertinent to engineering design, such as cross-modal synthesis, multi-modal prediction, and cross-modal information retrieval. Through this comprehensive overview, we highlight the inherent challenges in adopting MMML in engineering design, and proffer potential directions for future research. To spur on the continued evolution of MMML in engineering design, we advocate for concentrated efforts to construct extensive multi-modal design datasets, develop effective data-driven MMML techniques tailored to design applications, and enhance the scalability and interpretability of MMML models. MMML models, as the next generation of intelligent design tools, hold a promising future to impact how products are designed.",10.48550/arXiv.2302.10909
9736d7b05db9c1b157e8fb5022d0cc05fd484127,Sensor Fusion and Multimodal Learning for Robotic Grasp Verification Using Neural Networks,2022,"Different sensors on a robot help in understanding different aspects of the environment they are working in; however, each sensor modality is often processed individually and information from other sensors is not utilized jointly. One of the reasons is different sampling rates and different dimensions of input modalities. In this paper, we use multimodal data fusion techniques such as early, late and intermediate fusion for grasp failure identification using four different 3D convolution-based multimodal neural networks (3D-MNN). Our results on a visual-tactile dataset shows that the performance of the classification task is improved while using multimodal data. In addition, a neural network trained with 30:22 train-test split of multimodal data achieved accuracy comparable to a network trained with 78:22 train-test split of unimodal data1.",10.1109/ICPR56361.2022.9955646
5a27a89cd5c35f76b57b93aad41c75af65b2dc6d,Cross-Domain Pseudo-Sensors in IBSM,2018,"The applicable (sensor) function table (AFT) is one of the 6 major components of the Information Based Sensor Management (IBSM) approach to sensor management. The AFT lists all available sensing actions from which the information instantiator (II) can downselect to an admissible set of sensor functions which are capable of satisfying an information request. The II further orders the admissible set by their expected (sensor) information value rate (EIVR-sen) for passing observation requests to the sensors. While the AFT was initially developed for hard sensors, it is becoming increasingly important to not only develop methods of characterizing soft sensors for representation in the AFT, but also for characterizing approximate contemporaneous measurements by both hard and soft sensors as AFT entries. Hard sensor entries in the AFT are reviewed. Suggestions are presented for AFT entries of several representative soft sensors. Hard and soft sensing functions are then combined where possible into cross-domain (hard/soft) pseudo-sensor entries. Additionally, brief examples are presented which demonstrate how the AFT entries may be represented in the Sensor Model Language (SML) format for improved automation of the sensor management process.",10.23919/ICIF.2018.8455569
f536f46cce7e626324c06560cef892d1e397615f,Curious partner: An approach to realize common ground in human-autonomy collaboration,2016,"A dialog-based human-autonomy interaction ap- proach, called curious partner, is presented for a class of systems where the role of autonomy is to assist humans in decision making tasks. Even if the human and the autonomy share the environment and receive identical information, they may have inconsistencies in the representation of the environment due to difference in their perception and expert knowledge. The curious partner interaction framework is presented to resolve model-level differences between the human and the autonomy to establish common ground. The knowledge base of the autonomy is modeled using a Bayesian engine. The autonomy's dialog with the human acts as a feedback mechanism to resolve any differences either by suggesting maximally probable actions to human based on the state of its Bayesian model or by updating its model to achieve analogous world representation.",10.1109/SMC.2016.7844865
71641992094ed2e0f14cec6b1bec42caeb9ddebd,Speech signal analysis for the estimation of heart rates under different emotional states,2016,"A non-invasive method for the monitoring of heart activity can help to reduce the deaths caused by heart disorders such as stroke, arrhythmia and heart attack. Human voice can be considered as a biometric data that can be used for estimation of heart rate. In this paper, we propose a method for estimating the heart rate from human speech dynamically using voice signal analysis and by development of a empirical linear predictor model. The correlation between voice signal and heart rate are established by classifiers and prediction of the heart rates with or without emotions are done using linear models. The prediction accuracy was tested using the data collected from 15 subjects, it is about 4050 samples of speech signals and corresponding electrocardiogram samples. The proposed approach can use for early non-invasive detection of heart rate changes that can be correlated to emotional state of the individual and also can be used as tool for diagnosis of heart conditions in real-time situations.",10.1109/ICACCI.2016.7732201
ba8fabb994f41e1849616e44c3a645a21c22056a,Heart rate monitoring using human speech spectral features,2015,"This paper attempts to establish a correlation between the human speech, emotions and human heart rate. The study highlights a possible contactless human heart rate measurement technique useful for monitoring of patient condition from real-time speech recordings. The distance between the average peak-to-peak distances in speech Mel-frequency cepstral coefficients are used as the speech features. The features when tested on 20 classifiers from the data collected from 30 subjects indicate a non-separable classification problem, however, the classification accuracies indicate the existence of strong correlation between the human speech, emotion and heart-rates.",10.1186/s13673-015-0052-z
02a83a755a5e4c5dbb445b132a9c991deea0245c,The Paradigm Shift to Multimodality in Contemporary Computer Interfaces,2015,,10.1007/978-3-031-02213-5
d1e0e182117f80e2b364a856710f6ccf9091eb4f,Discriminative histogram taxonomy features for snake species identification,2014,"BackgroundIncorrect snake identification from the observable visual traits is a major reason for death resulting from snake bites in tropics. So far no automatic classification method has been proposed to distinguish snakes by deciphering the taxonomy features of snake for the two major species of snakes i.e. Elapidae and Viperidae. We identify 38 different taxonomically relevant features to develop the Snake database from 490 sample images of Naja Naja (Spectacled cobra), 193 sample images of Ophiophagus Hannah (King cobra), 88 images of Bungarus caeruleus (Common krait), 304 sample images of Daboia russelii (Russell’s viper), 116 images of Echis carinatus (Saw scaled viper) and 108 images of Hypnale hypnale (Hump Nosed Pit Viper).ResultsSnake identification performances with 13 different types of classifiers and 12 attribute elevator demonstrate that 15 out of 38 taxonomically relevant features are enough for snake identification. Interestingly, these features were almost equally distributed from the logical grouping of top, side and body views of snake images, and the features from the bottom view of snakes had the least role in the snake identification.ConclusionWe find that only few of the taxonomically relevant snake features are useful in the process of snake identification. These discriminant features are essential to improve the accuracy of snake identification and classification. The presented study indicate that automated snake identification is useful for practical applications such as in medical diagnosis, conservation studies and surveys by interdisciplinary practitioners with little expertise in snake taxonomy.",10.1186/s13673-014-0003-0
001dddf2b8e38d79564ce7b756d5ceead1e1da59,Routine learning: analyzing your whereabouts,2004,"A routine is a temporal context sequence that occurs often. In routine learning, already recognized contexts are utilized in modeling the user behavior. The methodology is presented via a use case scenario. Data collected from various ubiquitous sensors are used in recognizing and defining contexts, and association rules determine the routines. The focus is on testing the suitability of the apriori algorithm for this application area. Several useful routines were derived from the user data, and the results show that data mining can be utilized in pervasive computing.",10.1109/ITCC.2004.1286633
809624aa1b6a10ebee3e9ed1f3f3f25f67365e1a,Utilizing context-awareness in office-type working life,2004,This paper presents a context-aware mobile application for office-type working purposes. Via the user tests we have evaluated if this kind of application can support worker's daily life. In the experiments we had real users in real working environment. Our investigation illustrates that workers could utilize context-aware features to ease up their working routines such as keeping presentations. The results of this paper can help designers and developers to envision and implement future ubiquitous devices and environments.,10.1145/1052380.1052392
8722bbf08207d8713c61cad0b8f46130561b9d99,Virtual Reality in Computer Science Education: A Systematic Review,2020,"Virtual reality (VR) technologies have become more affordable and accessible in recent years. This is opening up new methods and opportunities in the field of digital learning. VR can offer new forms of interactive learning and working, especially for subjects from the STEM (Science, technology, engineering, and mathematics) area. In this context we investigate the potential and application of VR for computer science education with a systematic review in this paper. We present a formal literature review on the use of VR technologies in computer science education. We focus on the identification of factors such as learning objectives, technologies used, interaction characteristics, and challenges and advantages of using fully immersive VR for computer science education.",10.1145/3385956.3418947
30c2a18a1868089f3c67f1a033b2d22a6e766a20,CubeVR: Digital Affordances for Architecture Undergraduate Education using Virtual Reality,2019,"CubeVR incorporates the traditional studio-based learning in architecture with virtual reality (VR) and in doing so it enables features that are not available or difficult to achieve in the real-world. CubeVR allows students and instructors to move from the existing studio to a virtual environment, allowing for a more impactful learning experience. This paper presents our current state of the CubeVR prototype, informal impressions of users, and the use of different affordances in the virtual world.",10.1109/VR.2019.8798115
a7690966fff0f8961e85b911cdd2ca1ebcae4869,Incorporation of 3D ICT elements into class,2017,"The fact that there are relatively few job prospects has caused a decline in the vocational profile of students in engineering, while they demand more dynamic and attractive classes. Moreover, the difficulty of the concepts to be transmitted, together with the lack of good spatial visualization abilities of students, makes technical drawing a difficult subject to understand. The present study set out to investigate emerging trends in ICT to find a way to present and manipulate information in class that would serve as both a stimulating element and a tool for performing complex operations, thus improving the teaching methodology of the classes and enhancing students’ motivation. We present the results of an experimental study of the introduction of 3D augmented reality models for practical classes in technical drawing. Several measures are collected for the monitoring of the course by students, and for their motivation, according to the ARCS method. The results indicate a good response to the elements incorporated into first year classes of a university course in Graphic Expression in Engineering. In particular, the experimental group displayed a higher level of motivation, an increased attendance to practical classes, a higher rate of practical work delivered, and a lower percentage of dropouts. © 2017 Wiley Periodicals, Inc. Comput Appl Eng Educ 25:542–549, 2017; View this article online at wileyonlinelibrary.com/journal/cae; DOI 10.1002/cae.21802",10.1002/cae.21802
318683a3a0943930479e77c0d1322c79fed6ca07,Archinotes: A tool for assisting software architecture courses,2013,"During the last years, software architecture has become an increasingly necessary subject in IT education. At Universidad de los Andes, a software architecture course is taught midway through the Systems and Computer Engineering degree and aims to create skills within the student to identify quality requirements as well as designing software architectures for middle-sized systems. However, based on statistics over the last 8 years of our Software Architecture course, we've noticed that correctly identifying quality requirements seems to be a recurring issue among students. One of the course's objectives comprehends the construction of an architecture document during the semester, which must be done in groups who tend to find it difficult to get together and work as a team. This paper present Archinotes, a collaboration tool to support the teaching of software architectures by facilitating collaboration workload and providing useful information on the difficulties and progress of the students to the course's teacher. The conclusions of the paper prove Archinotes effective in helping students learn critical concepts and artifacts during the software architecture course.",10.1109/CSEET.2013.6595239
001e94f525a7642d01202a810044a3e84969216a,VisMOOC: Visualizing video clickstream data from massive open online courses,2014,"Massive Open Online Courses (MOOCs) are becoming increasingly popular and have attracted much research attention. Analyzing clickstreams on MOOC videos poses a special analytical challenge but provides a good opportunity for understanding how students interact with course videos, which in turn can help instructors and educational analysts gain insights into online learning behavior. In this poster, we develop a visual analytical system, VisMOOC, to help instructors analyze the clickstream data. VisMOOC consists of three main views: the List View to list all course videos for analysts to select the video they are interested in; the Content-based View to show how each type of click actions change along the video timeline, which enables the most viewed sections to be observed and the most interesting patterns to be discovered; The Dashboard View shows the information of the clickstream data in different aspects, including the course information, the geographic distribution, the video temporal information, the video popularity, and the animation. Furthermore, case studies made by the instructors demonstrate the usefulness of VisMOOC and helped them gaining deep insights into learning behavior for MOOCs.",10.1109/VAST.2014.7042528
ac6877ada2af50d03c23f9c9d15d5e0a06d46d6d,"LearningViz: a dashboard for visualizing, analyzing and closing learning performance gaps - a case study approach",2024,"The availability of large-scale learning data presents unprecedented opportunities for investigating student learning processes. However, it is challenging for instructors to fully make sense of this data and effectively support their teaching practices. This study introduces LearningViz, an interactive learning analytics dashboard to help instructors identify, analyze, and close performance gaps among students in their classes. In this dashboard, we incorporated three modules to enhance human and computer interactions for better supporting the teaching practices: the Student Overall Performance Analysis Module, which provides a comprehensive understanding of students’ learning in the course; the Student Group Performance Analysis Module, which examines performance gaps across different groups and identifies factors contributing to these gaps; and the Final Exam Item Analysis Module, which evaluates the quality of exam questions and identifies strategies for closing performance gaps. The overall design of the platform follows a user-centered approach, integrating data analysis with various visualization strategies in a unified platform. A case study is then conducted to highlight the effectiveness of LearningViz in supporting instructors analyzing students’ learning patterns and associated factors impacting learning performance. We further conduct a usability test with several domain experts, to evaluate the usefulness and effectiveness of this platform in supporting the teaching practices. Our findings underscore the platform's ability to support instructors in detecting performance gaps among students, investigating influential factors, evaluating assessment quality and implementing targeted instructional strategies for closing performance gaps.",10.1186/s40561-024-00346-1
95ce00a13f7e916ef62c50401b006b5daaaa7cd0,Visualization of potential differences in comprehension by distribution of notes and questions in online programming courses,2023,"This study presents an innovative approach to understanding differences in student comprehension in online programming courses by analyzing note-taking and questioning patterns and the distribution of grades. It enables instructing educators to understand learners' learning behaviors better. We designed and developed a system to support online learning that enables students to take notes and ask questions while viewing online courses and allows instructors to grasp overall feedback. We used a visualization method based on the location of notes and questions as they appear in the learning videos to enable instructors to capture their learners' learning better. Visualizing the distribution of notes and questions shows differences among students' note-taking patterns and learning strategies. The underlying comprehension patterns involved lead to differences in the patterns of notes and questions. We compared data from learners in different score bands to verify the correlation between note types and scores. By visualizing and comparing notes with the same type of correlation, the results show the correlation between note types and grades and the potential of the visualization method to analyze students' learning strategies and learning personalities. It provides valuable insights into student engagement, comprehension, and learning strategies, which can inform the development of more effective teaching methods. This study is relevant in the contemporary educational landscape, particularly as many institutions transition to online formats. Detecting student engagement and comprehension levels in these contexts is crucial.",10.1109/TALE56641.2023.10398287
25deb9cc0da78e2cf307f2823cfe8b280b154052,LiveRetro: Visual Analytics for Strategic Retrospect in Livestream E-Commerce,2023,"Livestream e-commerce integrates live streaming and online shopping, allowing viewers to make purchases while watching. However, effective marketing strategies remain a challenge due to limited empirical research and subjective biases from the absence of quantitative data. Current tools fail to capture the interdependence between live performances and feedback. This study identified computational features, formulated design requirements, and developed LiveRetro, an interactive visual analytics system. It enables comprehensive retrospective analysis of livestream e-commerce for streamers, viewers, and merchandise. LiveRetro employs enhanced visualization and time-series forecasting models to align performance features and feedback, identifying influences at channel, merchandise, feature, and segment levels. Through case studies and expert interviews, the system provides deep insights into the relationship between live performance and streaming statistics, enabling efficient strategic analysis from multiple perspectives.",10.1109/TVCG.2023.3326911
a3631aabe1a597ac9236236b0ba8c72ce3420e9e,Visualizing timeline‐anchored comments enhanced social presence and information searching in video‐based learning,2023,"Numerous learners watch knowledgeable videos with comments or annotations anchored to the video timeline. These comments are learners’ discussions with abundant informational and social content along with video timelines, but the content is usually fragmented and scattered. To extract, organize, and highlight useful information from the discussion, we adopted text mining approaches and designed an interactive visualization tool in the lecture interface for learners, including the following components along with the video timeline: (1) the relevance of comments to the lecture, (2) the comment topics throughout the lecture, and (3) the difficulty level perceived by learners. We conducted a lab experiment with 24 students to examine the effects of the visualization tool on the learning process and outcomes. We found that learners perceived a significantly higher social presence and performed better in open‐book quizzes, searching tasks, and summarizing lectures using the visualization tool. This suggests that the visualization of timeline‐anchored commenting potentially facilitates learners’ participation in discussions and contributions to the learning community.",10.1002/cae.22641
9a2d5eefd7ca7939a643294a87a1a324bd03ae0e,Efficacy improvement in searching MEDLINE database using a novel PubMed visual analytic system: EEEvis,2023,"PubMed is the most extensively used database and search engine in the biomedical and healthcare fields. However, users could experience several difficulties in acquiring their target papers facing massive numbers of search results, especially in their unfamiliar fields. Therefore, we developed a novel user interface for PubMed and conducted three steps of study: step A, a preliminary user survey with 76 medical experts regarding the current usability for the biomedical literature search task at PubMed; step B is implementing EEEvis, a novel interactive visual analytic system for the search task; step C, a randomized user study comparing PubMed and EEEvis. First, we conducted a Google survey of 76 medical experts regarding the unmet needs of PubMed and the user requirements for a novel search interface. According to the data of preliminary Google survey, we implemented a novel interactive visual analytic system for biomedical literature search. This EEEvis provides enhanced literature data analysis functions including (1) an overview of the bibliographic features including publication date, citation count, and impact factors, (2) an overview of the co-authorship network, and (3) interactive sorting, filtering, and highlighting. In the randomized user study of 24 medical experts, the search speed of EEEvis was not inferior to PubMed in the time to reach the first article (median difference 3 sec, 95% CI -2.1 to 8.5, P = 0.535) nor in the search completion time (median difference 8 sec, 95% CI -4.7 to 19.1, P = 0.771). However, 22 participants (91.7%) responded that they are willing to use EEEvis as their first choice for a biomedical literature search task, and 21 participants (87.5%) answered the bibliographic sorting and filtering functionalities of EEEvis as a major advantage. EEEvis could be a supplementary interface for PubMed that can enhance the user experience in the search for biomedical literature.",10.1371/journal.pone.0281422
001f207cc7a80ad08cdbab2b8eca23b0a8618cb3,Regularized robust fuzzy least squares twin support vector machine for class imbalance learning,2020,"Twin support vector machines (TWSVM) have been successfully applied to the classification problems. TWSVM is computationally efficient model of support vector machines (SVM). However, in real world classification problems issues of class imbalance and noise provide great challenges. Due to this, models lead to the inaccurate classification either due to higher tendency towards the majority class or due to the presence of noise. We provide an improved version of robust fuzzy least squares twin support vector machine (RFLSTSVM) known as regularized robust fuzzy least squares twin support vector machine (RRFLSTSVM) to handle the imbalance problem. The advantage of RRFLSTSVM over RFLSTSVM is that the proposed RRFLSTSVM implements the structural risk minimization principle by the introduction of regularization term in the primal formulation of the objective functions. This modification leads to the improved classification as it embodies the marrow of statistical learning theory. The proposed RRFLSTSVM doesn’t require any extra assumption as the matrices resulting in the dual are positive definite. However, RFLSTSVM is based on the assumption that the inverse of the matrices resulting in the dual always exist as the matrices are positive semi-definite. To subsidize the effects of class imbalance and noise, the data samples are assigned weights via fuzzy membership function. The fuzzy membership function incorporates the imbalance ratio knowledge and assigns appropriate weights to the data samples. Unlike TWSVM which solves a pair of quadratic programming problem (QPP), the proposed RRFLSTSVM method solves a pair of system of linear equations and hence is computationally efficient. Experimental and statistical analysis show the efficacy of the proposed RRFLSTSVM method.",10.1109/IJCNN48605.2020.9207724
f250ef0a7e3bdc60888dabea5ee31ec0e1e59662,Methods for Class-Imbalanced Learning with Support Vector Machines: A Review and an Empirical Evaluation,2024,"This paper presents a review on methods for class-imbalanced learning with the Support Vector Machine (SVM) and its variants. We first explain the structure of SVM and its variants and discuss their inefficiency in learning with class-imbalanced data sets. We introduce a hierarchical categorization of SVM-based models with respect to class-imbalanced learning. Specifically, we categorize SVM-based models into re-sampling, algorithmic, and fusion methods, and discuss the principles of the representative models in each category. In addition, we conduct a series of empirical evaluations to compare the performances of various representative SVM-based models in each category using benchmark imbalanced data sets, ranging from low to high imbalanced ratios. Our findings reveal that while algorithmic methods are less time-consuming owing to no data pre-processing requirements, fusion methods, which combine both re-sampling and algorithmic approaches, generally perform the best, but with a higher computational load. A discussion on research gaps and future research directions is provided.",10.1007/s00500-024-09931-5
786d99ce9386d77f22a59197a53e9a8e81d09b1a,Large-Scale Fuzzy Least Squares Twin SVMs for Class Imbalance Learning,2022,"Twin support vector machines (TSVMs) have been successfully employed for binary classification problems. With the advent of machine learning algorithms, data have proliferated and there is a need to handle or process large-scale data. TSVMs are not successful in handling large-scale data due to the following: 1) the optimization problem solved in the TSVM needs to calculate large matrix inverses, which makes it an ineffective choice for large-scale problems; 2) the empirical risk minimization principle is employed in the TSVM and, hence, may suffer due to overfitting; and 3) the Wolfe dual of TSVM formulation involves positive-semidefinite matrices, and hence, singularity issues need to be resolved manually. Keeping in view the aforementioned shortcomings, in this article, we propose a novel large-scale fuzzy least squares TSVM for class imbalance learning (LS-FLSTSVM-CIL). We formulate the LS-FLSTSVM-CIL such that the proposed optimization problem ensures that: 1) no matrix inversion is involved in the proposed LS-FLSTSVM-CIL formulation, which makes it an efficient choice for large-scale problems; 2) the structural risk minimization principle is implemented, which avoids the issues of overfitting and results in better performance; and 3) the Wolfe dual formulation of the proposed LS-FLSTSVM-CIL model involves positive-definite matrices. In addition, to resolve the issues of class imbalance, we assign fuzzy weights in the proposed LS-FLSTSVM-CIL to avoid bias in dominating the samples of class imbalance problems. To make it more feasible for large-scale problems, we use an iterative procedure known as the sequential minimization principle to solve the objective function of the proposed LS-FLSTSVM-CIL model. From the experimental results, one can see that the proposed LS-FLSTSVM-CIL demonstrates superior performance in comparison to baseline classifiers. To demonstrate the feasibility of the proposed LS-FLSTSVM-CIL on large-scale classification problems, we evaluate the classification models on the large-scale normally distributed clustered (NDC) dataset. To demonstrate the practical applications of the proposed LS-FLSTSVM-CIL model, we evaluate it for the diagnosis of Alzheimer’s disease and breast cancer disease. Evaluation on NDC datasets shows that the proposed LS-FLSTSVM-CIL has feasibility in large-scale problems as it is fast in comparison to the baseline classifiers.",10.1109/TFUZZ.2022.3161729
6144728018865691618dd340f3bad17dfc06d720,Automatic visual detection of human behavior: A review from 2000 to 2014,2015,,10.1016/j.eswa.2015.05.023
cfe9532588b532fd85822f3f71c570835f493aac,Mobile shopping cart application using kinect,2013,"This paper presents a novel application of a perception sensor Kinect to a mobile shopping cart (MSC). We attempt to exercise daily-used instructions as commands by using skeleton tracking of the Kinect sensor. A pair of differential-driven wheels together with dynamical system is mounted on the chassis of the shopping cart, which reshapes the shopping cart as a mobile robot. The commercialized Kinect sensor is employed to detect the gestures of the human body. The mobile shopping cart equipped with Kinect could execute different gesture command, for example, go ahead, draw back, set stop and so on. The response time is within 0.5 second. To do the right reflexes, we define a set of trajectory determined by the gesture of the target customer. The experiment result shows the validity of proposed method.",10.1109/URAI.2013.6677370
001fde8063a189d95ebad3831cdd9ddbc2b4eec1,Skill Memories for Parameterized Dynamic Action Primitives on the Pneumatically Driven Humanoid Robot Child Affetto,2018,"In this work, we propose an extension of parameterized skills to achieve generalization of forward control signals for action primitives that result in an enhanced control quality of complex robotic systems. We argue to shift the complexity of learning the full dynamics of the robot to a lower dimensional task related learning problem. Due to generalization over task variability, online learning for complex robots as well as complex scenarios becomes feasible. We perform an experimental evaluation of the generalization capabilities of the proposed online learning system through simulation of a compliant 2DOF arm. Scalability to a complex robotic system is demonstrated on the pneumatically driven humanoid robot Affetto including 6DOF.",10.1109/DEVLRN.2018.8761040
12a494043e159e0c6b47b8c72c16b3e3523e9272,Adaptation Through Prediction: Multisensory Active Inference Torque Control,2021,"Adaptation to external and internal changes is of major importance for robotic systems in uncertain environments. Here, we present a novel multisensory active inference (AIF) torque controller for industrial arms that shows how prediction can be used to resolve adaptation. Our controller, inspired by the predictive brain hypothesis, improves the capabilities of current AIF approaches by incorporating learning and multimodal integration of low- and high-dimensional sensor inputs (e.g., raw images) while simplifying the architecture. We performed a systematic evaluation of our model on a 7DoF Franka Emika Panda robot arm by comparing its behavior with previous AIF baselines and classic controllers, analyzing both qualitatively and quantitatively adaptation capabilities and control accuracy. The results showed improved control accuracy in goal-directed reaching with high noise rejection due to multimodal filtering, and adaptability to dynamical inertial changes, elasticity constraints, and human disturbances without the need to relearn the model or parameter retuning.",10.1109/TCDS.2022.3156664
12d56574d0c6bc3a2ecba49748c674d74c3022a2,Enhancing Robot-Environment Physical Interaction via Optimal Impedance Profiles,2020,"Physical interaction of robots with their environment is a challenging problem because of the exchanged forces. Hybrid position/force control schemes often exhibit problems during the contact phase, whereas impedance control appears to be more simple and reliable, especially when impedance is shaped to be energetically passive. Even if recent technologies enable shaping the impedance of a robot, how best to plan impedance parameters for task execution remains an open question. In this paper we present an optimization-based approach to plan not only the robot motion but also its desired end-effector mechanical impedance. We show how our methodology is able to take into account the transition from free motion to a contact condition, typical of physical interaction tasks. Results are presented for planar and three-dimensional open-chain manipulator arms. The compositionality of mechanical impedance is exploited to deal with kinematic redundancy and multi-arm manipulation.",10.1109/BioRob49111.2020.9224382
001ff9ef287fe7af74ef5766e7e6389465235b10,Online Adaptation of Robot Pushing Control to Object Properties,2018,"Pushing is a common task in robotic scenarios. In real-world environments, robots need to manipulate various unknown objects without previous experience. We propose a data-driven approach for learning local inverse models of robot-object interaction for push manipulation. The robot makes observations of the object behaviour on the fly and adapts its movement direction. The proposed model is probabilistic, and we update it using maximum a posteriori (MAP) estimation. We test our method by pushing objects with a holonomic mobile robot base. Validation of results over a diverse object set demonstrates a high degree of robustness and a high success rate in pushing objects towards a fixed target and along a path compared to previous methods. Moreover, based on learned inverse models, the robot can learn object properties and distinguish between different object behaviours when they are pushed from different sides.",10.1109/IROS.2018.8594192
56d1864e48f03bba56a632bb1fc11cf6cf59197d,Zero Moment Two Edge Pushing of Novel Objects With Center of Mass Estimation,2023,"Pushing is one of the fundamental nonprehensile manipulation skills to impart to an object changes in position and orientation. To exploit this skill to manipulate novel objects, explicit knowledge of their physical properties should be given a priori. In this work, we estimate the center of mass (CoM) of an object by narrowing down its probable location with a deep learning model and Mason’s voting theorem. In addition, we propose the Zero Moment Two Edge Pushing (ZMTEP) method to translate a novel object without rotation to a goal pose. The proposed method enables a pusher to select the most suitable two-edge-contact configuration for a given object using the estimated CoM and the geometrical shape of the object. Notably, neither the friction between the object and its support plane nor the friction between the object and the pusher are assumed to be known. We evaluate the proposed CoM estimation and ZMTEP methods through a series of experiments in both simulation and real robotic pusher settings. The result shows that the CoM estimation method has good mean squared error properties and small standard deviation, and the ZMTEP method significantly outperforms competitive baseline methods. Note to Practitioners—This article aims to endow robotic arms with the capability of moving or aligning objects by pushing, which is much more simple and secure than pick-and-place or in-hand manipulations. Most in-demand manipulation skills require sophisticated hand design and control, which might not be affordable for industrial applications staying cost-competitive. In contrast, robot pushing can be implemented with different types of simple pushers and straightforwardly applied to pre-grasp manipulation. This article makes the estimation of an object’s CoM location practical. Building upon the estimation method, a robust and noise-tolerant two-edge-contact pushing configuration selection method is presented to translate an arbitrarily shaped unknown object to its goal pose.",10.1109/TASE.2022.3208739
9a4da56e61b49a4ed905e4a9ad7d9d8163c4c485,Let's Push Things Forward: A Survey on Robot Pushing,2019,"As robots make their way out of factories into human environments, outer space, and beyond, they require the skill to manipulate their environment in multifarious, unforeseeable circumstances. With this regard, pushing is an essential motion primitive that dramatically extends a robot's manipulation repertoire. In this work, we review the robotic pushing literature. While focusing on work concerned with predicting the motion of pushed objects, we also cover relevant applications of pushing for planning and control. Beginning with analytical approaches, under which we also subsume physics engines, we then proceed to discuss work on learning models from data. In doing so, we dedicate a separate section to deep learning approaches which have seen a recent upsurge in the literature. Concluding remarks and further research perspectives are given at the end of the paper.",10.3389/frobt.2020.00008
63b1df2b3f7c33811101a671d6196cd73f7452dd,Design and Implementation of Non-prehensile Manipulation Strategies,2022,": Grasping of objects is not always feasible for robot manipulators, e.g",10.5220/0011320700003271
52e34be43a5c53a157b6515273d060fd241a9de6,PARSIR: a Package for Effective Parallel Discrete Event Simulation on Multi-processor Machines,2024,"In this article we present PARSIR (PARallel SImulation Runner), a package that enables the effective exploitation of shared-memory multi-processor machines for running discrete event simulation models. PARSIR is a compile/run-time environment for discrete event simulation models developed with the C programming language. The architecture of PARSIR has been designed in order to keep low the amount of CPU-cycles required for running models. This is achieved via the combination of a set of techniques like: 1) causally consistent batch-processing of simulation events at an individual simulation object for caching effectiveness; 2) high likelihood of disjoint access parallelism; 3) the favoring of memory accesses on local NUMA (Non-Uniform-Memory-Access) nodes in the architecture, while still enabling well balanced workload distribution via work-stealing from remote nodes; 4) the use of RMW (Read-Modify-Write) machine instructions for fast access to simulation engine data required by the worker threads for managing the concurrent simulation objects and distributing the workload. Furthermore, any architectural solution embedded in the PARSIR engine is fully transparent to the application level code implementing the simulation model. We also provide experimental results showing the effectiveness of PARSIR when running the reference PHOLD benchmark on a NUMA shared-memory multi-processor machine equipped with 40 CPUs.",10.1109/DS-RT62209.2024.00022
53eba4ee8f79ce86ab0b06303b23ef1a69fe67d4,Spatial/Temporal Locality-based Load-sharing in Speculative Discrete Event Simulation on Multi-core Machines,2024,"Shared-memory multi-processor/multi-core machines have become a reference for many application contexts. In particular, the recent literature on speculative parallel discrete event simulation has reshuffled the architectural organization of simulation systems in order to deeply exploit the main features of this type of machines. A core aspect dealt with has been the full sharing of the workload at the level of individual simulation events, which enables keeping the rollback incidence minimal. However, making each worker thread continuously switch its execution between events destined to different simulation objects does not favor locality. This problem appears even more evident in the case of Non-Uniform-Memory-Access (NUMA) machines, where memory accesses generating a cache miss to be served by a far NUMA node give rise to both higher latency and higher traffic at the level of the NUMA interconnection. In this article, we propose a workload-sharing algorithm where the worker threads can have short-term binding with specific simulation objects to favor spatial locality. The new bindings—carried out when a thread decides to switch its execution to other simulation objects—are based on both (a) the timeline according to which the object states have passed through the caching hierarchy and (b) the (dynamic) placement of objects within the NUMA architecture. At the same time, our solution still enables the worker threads to focus their activities on the events to be processed whose timestamps are closer to the simulation commit horizon—hence we exploit temporal locality along virtual time and keep the rollback incidence minimal. In our design we exploit lock-free constructs to support scalable thread synchronization while accessing the shared event pool. Furthermore, we exploit a multi-view approach of the event pool content, which additionally favors local accesses to the parts of the event pool that are currently relevant for the thread activity. Our solution has been released as an integration within the USE (Ultimate-Share-Everything) open source speculative simulation platform available to the community. Furthermore, in this article we report the results of an experimental study that shows the effectiveness of our proposal.",10.1145/3639703
d190eaaf0c52e9b2a509ccd4308866659fc3e5cb,NUMA Time Warp,2015,"It is well known that Time Warp may suffer from large usage of memory, which may hamper the efficiency of the memory hierarchy. To cope with this issue, several approaches have been devised, mostly based on the reduction of the amount of used virtual memory, e.g., by the avoidance of checkpointing and the exploitation of reverse computing. In this article we present an orthogonal solution aimed at optimizing the latency for memory access operations when running Time Warp systems on Non-Uniform Memory Access (NUMA) multi-processor/multi-core computing systems. More in detail, we provide an innovative Linux-based architecture allowing per simulation-object management of memory segments made up by disjoint sets of pages, and supporting both static and dynamic binding of the memory pages reserved for an individual object to the different NUMA nodes, depending on what worker thread is in charge of running that simulation object along a given wall-clock-time window. Our proposal not only manages the virtual pages used for the live state image of the simulation object, rather, it also copes with memory pages destined to keep the simulation object's event buffers and any recoverability data. Further, the architecture allows memory access optimization for data (messages) exchanged across the different simulation objects running on the NUMA machine. Our proposal is fully transparent to the application code, thus operating in a seamless manner. Also, a free software release of our NUMA memory manager for Time Warp has been made available within the open source ROOT-Sim simulation platform. Experimental data for an assessment of our innovative proposal are also provided in this article.",10.1145/2769458.2769479
1b7d4841b6bd050812d78af9cab9f556d19b8231,Reverse computation for rollback-based fault tolerance in large parallel systems,2014,,10.1007/s10586-013-0277-4
0020c8e9944c238cb50ac871e371aead1b582261,Cepstrum Based Algorithm for Motor Imagery Classification,2016,"A linear convolutive mixing model based real time motor imagery classification algorithm is proposed in this paper. The proposed cepstrum based method is very first and robust unsupervised learning algorithm, extremely useful for real time brain computer interface(BCI). The cepstrum is analyzed for estimation of combined action potential generated through the active synapses of raw electroencephalogram (EEG) signal. Maximum energy of the estimated cepstrum, is used as a feature. The extracted feature further subjected to simple Bayesian probabilistic classifier, for classification. The proposed method of EEG signal pre-processing and feature extraction outperforms the conventional temporal relative spectral power (TRSP) based movement imagery classification algorithm and BCI competition II results.",10.1109/ICMETE.2016.140
00970f4d0e3e822f4d3d5785f4a67c360081ac3a,Identifying EEG Binary Limb Motor Imagery Movements using Thick Data Analytics,2020,"Electroencephalography (EEG) is non-invasive technology that is widely used to record brain signals in brain computer interfacing (BCI) systems to control, motor imagery, in which movements signals occurring in limbs can control some services. Researchers have proposed numerous classification schemes of these motor imagery to incorporate it with various neurorehabilitation, neuroprosthetics and gaming applications. However, the existing classification schemes face the performance degradation caused by motor-imagery EEG signals with low signal to noise ratio. The paper’s main objective is to use possible thick data analytics techniques to classify effectively the motor imagery EEG signals. Our attempt start with notable classifiers including Decision Trees, Extra Trees, Naive Bayes, Random Forest and SVM and move later to enhance classifications using variety of ensemble learning techniques including Bagging, Adaboost and Stacking. More techniques has been applied on the results of the ensemble learring to eliminate classification noise and supply more relevant features such as substituting outliers with mean value and exercising band-pass filter and Common Spatial Pattern (CSP). The thick data methods has been validated on a public dataset rendered by BCI competition II dataset III and was found to produce better classification performance metric which included performance metric parameters like accuracy, specificity, sensitivity, precision and recall when confronted with the existing work, thus projecting the usefulness of motor imagery BCI. The analytics is inclusive of Area Under the Curve (AUC) score and Mathews Correlation Coefficient (MCC) score to display an impactful analysis. The first operation step involved idle for 6 seconds and then thinking of moving or hand on the alert The signal composed of three were The hybrid",10.47116/APJCRI.2020.09.15
b43cf5e271d2244a0a9877e901ce9d0c7c42c1db,Reactive frequency band-based real-time motor imagery classification,2018,"The requirement of an effective online processing algorithm becomes very vital to fulfilling the demand of the low-cost brain–computer interface (BCI) system. The authors proposed a very first and robust unsupervised machine learning algorithm, for the real-time classification of movement imagination. The reactive frequency band (RFB) of the individual subject has been identified through the dominant frequency detection algorithm over the training dataset. Based on the identified RFB, the feature extraction process has been applied to the testing dataset. The estimated 'feature' further classified as per probabilistic Bayesian classifier. The effectiveness of the proposed RFB detection method of electroencephalogram (EEG) signal is validated by self-generated artificial sine wave signal, single subject and nine subject movement imagery (MI) BCI competition dataset. The proposed method of EEG signal processing outperformed the conventional wavelet-based BCI competition II results and the wavelet-based algorithm applied over the BCI competition IV dataset.",10.1504/IJISTA.2018.10012890
0020ca8c077c3b871aa1e1ca47dcc84367f52a87,Why Should Computer and Information Science Programs Require Service Learning?,2022,"Service learning-an educational experience in which students provide service to a community partner while learning content knowledge, professional skills, and critical thinking-can provide significant benefits to students and the community. We present survey results from 227 postsecondary students in computing to provide insights into their attitudes toward service learning, and how these relate to course-taking motivations and sense of civic duty. Based on the survey results, we argue that service learning should be required in an undergraduate computing major. However, we problematize this provocation based on three types of pitfalls: courses that do not prepare students to understand social contexts in which technical solutions are promoted, lack of resources for faculty teaching the courses, and the potential to harm both community partners and students.",10.1145/3478431.3499390
874cded95b1f8cb362a102a15865f38a75e1bda7,"Faculty, Student, and Community Partner Experiences in Computer and Information Science Service Learning",2024,"Service learning, a high impact pedagogy, involves integrating academic outcomes with service to the community. The success of service learning experiences depends on the development of mutually reciprocal relationships between students, instructors, and community partners, ensuring equitable benefits for all stakeholders. To explore how relationship-building and growth are supported in computer and information science (CIS) service learning, we conducted semi-structured interviews with 13 informants—each a faculty, student, or community partner who participated in one of 5 computer or information science service learning courses. Our analysis identified three factors that were most crucial in supporting the formation of relationships among stakeholders: infrastructuring the relationship, valuing technical and other expertise equitably, and integrating soft skills and technical skills. Based on these findings, we discuss how growth, an important outcome of relationship-building and equitable service learning experiences, can be supported and assessed in CIS service learning experiences.",10.1145/3654678
dd2c38da738860e9b5a4d87f62210e26afed3e6a,Putting the Service into Service Learning: A Report on a Survey of CS Faculty,2024,"Service learning is an experiential pedagogy in which students learn through providing services or products for community partners. Computer and information science students can develop valuable products for community organizations. However, while service learning is shown to serve students and has potential to serve the field's diversity goals, community partners' needs are often not served. We explored this asymmetry using an exploratory survey. Faculty from across the U.S. were able to describe learning goals for students, including how they were assessed. In contrast, fewer than half of respondents had explicit partner goals; partner goals were often not assessed. Also, most respondents judged reaching student goals as more important than partner goals, with about 25% of respondents seeing benefits to partners as only a bonus. Faculty justified their choices by appealing to their mission as educators. Yet for a nontrivial partnership commitment under condition of scarce resources, the community partner may be seen as being taken advantage of, which may explain why some respondents have difficulty finding or keeping partners. Further, faculty may not accomplish civic duty goals, since students may tacitly learn that community organizations' needs are secondary. To aid faculty in making decisions and better integrating community partners' needs, we offer advice from survey respondents.",10.1145/3626252.3630910
712d1f8b85d4e25fc77bb0d143b3200f4db78258,Student Reflections on Service-Learning in Software Engineering and Their Experiences with Non-technical Clients,2023,"Participation in service-learning projects that impact society or serve the greater social good has been shown to have a broad range of positive impacts on students, including increased motivation and persistence, improved social outcomes, self-efficacy, and leadership skills, and an increased sense of civic duty. This paper describes a longitudinal study that examines the impacts of service learning projects on the students' perceptions about applying their knowledge and skills in a practical authentic context. The results suggest that while service learning has positive effects on student learning outcomes, there are challenges unique to software engineering projects done for non-technical clients that might not be present when similar projects are sponsored by industrial clients. This paper summarizes these challenges and describes a number of solutions that worked in our context.",10.1145/3576882.3617929
53f258ad9abc09728db7cbbead2c4e3e19d676f2,Teaching Ethics in Computing: A Systematic Literature Review of ACM Computer Science Education Publications,2023,"The computing education research community now has at least 40 years of published research on teaching ethics in higher education. To examine the state of our field, we present a systematic literature review of papers in the Association for Computing Machinery computing education venues that describe teaching ethics in higher-education computing courses. Our review spans all papers published to SIGCSE, ICER, ITiCSE, CompEd, Koli Calling, and TOCE venues through 2022, with 100 papers fulfilling our inclusion criteria. Overall, we found a wide variety in content, teaching strategies, challenges, and recommendations. The majority of the papers did not articulate a conception of “ethics,” and those that did used many different conceptions, from broadly applicable ethical theories to social impact to specific computing application areas (e.g., data privacy and hacking). Instructors used many different pedagogical strategies (e.g., discussions, lectures, assignments) and formats (e.g., stand-alone courses, incorporated within a technical course). Many papers identified measuring student knowledge as a particular challenge, and 59% of papers included mention of assessments or grading. Of the 69% of papers that evaluated their ethics instruction, most used student self-report surveys, course evaluations, and instructor reflections. While many papers included calls for more ethics content in computing, specific recommendations were rarely broadly applicable, preventing a synthesis of guidelines. To continue building on the last 40 years of research and move toward a set of best practices for teaching ethics in computing, our community should delineate our varied conceptions of ethics, examine which teaching strategies are best suited for each, and explore how to measure student learning.",10.1145/3634685
541433e00644840c25601b98509b6de954c3ea66,CISing Up Service Learning: A Systematic Review of Service Learning Experiences in Computer and Information Science,2023,"The benefits of service learning in computer and information science (CIS) are believed to be significant, ranging from providing students with real-world experiences to retaining students to positively impacting community partners. Although there are many benefits of service learning, the CIS domain does impose unique costs for integrating service learning into the curriculum. Yet there is little systematic research to help the CIS community understand best practices for maximizing benefits while minimizing costs. Experience reports about service learning courses in CIS have appeared in the literature annually since 2000, and thus we address this gap in knowledge by conducting a systematic review and content analysis of 84 experience reports from the The ACM Guide to Computing Literature. We synthesize the current state of service learning in CIS as well as derive recommendations for best practices and future research directions.",10.1145/3610776
5bb607e7246519375f75bed264acbc9c92307b09,Novel Human-in-the-Loop (HIL) Simulation Method to Study Synthetic Agents and Standardize Human–Machine Teams (HMT),2020,"This work presents a multi-year study conducted at the University of Toledo, aimed at improving human–machine teaming (HMT) methods and technologies. With the advent of artificial intelligence (AI) in 21st-century machines, collaboration between humans and machines has become highly complicated for real-time applications. The penetration of intelligent and synthetic assistants (IA/SA) in virtually every field has opened up a path to the area of HMT. When it comes to crucial tasks such as patient treatment/care, industrial production, and defense, the use of non-standardized HMT technologies may pose a risk to human lives and cost billions of taxpayer dollars. A thorough literature survey revealed that there are not many established standards or benchmarks for HMT. In this paper, we propose a method to design an HMT based on a generalized architecture. This design includes the development of an intelligent collaborative system and the human team. Followed by the identification of processes and metrics to test and validate the proposed model, we present a novel human-in-the-loop (HIL) simulation method. The effectiveness of this method is demonstrated using two controlled HMT scenarios: Emergency care provider (ECP) training and patient treatment by an experienced medic. Both scenarios include humans processing visual data and performing actions that represent real-world applications while responding to a Voice-Based Synthetic Assistant (VBSA) as a collaborator that keeps track of actions. The impact of various machines, humans, and HMT parameters is presented from the perspective of performance, rules, roles, and operational limitations. The proposed HIL method was found to assist in standardization studies in the pursuit of HMT benchmarking for critical applications. Finally, we present guidelines for designing and benchmarking HMTs based on the case studies’ results analysis.",10.20944/preprints202011.0352.v1
cdbdfbe7aaab94793857356e88e78c84fd48a166,Revising the structure of Bayesian network classifiers in the presence of missing data,2018,,10.1016/j.ins.2018.02.011
da19f3f67ac1dadbd8151dd8ad6e14afe307e1c2,On the use of stochastic local search techniques to revise first-order logic theories from examples,2017,,10.1007/s10994-016-5595-3
0021f46bda27ea105d722d19690f5564f2b8869e,Deep Region and Multi-label Learning for Facial Action Unit Detection,2016,"Region learning (RL) and multi-label learning (ML) have recently attracted increasing attentions in the field of facial Action Unit (AU) detection. Knowing that AUs are active on sparse facial regions, RL aims to identify these regions for a better specificity. On the other hand, a strong statistical evidence of AU correlations suggests that ML is a natural way to model the detection task. In this paper, we propose Deep Region and Multi-label Learning (DRML), a unified deep network that simultaneously addresses these two problems. One crucial aspect in DRML is a novel region layer that uses feed-forward functions to induce important facial regions, forcing the learned weights to capture structural information of the face. Our region layer serves as an alternative design between locally connected layers (i.e., confined kernels to individual pixels) and conventional convolution layers (i.e., shared kernels across an entire image). Unlike previous studies that solve RL and ML alternately, DRML by construction addresses both problems, allowing the two seemingly irrelevant problems to interact more directly. The complete network is end-to-end trainable, and automatically learns representations robust to variations inherent within a local region. Experiments on BP4D and DISFA benchmarks show that DRML performs the highest average F1-score and AUC within and across datasets in comparison with alternative methods.",10.1109/CVPR.2016.369
fc14be9376dc35069f58cf2f777952694437f1ff,Facial Action Unit Detection by Adaptively Constraining Self-Attention and Causally Deconfounding Sample,2024,"Facial action unit (AU) detection remains a challenging task, due to the subtlety, dynamics, and diversity of AUs. Recently, the prevailing techniques of self-attention and causal inference have been introduced to AU detection. However, most existing methods directly learn self-attention guided by AU detection, or employ common patterns for all AUs during causal intervention. The former often captures irrelevant information in a global range, and the latter ignores the specific causal characteristic of each AU. In this paper, we propose a novel AU detection framework called AC2D by adaptively constraining self-attention weight distribution and causally deconfounding the sample confounder. Specifically, we explore the mechanism of self-attention weight distribution, in which the self-attention weight distribution of each AU is regarded as spatial distribution and is adaptively learned under the constraint of location-predefined attention and the guidance of AU detection. Moreover, we propose a causal intervention module for each AU, in which the bias caused by training samples and the interference from irrelevant AUs are both suppressed. Extensive experiments show that our method achieves competitive performance compared to state-of-the-art AU detection approaches on challenging benchmarks, including BP4D, DISFA, GFT, and BP4D+ in constrained scenarios and Aff-Wild2 in unconstrained scenarios. The code is available at https://github.com/ZhiwenShao/AC2D.",10.1007/s11263-024-02258-6
e2ec34461e5fdb4e015142996e5b2b7df2dcb25b,A root location training method for polynomial cellular neural networks that implements totalistic cellular automata,2013,"The Polynomial Cellular Neural Network (PCNN) is a powerful non-linear processor that is capable of classifying non-linearly separable data points with a single neuron. Despite the capabilities of this model, the determination of the synaptic weights is not a trivial task. In this paper we present the root location training method as an effective, straightforward and high-speed procedure. Such method obtains the synaptic weights of a PCNN that implements any totalistic cellular automata behavior, dispensing the usage of heuristic methods such as genetic algorithms or numerical approaches such as quadratic programming procedures.",10.1109/IJCNN.2013.6706950
67af5676c6069143eab98330aa6d28ceec359bb1,Using a system-on-a-chip implantable device to filter circulating infected cells in blood or lymph,2003,"This paper describes a system on a chip (SoC) that makes use of nanoscale cellular adhesion mechanisms in an integrated electronic microsystem to filter infected cells from blood or lymph. An example of a human immunodeficiency virus-specific SoC is explored in depth. Such systems work in vivo, and blood and lymph are filtered on a continuous basis. With the intelligence on the chip, captured cells can be identified and lyzed, expelled, or otherwise acted upon. These types of systems transfer the burden of research from traditional chemotherapy to bioengineering and system design.",10.1109/TNB.2003.810160
7bb5e4db37dabc756a94c7f264700dad032cd628,CNN-based difference-controlled adaptive non-linear image filters,1998,"In this paper, we develop a common cellular neural network framework for various adaptive non-linear filters based on robust statistic and geometry-driven diffusion paradigms. The base models of both approaches are defined as difference-controlled non-linear CNN templates, while the self-adjusting property is ensured by simple analogic (analog and logic) CNN algorithms. Two adaptive strategies are shown for the order statistic class. When applied to the images distorted by impulse noise both give more visually pleasing results with lower-frequency weighted mean square error than the median base model. Generalizing a variational approach we derive the constrained anisotropic diffusion, where the output of the geometry-driven diffusion model is forced to stay close to a pre-defined morphological constraint. We propose a coarse-grid CNN approach that is capable of calculating an acceptable noise-level estimate (proportional to the variance of the Gaussian noise) and controlling the fine-grid anisotropic diffusion models. A combined geometrical-statistical approach has also been developed for filtering both the impulse and additive Gaussian noise while preserving the image structure. We briefly discuss how these methods can be embedded into a more complex algorithm performing edge detection and image segmentation. The design strategies are analysed primarily from VLSI implementation point of view; therefore all non-linear cell interactions of the CNN architecture are reduced to two fundamental non-linearities, to a sigmoid type and a radial basis function. The proposed non-linear characteristics can be approximated with simple piecewise-linear functions of the voltage difference of neighbouring cells. The simplification makes it possible to convert all space-invariant non-linear templates of this study to a standard instruction set of the CNN Universal Machine, where each instruction is coded by at most a dozen analog numbers. Examples and simulation results are given throughout the text using various intensity images.",10.1002/(SICI)1097-007X(199807/08)26:4%3C375::AID-CTA19%3E3.0.CO;2-%23
a2592d944a85df0ef87726cf2cabeb022a4aa1fb,Graph-Based Facial Affect Analysis: A Review,2021,"As one of the most important affective signals, facial affect analysis (FAA) is essential for developing human-computer interaction systems. Early methods focus on extracting appearance and geometry features associated with human affects while ignoring the latent semantic information among individual facial changes, leading to limited performance and generalization. Recent work attempts to establish a graph-based representation to model these semantic relationships and develop frameworks to leverage them for various FAA tasks. This article provides a comprehensive review of graph-based FAA, including the evolution of algorithms and their applications. First, the FAA background knowledge is introduced, especially on the role of the graph. We then discuss approaches widely used for graph-based affective representation in literature and show a trend towards graph construction. For the relational reasoning in graph-based FAA, existing studies are categorized according to their non-deep or deep learning methods, emphasizing the latest graph neural networks. Performance comparisons of the state-of-the-art graph-based FAA methods are also summarized. Finally, we discuss the challenges and potential directions. As far as we know, this is the first survey of graph-based FAA methods. Our findings can serve as a reference for future research in this field.",10.1109/TAFFC.2022.3215918
45bfb9de95046602b6c7512e827e0a6e206d6abb,FACS-Based Graph Features for Real-Time Micro-Expression Recognition,2020,"Several studies on micro-expression recognition have contributed mainly to accuracy improvement. However, the computational complexity receives lesser attention comparatively and therefore increases the cost of micro-expression recognition for real-time application. In addition, majority of the existing approaches required at least two frames (i.e., onset and apex frames) to compute features of every sample. This paper puts forward new facial graph features based on 68-point landmarks using Facial Action Coding System (FACS). The proposed feature extraction technique (FACS-based graph features) utilizes facial landmark points to compute graph for different Action Units (AUs), where the measured distance and gradient of every segment within an AU graph is presented as feature. Moreover, the proposed technique processes ME recognition based on single input frame sample. Results indicate that the proposed FACS-baed graph features achieve up to 87.33% of recognition accuracy with F1-score of 0.87 using leave one subject out cross-validation on SAMM datasets. Besides, the proposed technique computes features at the speed of 2 ms per sample on Xeon Processor E5-2650 machine.",10.3390/jimaging6120130
b0cfc5a2e4634344b0c6eb3beb1f9e6c66d14960,Facial Expression Recognition Using Spatial-Temporal Semantic Graph Network,2020,"Motions of facial components convey significant information of facial expressions. Although remarkable advancement has been made, the dynamic of facial topology has not been fully exploited. In this paper, a novel facial expression recognition (FER) algorithm called Spatial Temporal Semantic Graph Network (STSGN) is proposed to automatically learn spatial and temporal patterns through end-to-end feature learning from facial topology structure. The proposed algorithm not only has greater discriminative power to capture the dynamic patterns of facial expression and stronger generalization capability to handle different variations but also higher interpretability. Experimental evaluation on two popular datasets, CK+ and Oulu-CASIA, shows that our algorithm achieves more competitive results than other state-of-the-art methods.",10.1109/ICIP40778.2020.9191181
51ef0f6b6080ae713c196408888f1287fd1a1950,A computer vision based image processing system for depression detection among students for counseling,2019,"Psychological problems in college students like depression, pessimism, eccentricity, anxiety etc. are caused principally due to the neglect of continuous monitoring of students’ psychological well-being. Identification of depression at college level is desirable so that it can be controlled by giving better counseling at the starting stage itself. The disturbed mental state of a student suffering from depression would be clearly evident in the student’s facial expressions.Identification of depression in large group of college students becomes a tedious task for an individual. But advances in the Image-Processing field have led to the development of effective systems, which prove capable of detecting emotions from facial images, in a much simpler way. Thus, we need an automated system that captures facial images of students and analyze them, for effective detection of depression. In the proposed system, an attempt is being made to make use of the Image processing techniques, to study the frontal face features of college students and predict depression. This automated system will be trained with facial features of positive and negative facial emotions. To predict depression, a video of the student is captured, from which the face of the student is extracted. Then using Gabor filters, the facial features are extracted. Classification of these facial features is done using SVM classifier. The level of depression is identified by calculating the amount of negative emotions present in the entire video. Based on the level of depression, notification is send to the class advisor, department counselor or university counselor, indicating the student’s disturbed mental state. The present system works with an accuracy of 64.38%. The paper concludes with the description of an extended architecture for depression detection as future work.",10.11591/IJEECS.V14.I1.PP503-512
aae227322ac72cdf982fa8b7e3249e9ea6b05910,Facial Expression Region Segmentation Based Approach to Emotion Recognition Using 2D Gabor Filter and Multiclass Support Vector Machine,2018,"Facial expressions have been studied extensively for the analysis of human sentiment properly. A human emotion recognition system through recognizing human facial expression is proposed in this paper. After preprocessing, segmentation of the facial expression regions is done in a unique yet effective and easy way to segment the left eye, right eye, nose, mouth properly from the facial region. 2D Gabor filter is used for the extraction of features from the expression regions. For reducing the dimension of the extracted features, downsampling and Principal Component Analysis (PCA) is used. For carrying out the classification task multiclass Support Vector Machine (SVM) is used for its ability to handle complex problems in high dimensional spaces. Three publicly available facial expression dataset was used to evaluate the performance of the proposed system. Finally, performance on these datasets by the proposed method is compared to previously attained performance by different methods which indicate that the proposed method attains state-of-the-art performance.",10.1109/ICCITECHN.2018.8631922
9e2d2d0392d8a2ae92509ecb687902929c6c22e9,Driver’s Facial Expression Recognition in Real-Time for Safe Driving,2018,"In recent years, researchers of deep neural networks (DNNs)-based facial expression recognition (FER) have reported results showing that these approaches overcome the limitations of conventional machine learning-based FER approaches. However, as DNN-based FER approaches require an excessive amount of memory and incur high processing costs, their application in various fields is very limited and depends on the hardware specifications. In this paper, we propose a fast FER algorithm for monitoring a driver’s emotions that is capable of operating in low specification devices installed in vehicles. For this purpose, a hierarchical weighted random forest (WRF) classifier that is trained based on the similarity of sample data, in order to improve its accuracy, is employed. In the first step, facial landmarks are detected from input images and geometric features are extracted, considering the spatial position between landmarks. These feature vectors are then implemented in the proposed hierarchical WRF classifier to classify facial expressions. Our method was evaluated experimentally using three databases, extended Cohn-Kanade database (CK+), MMI and the Keimyung University Facial Expression of Drivers (KMU-FED) database, and its performance was compared with that of state-of-the-art methods. The results show that our proposed method yields a performance similar to that of deep learning FER methods as 92.6% for CK+ and 76.7% for MMI, with a significantly reduced processing cost approximately 3731 times less than that of the DNN method. These results confirm that the proposed method is optimized for real-time embedded applications having limited computing resources.",10.3390/s18124270
3fe25fa3092abb2e0f8885ede7777709174c57fa,Recognition of student emotion based on matrix-1 median fisher's face and backpropagation algorithm,2017,"Emotions drive learning success because they hold a willingness to process information. However, it is a challenge for understanding the emotions of student in the real class. In this study, we proposed recognition of student emotion using matrix-1 median fisher's face and backpropagation algorithm. The computation of backpropagation is influenced by neuron architecture which is can be handled by feature reducing, such as fisher face. However the number of fisher's vector due to the number of class. In order to map the lower dimensional feature space than fisher's face vector, we proposed matrix-1 median of the fisher's face. In this proposed method, after face is detected, LDA on PCA space is employed for getting the fisher's face. Then fisher face is transformed into fisher's median. The backpropagation algorithm is trained using this feature to distinguish student emotions. The performances of proposed algorithm are evaluated on the UM's learning video using accuracy and iteration consuming. Our proposed method reach accuration of overly interest, interest and bored up to 0.83, 0.91, and 1, whereas original fisher face reach accuration of overly interest, interest and bored up to 0.83, 0.91, and 0.91. Combination of backpropagation and matrix-1 median fisher face need 9 iteration for training. Whereas the combination of backpropagation and fisher Face need 11 iteration. Experiment result shows that our proposed method outperform than the existing method.",10.1109/ICELTICS.2017.8253255
1599718bf756a0fb7157277b93f21cfcad04e383,Facial expression recognition using geometric features,2016,"Recognition of emotions by analyzing the facial expressions has been an important research area in last decade. Existing studies in the literature mostly seek for a classification model that is linking the emotions with the facial expressions that are defined by the basic components of face (such as eyes, mouth, and ear). In this regard, Artificial Neural Networks (ANN) has been a widely employed approach for classification purposes. Being different than the existing approaches, we searched for the points/landmarks that can contribute to accuracy of ANN model. With this purpose we have started our search with the 153 possible distances among 18 critical candidate points/landmarks. The 16 of these distances having significant contribution to the model accuracy was selected by using the correlation-based feature subset selection (CFS) method. The results indicated that our ANN model has 91.2% correct classification rate.",10.1109/IWSSIP.2016.7502700
0022160fefba1baa000f9d5c20a9a4caea9474ef,USB receiver/transmitter for FPGA implementation,2012,"The paper is a study of USB standard based on authors own concept of specialized digital architecture providing USB communication. Presented module connects peripheral device with a computer via USB cable. In accordance with functionality, the design was partitioned to three units. UTMI block deals with USB cable, time frame synchronization and serial transmission. PIE block, divided to two parts, is responsible for packet construction/extraction and byte oriented communication with the peripheral system. Each module is controlled by the dedicated Finite State Machine. The architecture was implemented in VHDL, verified and synthesized. Design complexity arises questions about feasibility of USB interface application in specialized devices and low-volume segments of electronic devices, especially when compared to traditional RS232/UART interfaces.",10.1109/ICSES.2012.6382226
334ee140b83834d9f66583b4a69ee826686e7dfe,Design and Implementation of a Strong Arbiter PUF for the Security of FPGA,2023,"Physical unclonable function (PUF) is a promising process for safe key generation on ICs and IoT devices for authentication and cryptographic applications. Each Physical Unclonable Function (PUF) stands apart due to tiny variations introduced during manufacturing, like minuscule gate delays. PUF provides responses when input challenges are given. One of the core advantages of Arbiter PUFs lies in their resilience against conventional attack methods like reverse engineering, side-channel attacks, and brute-force attempts. As the PUF’s distinctiveness and randomness stem from its physical structure, adversaries face formidable obstacles when attempting to simulate or forecast its behavior. Recently, researchers proved that some arbiter PUFs can be susceptible to modeling and Machine Learning (ML) Attacks. Researchers continue to explore countermeasures and improvements to enhance the security of Arbiter PUFs and other PUF variants. In our paper, utilizing a computer-aided (CAD) tool, a strong arbiter PUF (APUF) is designed and implemented on a Field Programmable Gate Array (FPGA). The challenges and responses of APUF are 64 bits, which makes it unpredictable and difficult to break using machine learning and deep learning algorithms. Our implemented APUF’s performance was evaluated based on inter and intra-hamming distance of responses. The arbiter PUF achieved 0.031% intra-hamming distance (HDintra) and 30% inter-hamming distance (HDinter), which are closely ideal.",10.1109/UEMCON59035.2023.10315970
1cea4d24bfeffc73a5d5069d7349edc0a84c6e2a,A review on bus protocols and conversion/translator between different protocols,2019,"Abstract A computer consists of different components/modules and different communication protocols are used among them. These protocols are needed to achieve the interoperability between two devices/modules. Different protocols cannot communicate directly, there is a need for conversion. A protocol converters block is needed. These compact protocol converters create low power, low voltage connection as well as smaller chip area, making it easy to add multiple devices or components. This paper give a brief description of some commonly used bus protocols such as Serial Peripheral Interface (SPI), Inter-Integrated Circuit (I2C), Advanced Microcontroller Bus Architecture (AMBA), Universal Serial Bus (USB), Peripheral Component Interconnect Express (PCIe) and Ethernet. A review of some protocol converter/translator are done with a view to explore their usability.",10.1080/02522667.2020.1715570
cd23c4a94aa04bf6134cb8b9c279298251693d26,USB Transceiver With a Serial Interface Engine and FIFO Queue for Efficient FPGA-to-FPGA Communication,2020,"This paper presents a universal serial bus (USB) transceiver with a serial interface engine (SIE) and an asynchronous first-in first-out (FIFO) queue for packet transformation and data transmission in field-programmable gate array (FPGA)-to-FPGA communication. The SIE block receives the data to be transmitted from the central processing unit of a PC and transfers those data to the universal transceiver macrocell interface, which handles data serialization and deserialization, bit stuffing, clock recovery, and clock synchronization. An asynchronous FIFO queue of 2 kilobits is designed to guarantee correct communication between two FPGA development boards. A parallel-in serial-out block converts parallel input data into serial data. A product identification (PID) check block determines whether the serial data are in the USB packet format. The cyclic redundancy check (CRC) checksums, namely CRC5 and CRC16, are presented with data check statements. After passing through the NRZI decoder, bit-unstuffing, PID check, and CRC16 blocks, the received serial data are converted into parallel output data by using a serial-in parallel-out block. The FPGA-to-FPGA communication design operates correctly. An application-specific integrated circuit (ASIC) of the USB transceiver is implemented using TSMC 0.18- $\mu \text{m}$ CMOS technology. The gate counts, power consumption, operating frequency, and chip area of the ASIC are 14,547, 2.6742 mW, 50 MHz, and $0.7\times0.67$ mm2, respectively, at a supply voltage of 1.8 V and total pin number of 38.",10.1109/ACCESS.2020.2986510
cd6402234500f37286e52811dbdbfb87b557a437,Large Scale Foundation Models for Intelligent Manufacturing Applications: A Survey,2023,"Although the applications of artificial intelligence especially deep learning had greatly improved various aspects of intelligent manufacturing, they still face challenges for wide employment due to the poor generalization ability, difficulties to establish high-quality training datasets, and unsatisfactory performance of deep learning methods. The emergence of large scale foundational models(LSFMs) had triggered a wave in the field of artificial intelligence, shifting deep learning models from single-task, single-modal, limited data patterns to a paradigm encompassing diverse tasks, multimodal, and pre-training on massive datasets. Although LSFMs had demonstrated powerful generalization capabilities, automatic high-quality training dataset generation and superior performance across various domains, applications of LSFMs on intelligent manufacturing were still in their nascent stage. A systematic overview of this topic was lacking, especially regarding which challenges of deep learning can be addressed by LSFMs and how these challenges can be systematically tackled. To fill this gap, this paper systematically expounded current statue of LSFMs and their advantages in the context of intelligent manufacturing. and compared comprehensively with the challenges faced by current deep learning models in various intelligent manufacturing applications. We also outlined the roadmaps for utilizing LSFMs to address these challenges. Finally, case studies of applications of LSFMs in real-world intelligent manufacturing scenarios were presented to illustrate how LSFMs could help industries, improve their efficiency.",10.48550/arXiv.2312.06718
4560aaf6aa9d59dddbc4939c6573b1385761f567,CluSem: Accurate clustering-based ensemble method to predict motor imagery tasks from multi-channel EEG data,2021,"Background The classification of motor imagery electroencephalogram (MI-EEG) is a pivotal task in the biosignal classification process in the brain-computer interface (BCI) applications. Currently, this bio-engineering-based technology is being employed by researchers in various fields to develop cuttingedge applications. The classification of real-time MI-EEG signals is the most challenging task in these applications. The prediction performance of the existing classification methods is still limited due to the high dimensionality and dynamic behaviors of the real-time EEG data. Proposed Method To enhance the classification performance of real-time BCI applications, this paper presents a new clustering-based ensemble technique called CluSem to mitigate this problem. We also develop a new brain game called CluGame using this method to evaluate the classification performance of real-time motor imagery movements. In this game, real-time EEG signal classification and prediction tabulation through animated balls are controlled via threads. By playing this game, users can control the movements of the balls via the brain signals of motor imagery movements without using any traditional input devices. Results Our results demonstrate that CluSem is able to improve the classification accuracy between 5% and 15% compared to the existing methods on our collected as well as the publicly available EEG datasets. The source codes used to implement CluSem and CluGame are publicly available at https://github.com/MdOchiuddinMiah/MI-BCI_ML.",10.1101/2021.09.05.458710
1222c3eec67aa8d09799c60740360cae9d6026cb,Prediction of Motor Imagery Tasks from Multi-Channel EEG Data for Brain-Computer Interface Applications,2020,"The classification of motor imagery electroencephalogram (MI-EEG) is a pivotal part of the biosignal classification in the brain-computer interface (BCI) applications. Currently, this bio-engineering based technology is being employed by researchers in various fields to develop cutting edge applications. The classification of real-time MI-EEG signal is the core computing and challenging task in these applications. It is well-known that the existing classification methods are not so accurate due to the high dimensionality and dynamic behaviors of the real-time EEG data. To improve the classification performance of real-time BCI applications, this paper presents a clustering-based ensemble technique and a developed brain game that distinguishes different human thoughts. At first, we have gathered the brain signals, extracted and selected informative features from these signals to generate training and testing sets. After that, we have constructed several classifiers using Artificial Neural Network (ANN), Support Vector Machine (SVM), naïve Bayes, Decision Tree (DT), Random Forest, Bagging, AdaBoost and compared the performance of these existing approaches with suggested clustering-based ensemble technique. On average, the proposed ensemble technique improved the classification accuracy of roughly 5 to 15% compared to the existing methods. Finally, we have developed the targeted brain game employing our suggested ensemble technique. In this game, real-time EEG signal classification and prediction tabulation through animated ball are controlled via threads. By playing this game, users can control the movements of the balls via the brain signals of motor imagery movements without using any traditional input devices. All relevant codes are available via open repository at: https://github.com/mrzResearchArena/MI-EEG.",10.1101/2020.04.08.032201
23396e5737f0da128c9a011d28ac88735db1357e,An optimized design of seizure detection system using joint feature extraction of multichannel EEG signals,2019,"The detection of seizure onset and events using electroencephalogram (EEG) signals are important tasks in epilepsy research. The literature available on seizure detection has discussed the implementation of advanced signal processing algorithms using tools accessed over the cloud. However, seizure monitoring application needs near sensor processing due to privacy and latency issues. In this paper, a real time seizure detection system has been implemented using an embedded system. The proposed system is based on ensemble empirical mode decomposition (EEMD) and tunable-Q wavelet transform (TQWT) algorithms. The analysis and classification of non-stationary EEG signals require the wavelet transform with high Q-factor. However, direct use of TQWT increases the computational complexity of feature extraction from multivariate EEG signals. In this paper, the first step is to process the signal by using EEMD to obtain 8 intrinsic mode functions (IMFs). The Kraskov (KraEn), sample (SampEn), and permutation (PermEn) entropy features of IMFs are extracted and based on optimum values, and 4 IMFs are decomposed using TQWT. Secondly, centered correntropy (CenCorrEn) features of the 1st and 16th sub-band of TQWT have been used as classifier inputs. The performance of multilayer perceptron neural networks (MLPNN), least squares support vector machine (LSSVM), and random forest (RF) classifiers has been tested on the multichannel EEG data recorded from a local hospital. The RF classifier has produced the highest accuracy of 96.2% in classifying the signals. The proposed scheme has been employed in developing an embedded seizure detection system to assist neurologists in making seizure diagnostic decisions.",10.7555/JBR.33.20190019
186ed3abeb8e97f529bfcad2e7502a395936d364,"Performance Evaluation of Discrete Wavelet Transform, and Wavelet Packet Decomposition for Automated Focal and Generalized Epileptic Seizure Detection",2019,"In the past decades, wavelet transforms are widely employed for characterizing the electroencephalogram (EEG) signals for automatic diagnosis of epileptic seizure. But few vital issues like the classification of epileptic seizure types from normal EEG signals has not yet been benefited with wavelet transforms. Hence, in this paper, the two major types of wavelet transform, namely discrete wavelet transform (DWT) and wavelet packet decomposition (WPD) are employed for the automatic diagnosis of the epileptic seizure and its types. The publicly available KITS EEG database consisting of three groups namely, normal, focal epilepsy and generalized epilepsy are utilized in this work. Four experimental cases namely (i) normal-generalized epilepsy, (ii) normal-focal epilepsy, (iii) normal-focal-generalized and (iv) normal-epilepsy are used to investigate the proposed approach. Further, this paper attempts to identify the best wavelet function from the commonly used seven wavelet families and the level of decomposition required to analyse the EEG signals. The nine statistical features are extracted from the wavelet coefficients and fed into the support vector machine (SVM) classifier. From the experimental result it was found out that the DWT with rbio1.1 attained the highest classification accuracy for all the experimental cases.",10.1080/03772063.2019.1568206
002285ade41b9e1313f9c914b99e68b62fab7ff1,Scalable Recollections for Continual Lifelong Learning,2017,"Given the recent success of Deep Learning applied to a variety of single tasks, it is natural to consider more human-realistic settings. Perhaps the most difficult of these settings is that of continual lifelong learning, where the model must learn online over a continuous stream of non-stationary data. A successful continual lifelong learning system must have three key capabilities: it must learn and adapt over time, it must not forget what it has learned, and it must be efficient in both training time and memory. Recent techniques have focused their efforts primarily on the first two capabilities while questions of efficiency remain largely unexplored. In this paper, we consider the problem of efficient and effective storage of experiences over very large time-frames. In particular we consider the case where typical experiences are O(n) bits and memories are limited to O(k) bits for k",10.1609/aaai.v33i01.33011352
002359d4160e310b20637e3450da4757ec22dc45,Restarts Can Help in the On-Line Minimization of the Maximum Delivery Time on a Single Machine,2000,,10.1007/3-540-45253-2_39
59ad2aecfde8512480b5d7340a368dd834c64e0f,Online NDP-constraint scheduling of jobs with delivery times or weights,2022,,10.1007/s11590-022-01889-3
6d32bb42a9d27ecb4dfdde5a0e1c82cfc1c65bf7,Online Scheduling on a Parallel Batch Machine with Delivery Times and Limited Restarts,2021,,10.1007/s40305-021-00356-7
0e11db239f53293e3834828eaae33894b83bb880,Single-machine online scheduling of jobs with non-delayed processing constraint,2021,,10.1007/s10878-021-00722-4
1b105488d1908164daa92b4f380e345f053b5988,A best possible online algorithm for parallel batch scheduling with delivery times and limited restart,2020,,10.1007/s11590-020-01618-8
4414b3bae922531cdb1086fbd25697cbd016da82,An improved semi-online algorithm for scheduling on a single machine with unexpected breakdown,2020,,10.1007/s10878-020-00572-6
d8dcfb5718eaf1d9931998f4e2bb04c0ff8ca86d,Improved Approximation Algorithm for Scheduling on a Serial Batch Machine with Split-Allowed Delivery,2018,,10.1007/s40305-018-0210-x
5c5143c613d0edd845aba18132b037386814f388,Online integrated production–distribution scheduling problems without preemption,2016,,10.1007/s10878-015-9841-6
007b885f74eb1783a86e9ac0670c1035c08a199f,Bilateral Privacy Protection Scheme Based on Adaptive Location Generalization and Grouping Aggregation in Mobile Crowdsourcing,2024,"In mobile crowdsourcing (MCS), the task information released by task publishers and the sensed data submitted by workers may expose their privacy, while the rapid growth of MCS imposes increasing data processing pressure on cloud platforms and mobile devices. To address these challenges, a bilateral privacy protection scheme based on adaptive location generalization and grouping aggregation is presented in this article. The scheme uses federated learning as a framework and utilizes edge computing to reduce the data processing burden on cloud platforms and mobile devices. This article proposes the adaptive location generalization algorithm (KM-ALG) and a real task location release mechanism based on the RSA algorithm to protect the task location privacy of the task publisher. For workers’ privacy protection, the lightweight multiple perturbation algorithm based on localized differential privacy (LDP-MP) proposed in this article is used to protect workers’ data privacy. Aiming at the problem of data quality loss caused by perturbation, a perturbation elimination mechanism based on homomorphic encryption technology is proposed. In order to prevent workers’ sensed data from leaking location information, a grouping aggregation mechanism is used to destroy the correspondence between workers and submitted data, thereby protecting workers’ location privacy. In addition, a task allocation scheme adapted to task location privacy protection is also proposed. Finally, the effectiveness of the proposed algorithm is verified through experiments on multiple real data sets.",10.1109/JIOT.2024.3358799
3170c2174a34d19cae9ba9002c10993bca52813a,UAV-Assisted Cluster-Based Task Allocation for Mobile Crowdsensing in a Space–Air–Ground–Sea Integrated Network,2023,"Mobile crowdsensing (MCS), which is a grassroots sensing paradigm that utilizes the idea of crowdsourcing, has attracted the attention of academics. More and more researchers have devoted themselves to adopting MCS in space–air–ground–sea integrated networks (SAGSINs). Given the dynamics of the environmental conditions in SAGSINs and the uncertainty of the sensing capabilities of mobile people, the quality and coverage of the sensed data change periodically. To address this issue, we propose a novel UAV-assisted cluster-based task allocation (UCTA) algorithm for MCS in SAGSINs in a two-stage process. We first introduce the edge nodes and establish a three-layer hierarchical system with UAV-assistance, called “Platform–Edge Cluster–Participants”. Moreover, an edge-aided attribute-based cluster algorithm is designed, aiming at organizing tasks into clusters, which significantly diminishes both the communication overhead and computational complexity while enhancing the efficiency of task allocation. Subsequently, a greedy selection algorithm is proposed to select the final combination that performs the sensing task in each cluster. Extensive simulations are conducted comparing the developed algorithm with the other three benchmark algorithms, and the experimental results unequivocally endorse the superiority of our proposed UCTA algorithm.",10.3390/s24010208
1aa968934976d9be4ed2962aa3cd943e0bfc8ed4,Air-Ground Spatial Crowdsourcing with UAV Carriers by Geometric Graph Convolutional Multi-Agent Deep Reinforcement Learning,2023,"Spatial Crowdsourcing (SC) has been proved as an effective paradigm for data acquisition in urban environments. Apart from using human participants, with the rapid development of unmanned vehicles (UVs) technologies, unmanned aerial or ground vehicles (UAVs, UGVs) are equipped with various high-precision sensors, enabling them to become new types of data collectors. However, UGVs’ operational range is constrained by the road network, and UAVs are limited by power supply, it is thus natural to use UGVs and UAVs together as a coalition, and more precisely, UGVs behave as the UAV carriers for range extensions to achieve complicated air-ground SC tasks. In this paper, we propose a novel communication-based multi-agent deep reinforcement learning method called ""GARL"", which consists of a multi-center attention-based graph convolutional network (GCN) to accurately extract UGV specific features from UGV stop network called ""MC-GCN"", and a novel GNN-based communication mechanism called ""E-Comm"" to make the cooperation among UGVs adaptive to constant changing of geometric shapes formed by UGVs. Extensive simulation results on two campuses of KAIST and UCLA campuses show that GARL consistently outperforms eight other baselines in terms of overall efficiency.",10.1109/ICDE55515.2023.00140
b4293f17cfdb1ad1f24aa687794f52001516dc94,Space-Air-Ground Integrated Mobile Crowdsensing for Partially Observable Data Collection by Multi-Scale Convolutional Graph Reinforcement Learning,2022,"Mobile crowdsensing (MCS) is attracting considerable attention in the past few years as a new paradigm for large-scale information sensing. Unmanned aerial vehicles (UAVs) have played a significant role in MCS tasks and served as crucial nodes in the newly-proposed space-air-ground integrated network (SAGIN). In this paper, we incorporate SAGIN into MCS task and present a Space-Air-Ground integrated Mobile CrowdSensing (SAG-MCS) problem. Based on multi-source observations from embedded sensors and satellites, an aerial UAV swarm is required to carry out energy-efficient data collection and recharging tasks. Up to date, few studies have explored such multi-task MCS problem with the cooperation of UAV swarm and satellites. To address this multi-agent problem, we propose a novel deep reinforcement learning (DRL) based method called Multi-Scale Soft Deep Recurrent Graph Network (ms-SDRGN). Our ms-SDRGN approach incorporates a multi-scale convolutional encoder to process multi-source raw observations for better feature exploitation. We also use a graph attention mechanism to model inter-UAV communications and aggregate extra neighboring information, and utilize a gated recurrent unit for long-term performance. In addition, a stochastic policy can be learned through a maximum-entropy method with an adjustable temperature parameter. Specifically, we design a heuristic reward function to encourage the agents to achieve global cooperation under partial observability. We train the model to convergence and conduct a series of case studies. Evaluation results show statistical significance and that ms-SDRGN outperforms three state-of-the-art DRL baselines in SAG-MCS. Compared with the best-performing baseline, ms-SDRGN improves 29.0% reward and 3.8% CFE score. We also investigate the scalability and robustness of ms-SDRGN towards DRL environments with diverse observation scales or demanding communication conditions.",10.3390/e24050638
00240c74a2f1b812b7faf98556fffaa6ae2221da,Applying adaptive hypermedia technologies to a learning tool,2005,"In this paper, we present an adaptive learning tool that can help students learn the concepts of automata theory. It is an extension of an existing software called NEO/spl I.bar/VAM (Lee and Park, 2005) which was originally developed for helping students understand the concepts of automata. The extensions were made in order to provide students who have different backgrounds with appropriate contents by utilizing AHA! (De Bra et al., 2003) which is an open source adaptive hypermedia system. With the learning tool, students can experiment constructing a simple computer and test transition functions based on user's knowledge level. Different types of adaptations are employed in order to provide appropriate contents to users.",10.1109/ICALT.2005.67
00242a60e127c5d66f23d039428caaed57a48cd2,Multiple virtual machine live migration in federated cloud systems,2014,"The idea of computing utility incarnated by the cloud paradigm is gaining a lot of success, both for entertainment and business applications. The consequent increasing demand of computing, storage and communication resources within data centers is fostering new forms of infrastructure sharing such as cloud federations, which can take advantage of virtualization technologies and, in particular, of multiple virtual machine live migration techniques. Such a scenario requires a quantitative characterization of the performance of the inter-data center network infrastructure underlying the cloud federation. In this paper we propose an analytical model useful to dimension network capacity in order to achieve some given performance level in a federate cloud, assuming some simple multiple virtual machine live migration strategies.",10.1109/INFCOMW.2014.6849163
078241442033645965d810576d7f93f9c3a15b54,Virtual machines pre-copy live migration cost modeling and prediction: a survey,2021,"Live migration is an essential feature in virtual infrastructure and cloud computing datacenters. Using live migration, virtual machines can be online migrated from a physical machine to another with negligible service interruption. Load balance, power saving, dynamic resource allocation, and high availability algorithms in virtual data-centers and cloud computing environments are dependent on live migration. Live migration process has six phases that result in live migration cost. Several papers analyze and model live migration costs for different hypervisors, different kinds of workloads and different models of analysis. In addition, there are also many other papers that provide prediction techniques for live migration costs. It is a challenge for the reader to organize, classify, and compare live migration overhead research papers due to the broad focus of the papers in this domain. In this survey paper, we classify, analyze, and compare different papers that cover pre-copy live migration cost analysis and prediction from different angels to show the contributions and the drawbacks of each study. Papers classification helps the readers to get different studies details about a specific live migration cost parameter. The classification of the paper considers the papers’ research focus, methodology, the hypervisors, and the cost parameters. Papers analysis helps the readers to know which model can be used for which hypervisor and to know the techniques used for live migration cost analysis and prediction. Papers comparison shows the contributions, drawbacks, and the modeling differences by each paper in a table format that simplifies the comparison. Virtualized Data-center and cloud computing clusters admins can also make use of this paper to know which live migration cost prediction model can fit for their environments.",10.1007/s10619-021-07387-2
d00a5de089cc3ef11420cc63f43cc8f29c67e51e,Live Migration Timing Optimization Integration with VMware Environments,2021,"Live migration is an essential feature in virtual infrastructure and cloud computing datacenters. Using live migration, virtual machines can be online migrated from a physical machine to another with negligible service interruption. Load balance, power saving, dynamic resource allocation, and high availability algorithms in virtual data-centers and cloud computing environments are dependent on live migration. Live migration process has six phases that result in live migration overhead. Currently, virtual datacenters admins run live migrations without an idea about the migration cost prediction and without recommendations about the optimal timing for initiating a VM live migration especially for large memory VMs or for concurrently multiple VMs migration. Without cost prediction and timing optimization, live migration might face longer duration, network bottlenecks and migration failure in some cases. The previously proposed timing optimization approach is based on using machine learning for live migration cost prediction and the network utilization predict ion of the cluster. In this paper, we show how to integrate our machine learning based timing optimization algorithm with VMware vSphere. This integration deployment proves the practicality of the proposed algorithm by presenting the building blocks of the tools and backend scripts that should run to implement this timing optimization feature. The paper shows also how the IT admins can make use of this novel cost prediction and timing optimization option as an integrated plug-in within VMware vSphere UI to be notified with the optimal timing recommendation in case of a having live migration request.",10.1007/978-3-030-72369-9_6
c0cf85f769992fd6fe0c9d76c2869b2dc0f06321,Design and Implementation of SCC System based on Cloud Architecture,2019,"This paper addresses a service-oriented, customized on-demand, integrated cloud architecture for the satellite control center system. The cloud architecture is a three-layer hierarchy including service/business layer, platform layer and hardware layer. Based on the cloud architecture, the SCC system is designed to integrate computing, storage, and networking resources to form a powerful converged infrastructure platform. The SCC software are implemented as eleven functional services: data display software, data processing software, data storage and management software, database software, orbit calculation and analysis software, control calculation and analysis software, duplex management software, command operation software, monitor and control software, interface computer software and time service software. Details of the overall system architecture and the integration of the above software services are included in the paper. The paper concludes that design and implementation of efficient, reliable, scalable and flexible ground control system is an attractive but challenging area, and there is still work to be done.",10.1145/3358331.3358369
b6471b18d19a77a75598b2a3c4a6bcbb46c171bd,Efficient Online Virtual Machines Migration for Alert-Based Disaster Resilience,2019,"Several recent weather-based disasters had very negative impacts on cloud networks, causing Data Center (DC) shutdown, consequent data-loss and intolerable downtime of cloud services. This has put the reactive disaster-resilient design of cloud networks on top the agenda of several cloud DC operators. DC operators are investigating approaches to avoid downtime of cloud services in case a DC is affected by a disaster. Thanks to virtualization most cloud services run on Virtual Machines (VMs) hosted by DCs, so it is possible to keep these services alive if the VMs are evacuated (namely, migrated) before the disaster from a DC affected by the disaster to a DC in a safe location, in an online technique. This technique is known as online “VM migration”, which results without or with a minimal service downtime. In this paper, we present an Integer Linear Programming (ILP) model for efficient online VMs migration in case of an alerted disaster (e.g., most weather-based disasters, as hurricanes) such as to avoid service downtime. The ILP performs scheduling and assigns route and bandwidth to the migration of VMs towards a safe DC within an alert time, with the objective of maximizing the number of VMs migrated and minimizing service downtime, network resource occupation and migration duration. We present a comparative analysis of offline and online migration strategies such as to quantify the trade-off between downtime, network resource utilization and migration duration. Moreover, we investigate the impact of the memory dirtying rate on the online migration process, i.e., the number of VMs evacuated and network resource occupation.",10.1109/DRCN.2019.8713760
92785f08d2fd9d18fddaa431023f0d7a4756cd66,Enhanced Cost Analysis of Multiple Virtual Machines Live Migration in VMware Environments,2018,"Live migration is an important feature in modern software-defined datacenters and cloud computing environments. Dynamic resource management, load balance, power saving and fault tolerance are all dependent on the live migration feature. Despite the importance of live migration, the cost of live migration cannot be ignored and may result in service availability degradation. Live migration cost includes the migration time, downtime, CPU overhead, network and power consumption. There are many research articles that discuss the problem of live migration cost with different scopes like analyzing the cost and relate it to the parameters that control it, proposing new migration algorithms that minimize the cost and also predicting the migration cost. For the best of our knowledge, most of the papers that discuss the migration cost problem focus on open source hypervisors. For the research articles focus on VMware environments, none of the published articles proposed migration time, network overhead and power consumption modeling for single and multiple VMs live migration. In this paper, we propose empirical models for the live migration time, network overhead and power consumption for single and multiple VMs migration. The proposed models are obtained using a VMware based testbed.",10.1109/SC2.2018.00010
deccdc6a2b53bfdd7d0817fc2651d7cfcd6114fb,Multiple Virtual Machines Live Migration Scheduling Method Study on VMware vMotion,2018,"VM vMotion is the key feature of virtualization and it allows a working virtual machine moving to another physical host without disrupting its service. When administrators perform VM vMotion, they need to consider the vMotion sequence, resource allocation problem and then run vMotion one by one by monitoring the whole vMotion process. However, when the data center is larger and needs to be redeployed, it will become highly complicated and manual operation will be very inefficient. Therefore we present the automatically scheduling method of multiple virtual machines migration to reduce operating procedures and minimize the total migration time. Experimental results show that 50% of the total migration time can be reduced.",10.1109/CCOMS.2018.8463330
569c9dcd9c9b7d0797349396589a65fa1bb4c3c4,Live Migration for Multiple Correlated Virtual Machines in Cloud-Based Data Centers,2018,"With the development of cloud computing, virtual machine migration is emerging as a promising technique to save energy, enhance resource utilizations, and guarantee Quality of Service (QoS) in cloud datacenters. Most of existing studies on the virtual machine migration, however are based on a single virtual machine migration. Although there are some researches on multiple virtual machines migration, the author usually does not consider the correlation among these virtual machines. In practice, in order to save energy and maintain system performance, cloud providers usually need to migrate multiple correlated virtual machines or migrate the entire virtual datacenter (VDC) request. In this paper, we focus on the efficient online live migration of multiple correlated VMs in VDC requests, for optimizing the migration performance. To solve this problem, we propose an efficient VDC migration algorithm (VDC-M). We use the US-wide US National Science Foundation (NSF) network as substrate network to conduct extensive simulation experiments. Simulation results show that the performance of the proposed algorithm is promising in terms of the total VDC remapping cost, the blocking ratio, the average migration time and the average downtime.",10.1109/TSC.2015.2477825
0036975a5e51251e3ccde743ecc3aa18a9ae7623,"A Survey on Virtual Machine Migration: Challenges, Techniques, and Open Issues",2018,"When users flood in cloud data centers, how to efficiently manage hardware resources and virtual machines (VMs) in a data center to both lower economical cost and ensure a high service quality becomes an inevitable work for cloud providers. VM migration is a cornerstone technology for the majority of cloud management tasks. It frees a VM from the underlying hardware. This feature brings a plenty of benefits to cloud providers and users. Many researchers are focusing on pushing its cutting edge. In this paper, we first give an overview of VM migration and discuss both its benefits and challenges. VM migration schemes are classified from three perspectives: 1) manner; 2) distance; and 3) granularity. The studies on non-live migration are simply reviewed, and then those on live migration are comprehensively surveyed based on the three main challenges it faces: 1) memory data migration; 2) storage data migration; and 3) network connection continuity. The works on quantitative analysis of VM migration performance are also elaborated. With the development and evolution of cloud computing, user mobility becomes an important motivation for live VM migration in some scenarios (e.g., fog computing). Thus, the studies regarding linking VM migration to user mobility are summarized as well. At last, we list the open issues which are waiting for solutions or further optimizations on live VM migration.",10.1109/COMST.2018.2794881
ad9127d52b33f1e09fedec669c87ac83094b360e,AHP for a Comparative Study of Tools Used for Programming Learning,2020,,10.1007/978-3-030-50896-8_51
c303d8bf837cd173bb9bd8a0060d5b28e79b4bf8,BioMove: Biometric User Identification from Human Kinesiological Movements for Virtual Reality Systems,2020,"Virtual reality (VR) has advanced rapidly and is used for many entertainment and business purposes. The need for secure, transparent and non-intrusive identification mechanisms is important to facilitate users’ safe participation and secure experience. People are kinesiologically unique, having individual behavioral and movement characteristics, which can be leveraged and used in security sensitive VR applications to compensate for users’ inability to detect potential observational attackers in the physical world. Additionally, such method of identification using a user’s kinesiological data is valuable in common scenarios where multiple users simultaneously participate in a VR environment. In this paper, we present a user study (n = 15) where our participants performed a series of controlled tasks that require physical movements (such as grabbing, rotating and dropping) that could be decomposed into unique kinesiological patterns while we monitored and captured their hand, head and eye gaze data within the VR environment. We present an analysis of the data and show that these data can be used as a biometric discriminant of high confidence using machine learning classification methods such as kNN or SVM, thereby adding a layer of security in terms of identification or dynamically adapting the VR environment to the users’ preferences. We also performed a whitebox penetration testing with 12 attackers, some of whom were physically similar to the participants. We could obtain an average identification confidence value of 0.98 from the actual participants’ test data after the initial study and also a trained model classification accuracy of 98.6%. Penetration testing indicated all attackers resulted in confidence values of less than 50% (<50%), although physically similar attackers had higher confidence values. These findings can help the design and development of secure VR systems.",10.3390/s20102944
544ed7b9035fc22df84f7a82ae4eeb37b3d7398b,Exploring the Vulnerabilities and Advantages of SWIPE or Pattern authentication in Virtual Reality (VR),2020,"Virtual reality applications are carving out a new niche within the entertainment and business user-sphere, therefore reliable security and usability are essential to achieving consumer confidence. In this paper, we are exploring (1) the suitability of porting the popular SWIPE mobile device authentication system for use within virtual reality (VR) by observing the advantages and vulnerabilities. (2) The effects of the interaction devices such as the hand-held-controller (HHC), the LeapMotion sensor, EyeTracker and the head-mounted-display (HMD). Our study is in three-folds, a web study (N=219) to collect and analyze possible SWIPE password patterns, then a mobile device study (N=15) and a VR study (N = 15) to evaluate the speed, login errors, usability of the SWIPE authentication system in both environment for comparison. We are also interested in the effectiveness of shoulder-surfing within VR as it is known to be a weakness in mobile devices.",10.1145/3385378.3385385
edd26addf7a22eb588f784695eb833b98af6d843,MOOC Videos in Project MANEUVER,2019,"Project MANEUVER (Manufacturing Education Using Virtual Environment Resources) is developing an affordable virtual reality (VR) framework to address the imminent demand for well-trained digital manufacturing (DM) technicians. MANEUVER project has a set of MOOC (Massive Open Online Course) video contents that provides a process of setting up local VR server, navigating inside 3D environment, interacting with 3D objects, simulating CNC machines and different 3D printer models. The previously stated topics are the list of successfully created MOOC videos that have been deployed on the project MANEUVER website. MANEUVER is developing an innovative multi-modal VR framework for DM instruction. Using a ""train-the-trainers"" approach, a replicable faculty development model is being developed for secondary and post-secondary institutions. By addressing regional and national entry-level needs of the workforce, the project benefits society and contributes to national economic progress and prosperity.",10.1109/lwmoocs47620.2019.8939612
00251e9fd7258657b86dedb5f0ad814a8af4ebbb,OCR signage recognition with skew & slant correction for visually impaired people,2011,"It is a challenge for visually impaired people (VIPs) to navigate independently whenever they attempt to find their way in unfamiliar buildings searching for amenities (i.e. exits, ladies/gents toilets) even with a walking stick or a guide dog. Camera-based computer vision systems have the potential to assist VIPs in independent navigation or way finding in unfamiliar places. To leverage on previous research of Signage Recognition Framework which could only recognize public signage with slanted angle less than30°, an improved OCR signage recognition model with skew and slant correction in public signage is presented. The proposed OCR method consists of Canny edge detection algorithm, Hough Transformation and Shearing Transformation were used to detect and correct skewed and slanted images. The proposed model would capture a public signage image, compare the image in the database using template matching algorithm and convert to machine readable text in a text file. The text will then be processed by Microsoft Speech Application Program Interface (SAPI) speech synthesizer and translated to voice as output. Experiments were conducted on 5 blind folded subjects to test the performance of the model. The proposed OCR recognition model has achieved satisfactory recognition rate of 82.7%.",10.1109/HIS.2011.6122123
ba753d93d3430673158fad6f313b6d99be2fa4fe,Review of Navigation Assistive Tools and Technologies for the Visually Impaired,2022,"The visually impaired suffer greatly while moving from one place to another. They face challenges in going outdoors and in protecting themselves from moving and stationary objects, and they also lack confidence due to restricted mobility. Due to the recent rapid rise in the number of visually impaired persons, the development of assistive devices has emerged as a significant research field. This review study introduces several techniques to help the visually impaired with their mobility and presents the state-of-the-art of recent assistive technologies that facilitate their everyday life. It also analyses comprehensive multiple mobility assistive technologies for indoor and outdoor environments and describes the different location and feedback methods for the visually impaired using assistive tools based on recent technologies. The navigation tools used for the visually impaired are discussed in detail in subsequent sections. Finally, a detailed analysis of various methods is also carried out, with future recommendations.",10.3390/s22207888
643aa9328e1d576400588870a2efa00befeb306a,A Systematic Review of Urban Navigation Systems for Visually Impaired People,2021,"Blind and Visually impaired people (BVIP) face a range of practical difficulties when undertaking outdoor journeys as pedestrians. Over the past decade, a variety of assistive devices have been researched and developed to help BVIP navigate more safely and independently. In addition, research in overlapping domains are addressing the problem of automatic environment interpretation using computer vision and machine learning, particularly deep learning, approaches. Our aim in this article is to present a comprehensive review of research directly in, or relevant to, assistive outdoor navigation for BVIP. We breakdown the navigation area into a series of navigation phases and tasks. We then use this structure for our systematic review of research, analysing articles, methods, datasets and current limitations by task. We also provide an overview of commercial and non-commercial navigation applications targeted at BVIP. Our review contributes to the body of knowledge by providing a comprehensive, structured analysis of work in the domain, including the state of the art, and guidance on future directions. It will support both researchers and other stakeholders in the domain to establish an informed view of research progress.",10.3390/s21093103
216715a74a2ceb71ea2c044d9f6b4cac3e8a8ef5,Review of machine vision-based electronic travel aids,2017,"Visual impaired people have navigation and mobility problems on roads. Up to now, many approaches have been proposed to help these people navigate around using different sensing techniques. This paper reviews several machine vision-based Electronic Travel Aids (ETAs) and compares them with those using other sensing techniques. The functionalities of machine vision-based ETAs are categorized from low-level image processing such as detection of the road regions and obstacles to high-level functionalities such as recognition of digital tags and texts. In addition, the characteristics of the ETA systems for blind people are discussed.",10.23919/IConAC.2017.8082021
9aaa1213b9c7f531b8c6e8ed1ce7e07b9b33d012,Text Signage Recognition in Android Mobile Devices,2013,"This study presents a Text Signage Recognition (TSR) model in Android mobile devices for Visually Impaired People (VIP). Independence navigation is always a challenge to VIP for indoor navigation in unfamiliar surroundings. Assistive Technology such as Android smart devices has great potential to assist VIPs in indoor navigation using built-in speech synthesizer. In contrast to previous TSR research which was deployed in standalone personal computer system using Otsu’s algorithm, we have developed an affordable Text Signage Recognition in Android Mobile Devices using Tesseract OCR engine. The proposed TSR model used the input images from the International Conference on Document Analysis and Recognition (ICDAR) 2003 dataset for system training and testing. The TSR model was tested by four volunteers who were blind-folded. The system performance of the TSR model was assessed using different metrics (i.e., Precision, Recall, F-Score and Recognition Formulas) to determine its accuracy. Experimental results show that the proposed TSR model has achieved recognition rate satisfactorily.",10.3844/jcssp.2013.1793.1802
1592bbf195852d24b294eb040d681efaf7a859d0,Character extraction from the region pointed at with a fingertip for the visually handicapped,2012,"We have proposed a system that recognizes text characters from the region pointed at with a fingertip for the visually handicapped. For example, in a supermarket, the blind user can pick up and touch the goods. However, the user cannot read the characters printed on the surface of the object. He/she cannot exactly know what the object is. It is very inconvenient for him/her. The proposed system will help the user to recognize the characters on the object. The system detects the ring which the user wears on the index finger and calculates the position of the user's fingertip of the index finger. Some acrylic infrared-reflective beads are arranged on the ring and it is easy to detect them from an infrared image. After fingertip detection, a region in which characters to be recognized is generated according to the position of the fingertip. The characters are extracted within the region using image processing. Thus, the redundant character recognition is avoided. The extracted characters are combined as a text and the text is read aloud by OCR software and free reading software for the user. Some experiments show the basic validity of the proposed method and system.",10.1109/ROMAN.2012.6343764
002544729825daf6843a471ccb22d446969511b7,A survey on Arabic Image Captioning Systems Using Deep Learning Models,2020,"This paper describes a literature survey for the deep leaning approaches used in image captioning. Approaches will be discussed based on four main categories: model architecture, attention mechanism, image model and language model. Most of the current research focuses on generating captions in English language, leaving a gap in research for other languages, especially for Arabic language. Therefore, we will highlight the available research and approaches used to generate captions in Arabic. We will discuss the used datasets, translation approach, evaluation metrics and the results for each method. We conclude the survey by proposing some possible future directions for Arabic image captioning.",10.1109/IIT50501.2020.9299027
d5dc38ed5a3e386a2f4303ef858796f94bbb44d0,Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks,2024,"Multimodal large language models (MLLMs) have proven effective in a wide range of tasks requiring complex reasoning and linguistic comprehension. However, due to a lack of high-quality multimodal resources in languages other than English, success of MLLMs remains relatively limited to English-based settings. This poses significant challenges in developing comparable models for other languages, including even those with large speaker populations such as Arabic. To alleviate this challenge, we introduce a comprehensive family of Arabic MLLMs, dubbed \textit{Peacock}, with strong vision and language capabilities. Through comprehensive qualitative and quantitative analysis, we demonstrate the solid performance of our models on various visual reasoning tasks and further show their emerging dialectal potential. Additionally, we introduce ~\textit{Henna}, a new benchmark specifically designed for assessing MLLMs on aspects related to Arabic culture, setting the first stone for culturally-aware Arabic MLLMs.The GitHub repository for the \textit{Peacock} project is available at \url{https://github.com/UBC-NLP/peacock}.",10.48550/arXiv.2403.01031
5c1927090959676f40eb91442b0295bedf67cdf2,Arabic Captioning for Images of Clothing Using Deep Learning,2023,"Fashion is one of the many fields of application that image captioning is being used in. For e-commerce websites holding tens of thousands of images of clothing, automated item descriptions are quite desirable. This paper addresses captioning images of clothing in the Arabic language using deep learning. Image captioning systems are based on Computer Vision and Natural Language Processing techniques because visual and textual understanding is needed for these systems. Many approaches have been proposed to build such systems. The most widely used methods are deep learning methods which use the image model to analyze the visual content of the image, and the language model to generate the caption. Generating the caption in the English language using deep learning algorithms received great attention from many researchers in their research, but there is still a gap in generating the caption in the Arabic language because public datasets are often not available in the Arabic language. In this work, we created an Arabic dataset for captioning images of clothing which we named “ArabicFashionData” because this model is the first model for captioning images of clothing in the Arabic language. Moreover, we classified the attributes of the images of clothing and used them as inputs to the decoder of our image captioning model to enhance Arabic caption quality. In addition, we used the attention mechanism. Our approach achieved a BLEU-1 score of 88.52. The experiment findings are encouraging and suggest that, with a bigger dataset, the attributes-based image captioning model can achieve excellent results for Arabic image captioning.",10.3390/s23083783
8b3298e265545853819add7fbbb93e7a73d1480d,Challenges Designing for FPGAs Using High-Level Synthesis,2022,"High-Level Synthesis (HLS) tools are aimed at enabling performant FPGA designs that are authored in a high-level language. While commercial HLS tools are available today, there is still a substantial performance gap between most designs developed via HLS relative to traditional, labor intensive approaches. We report on several cases where an anticipated performance improvement was either not realized or resulted in decreased performance. These include: programming paradigm choices between data parallel vs. pipelined designs; dataflow implementations; configuration parameter choices; and handling odd data set sizes. The results point to a number of improvements that are needed for HLS tool flows, including a strong need for performance modeling that can reliably guide the compilation optimization process.",10.1109/HPEC55821.2022.9926398
00b6d0fea5fcc06a46e221dea1079b7d7e1d9b1f,Optimal Binding and Port Assignment for Loop Pipelining in High-Level Synthesis,2022,"In order to provide high throughput for custom hardware implementations, academic and commercial high-level synthesis (HLS) tools use loop pipelining by modulo scheduling. When provided a resource allocation and a schedule, the binding algorithm can be used to reduce the number of required lifetime registers (LR) and multiplexers (MUX). Contrary to non-modulo schedules, optimal solutions to the binding problem for implementing modulo schedules with respect to minimizing required LRs and MUXs have not been published. To address this topic, we propose a novel optimal binding algorithm to simultaneously minimize MUX and LR costs for loop pipelining using Integer Linear Programming. We evaluated our algorithm on a set of commonly used benchmark instances from digital signal processing and report that all encountered problems could be solved, with 36.53% of the solutions being optimal within a time limit of only five minutes. Compared to worst case evaluations, we report MUX and LR savings of up to 42.74% and 26.62%, respectively. To evaluate the impact on the resulting circuit after place and route, we studied FPGA implementations of several benchmark instances and recorded look-up table and flip-flop reductions of up to 13.70% and 5.24%, respectively, compared to previous work and to an extensive set of randomly generated bindings when state-of-the-art algorithms fail to find a feasible solution.",10.1109/FPL57034.2022.00047
348e380a48afcc55e72438bfac2747ec4c8cbfd4,COSMO: Computing with Stochastic Numbers in Memory,2022,"Stochastic computing (SC) reduces the complexity of computation by representing numbers with long streams of independent bits. However, increasing performance in SC comes with either an increase in area or a loss in accuracy. Processing in memory (PIM) computes data in-place while having high memory density and supporting bit-parallel operations with low energy consumption. In this article, we propose COSMO, an architecture for computing with stochastic numbers in memory, which enables SC in memory. The proposed architecture is general and can be used for a wide range of applications. It is a highly dense and parallel architecture that supports most SC encodings and operations in memory. It maximizes the performance and energy efficiency of SC by introducing several innovations: (i) in-memory parallel stochastic number generation, (ii) efficient implication-based logic in memory, (iii) novel memory bit line segmenting, (iv) a new memory-compatible SC addition operation, and (v) enabling flexible block allocation. To show the generality and efficiency of our stochastic architecture, we implement image processing, deep neural networks (DNNs), and hyperdimensional (HD) computing on the proposed hardware. Our evaluations show that running DNN inference on COSMO is 141× faster and 80× more energy efficient as compared to GPU.",10.1145/3484731
edc0bd4137cc3de24a2367ecfc2dde1203d1b682,Resource and Performance Estimation for CNN Models using Machine Learning,2021,"Field-Programmable Gate Array (FPGA) based hardware accelerators offer reconfigurability, performance, adaptability, and good energy efficiency. The majority of Convolutional Neural Network (CNN) based inference systems are initially developed using standardized frameworks like PyTorch, Tensor Flow, and more. These Python or Python-like models can be mapped on FPGAs to build accelerators. Mapping frameworks to port designs on an FPGA convert the CNN models to high-level languages like C/C++ or OpenCL so that standard tools like high-level synthesis can facilitate the mapping of models on an FPGA. The logic utilization and performance of FPGA-based accelerators are dependent on the CNN network parameters, architectural selection (data-flow, pipelined, etc.), and synthesis-based control of design generation. A scalable multi-layer CNN hardware accelerator is modeled in Vitis 2020 HLS tool. Early estimation of performance and hardware resources helps choose the best CNN network before those are executed for time-consuming high-level synthesis and physical design mapping for FPGAs. We present various Machine Learning (ML) models to estimate the Logic Utilization and Computation Time from the Python design descriptions of CNNs. Our results show a very successful and accurate estimation for performance and resource utilization over various multi-layer CNN networks in negligible time before running HLS synthesis.",10.1109/ISVLSI51109.2021.00019
71bda1c7e69c62b648c1fd44c7160f457e73e00c,An Automated Tool for Implementing Deep Neural Networks on FPGA,2021,"FPGA based Deep Neural Networks provide the advantage of high performance, highly parallel implementation with very low energy requirements. A designer must consider various configuration choices, processing components, data-flow types, local memory hierarchy, and fixed-data-precision for a DNN implementation. An exploration tool is essential for building a reconfigurable, fast, and efficient DNN hardware accelerator. We present a methodology to automatically create an optimized FPGA-based hardware accelerator given DNNs from standard machine learning frameworks. We generate a High-Level-Synthesis (HLS) code depending on the user preferences with a set of optimization pragmas. For a faster and cost-effective hardware accelerator, the tool employs a software-model to estimate the execution time and hardware utilization using trained machine-learning models. The model evaluation results show that our framework performance speed-up compares well with the state-of-the-art accelerators using Xilinx FPGA platforms.",10.1109/VLSID51830.2021.00060
0025ab61c405d2f60446d6c8cdcdf0d9680f6163,A Re-Balancing Strategy for Class-Imbalanced Classification Based on Instance Difficulty,2022,"Real-world data often exhibits class-imbalanced distributions, where a few classes (a.k.a. majority classes) occupy most instances and lots of classes (a.k.a. minority classes) have few instances. Neural classification models usually perform poorly on minority classes when training on such imbalanced datasets. To improve the performance on minority classes, existing methods typically re-balance the data distribution at the class level, i.e., assigning higher weights to minority classes and lower weights to majority classes during the training process. However, we observe that even the majority classes contain difficult instances to learn. By reducing the weights of the majority classes, such instances would become more difficult to learn and hurt the overall performance consequently. To tackle this problem, we propose a novel instance-level re-balancing strategy, which dynamically adjusts the sampling probabilities of instances according to the instance difficulty. Here the instance difficulty is measured based on the learning speed of instance, which is inspired by the human-leaning process (i.e., easier instances will be learned faster). We theoretically prove the correctness and convergence of our resampling algorithm. Empirical experiments demonstrate that our method significantly outperforms state-of-the-art re-balancing methods on the class-imbalanced datasets.",10.1109/CVPR52688.2022.00017
